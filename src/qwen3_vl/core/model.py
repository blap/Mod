"""\nQwen3-VL Model core implementation\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Union, List\nfrom core.config import Qwen3VLConfig\nfrom model_layers.language_decoder import Qwen3VLDecoder\nfrom model_layers.vision_transformer import Qwen3VLVisionTransformer\nfrom model_layers.multimodal_projector import Qwen3VLMultimodalProjector\n\n\nclass Qwen3VLModel(nn.Module):\n    """\n    The Qwen3-VL Model transformer with multimodal capabilities\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n\n        # Initialize vision encoder\n        self.vision_embed_tokens = Qwen3VLVisionTransformer(config)\n\n        # Initialize language decoder\n        self.decoder = Qwen3VLDecoder(config)\n\n        # Initialize multimodal projector\n        self.multi_modal_projector = Qwen3VLMultimodalProjector(config)\n\n        # Initialize weights\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.decoder.embed_tokens = value\n\n    def post_init(self):\n        """\n        A method executed at the end of each model initialization, to execute code that needs the model's modules\n        to be initialized.\n        """\n        # Initialize weights\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        """Initialize the weights"""\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n    ):\n        # Process image input if provided\n        if pixel_values is not None:\n            # Get vision embeddings\n            if hasattr(self, 'vision_embed_tokens'):\n                vision_embeddings = self.vision_embed_tokens(pixel_values)\n            else:\n                # If vision embed tokens don't exist, we'll handle this differently\n                # For now, we'll just continue without vision processing\n                vision_embeddings = None\n\n            if vision_embeddings is not None:\n                # Project vision embeddings to language space\n                if hasattr(self, 'multi_modal_projector'):\n                    vision_embeddings = self.multi_modal_projector(vision_embeddings)\n\n                # Combine with text embeddings if both are provided\n                if inputs_embeds is not None:\n                    # Concatenate vision and text embeddings\n                    inputs_embeds = torch.cat([vision_embeddings, inputs_embeds], dim=1)\n                else:\n                    inputs_embeds = vision_embeddings\n\n        # Forward through the decoder\n        hidden_states = self.decoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        return hidden_states\n\n\ndef load_qwen3_vl_model(config: Qwen3VLConfig):\n    """\n    Load a Qwen3-VL model from configuration\n    """\n    model = Qwen3VLModel(config)\n    return model