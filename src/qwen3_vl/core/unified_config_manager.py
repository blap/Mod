"""Unified Configuration Manager for Qwen3-VL model with support for multiple optimization techniques."""\n\nimport json\nimport os\nfrom dataclasses import dataclass, field, fields\nfrom typing import Any, Dict, List, Optional, Union, Type\nfrom pathlib import Path\nimport yaml\nimport logging\nfrom copy import deepcopy\nimport warnings\nimport torch\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigValidator:\n    """Validates configuration parameters and their compatibility."""\n\n    @staticmethod\n    def validate_config(config) -> List[str]:\n        """Validate the configuration and return list of validation errors."""\n        errors = []\n\n        # Validate core model parameters\n        if hasattr(config, 'num_hidden_layers') and config.num_hidden_layers <= 0:\n            errors.append(f"num_hidden_layers must be positive, got {config.num_hidden_layers}")\n\n        if hasattr(config, 'num_attention_heads') and config.num_attention_heads <= 0:\n            errors.append(f"num_attention_heads must be positive, got {config.num_attention_heads}")\n\n        if hasattr(config, 'hidden_size') and config.hidden_size <= 0:\n            errors.append(f"hidden_size must be positive, got {config.hidden_size}")\n\n        if hasattr(config, 'intermediate_size') and config.intermediate_size <= 0:\n            errors.append(f"intermediate_size must be positive, got {config.intermediate_size}")\n\n        if hasattr(config, 'vocab_size') and config.vocab_size <= 0:\n            errors.append(f"vocab_size must be positive, got {config.vocab_size}")\n\n        if hasattr(config, 'max_position_embeddings') and config.max_position_embeddings <= 0:\n            errors.append(f"max_position_embeddings must be positive, got {config.max_position_embeddings}")\n\n        # Validate memory configuration if it exists\n        if hasattr(config, 'memory_config') and config.memory_config:\n            if hasattr(config.memory_config, 'memory_pool_size') and config.memory_config.memory_pool_size <= 0:\n                errors.append(f"memory_pool_size must be positive, got {config.memory_config.memory_pool_size}")\n\n            if hasattr(config.memory_config, 'compression_threshold'):\n                if config.memory_config.compression_threshold < 0 or config.memory_config.compression_threshold > 1:\n                    errors.append(f"compression_threshold must be between 0 and 1, got {config.memory_config.compression_threshold}")\n\n            if hasattr(config.memory_config, 'swap_threshold'):\n                if config.memory_config.swap_threshold < 0 or config.memory_config.swap_threshold > 1:\n                    errors.append(f"swap_threshold must be between 0 and 1, got {config.memory_config.swap_threshold}")\n\n        # Validate CPU configuration if it exists\n        if hasattr(config, 'cpu_config') and config.cpu_config:\n            if hasattr(config.cpu_config, 'num_threads') and config.cpu_config.num_threads <= 0:\n                errors.append(f"num_threads must be positive, got {config.cpu_config.num_threads}")\n\n            if hasattr(config.cpu_config, 'l1_cache_size') and config.cpu_config.l1_cache_size <= 0:\n                errors.append(f"l1_cache_size must be positive, got {config.cpu_config.l1_cache_size}")\n\n        # Validate GPU configuration if it exists\n        if hasattr(config, 'gpu_config') and config.gpu_config:\n            if hasattr(config.gpu_config, 'max_threads_per_block') and config.gpu_config.max_threads_per_block <= 0:\n                errors.append(f"max_threads_per_block must be positive, got {config.gpu_config.max_threads_per_block}")\n\n            if hasattr(config.gpu_config, 'memory_bandwidth_gbps') and config.gpu_config.memory_bandwidth_gbps <= 0:\n                errors.append(f"memory_bandwidth_gbps must be positive, got {config.gpu_config.memory_bandwidth_gbps}")\n\n        # Validate optimization configuration if it exists\n        if hasattr(config, 'optimization_config') and config.optimization_config:\n            if hasattr(config.optimization_config, 'sparsity_ratio'):\n                if config.optimization_config.sparsity_ratio < 0 or config.optimization_config.sparsity_ratio > 1:\n                    errors.append(f"sparsity_ratio must be between 0 and 1, got {config.optimization_config.sparsity_ratio}")\n\n            if hasattr(config.optimization_config, 'moe_num_experts'):\n                if config.optimization_config.moe_num_experts < 2:\n                    errors.append(f"moe_num_experts must be at least 2, got {config.optimization_config.moe_num_experts}")\n\n            if hasattr(config.optimization_config, 'moe_top_k'):\n                if config.optimization_config.moe_top_k < 1 or (hasattr(config.optimization_config, 'moe_num_experts') and \n                    config.optimization_config.moe_top_k > config.optimization_config.moe_num_experts):\n                    errors.append(f"moe_top_k must be between 1 and moe_num_experts ({config.optimization_config.moe_num_experts}), got {config.optimization_config.moe_top_k}")\n\n        return errors\n\n    @staticmethod\n    def validate_optimization_compatibility(config) -> List[str]:\n        """Validate that optimization settings are compatible with each other."""\n        errors = []\n\n        # Some optimizations might conflict with each other\n        if (hasattr(config, 'use_gradient_checkpointing') and config.use_gradient_checkpointing and \n            hasattr(config, 'use_memory_efficient_attention') and config.use_memory_efficient_attention):\n            warnings.warn("Using gradient checkpointing with memory efficient attention may lead to suboptimal performance.")\n\n        if (hasattr(config, 'use_moe') and config.use_moe and \n            hasattr(config, 'use_sparsity') and config.use_sparsity):\n            # These can coexist but may need special handling\n            pass\n\n        return errors\n\n\nclass ConfigSourceManager:\n    """Manages configuration from multiple sources with proper precedence."""\n\n    def __init__(self):\n        self.sources = {}\n\n    def add_source(self, name: str, source: Union[Dict[str, Any], str, Path, 'Qwen3VLConfig']):\n        """Add a configuration source."""\n        self.sources[name] = source\n\n    def load_from_file(self, file_path: Union[str, Path]) -> Dict[str, Any]:\n        """Load configuration from file."""\n        file_path = Path(file_path)\n\n        if file_path.suffix.lower() in ['.yaml', '.yml']:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                return yaml.safe_load(f)\n        else:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                return json.load(f)\n\n    def load_from_env(self, prefix: str = "QWEN3_") -> Dict[str, Any]:\n        """Load configuration from environment variables."""\n        env_config = {}\n\n        for key, value in os.environ.items():\n            if key.startswith(prefix):\n                config_key = key[len(prefix):].lower()\n\n                # Try to convert to appropriate type\n                if value.lower() in ['true', 'false']:\n                    env_config[config_key] = value.lower() == 'true'\n                elif value.isdigit():\n                    env_config[config_key] = int(value)\n                elif '.' in value and all(part.isdigit() for part in value.split('.', 1)):\n                    env_config[config_key] = float(value)\n                else:\n                    # Check if it looks like a list/tuple\n                    if value.startswith('[') and value.endswith(']'):\n                        # Handle list values\n                        try:\n                            env_config[config_key] = eval(value)\n                        except:\n                            env_config[config_key] = value\n                    elif value.startswith('(') and value.endswith(')'):\n                        # Handle tuple values\n                        try:\n                            env_config[config_key] = eval(value)\n                        except:\n                            env_config[config_key] = value\n                    else:\n                        env_config[config_key] = value\n\n        return env_config\n\n    def merge_configs(self, base_config: Union[Dict[str, Any], 'Qwen3VLConfig'], overrides: Dict[str, Any]) -> Dict[str, Any]:\n        """Merge override configuration into base configuration."""\n        if hasattr(base_config, 'to_dict'):\n            base_dict = base_config.to_dict()\n        else:\n            base_dict = base_config\n\n        # Deep merge the dictionaries\n        merged_dict = self._deep_merge(base_dict, overrides)\n\n        return merged_dict\n\n    def _deep_merge(self, base: Dict[str, Any], overrides: Dict[str, Any]) -> Dict[str, Any]:\n        """Deep merge two dictionaries."""\n        result = deepcopy(base)\n\n        for key, value in overrides.items():\n            if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n                result[key] = self._deep_merge(result[key], value)\n            else:\n                result[key] = value\n\n        return result\n\n    def resolve_config(self, precedence: List[str] = None) -> 'Qwen3VLConfig':\n        """Resolve configuration from all sources based on precedence."""\n        if precedence is None:\n            precedence = list(self.sources.keys())\n\nfrom core.config import Qwen3VLConfig\n        resolved_config = Qwen3VLConfig()\n\n        for source_name in precedence:\n            if source_name in self.sources:\n                source = self.sources[source_name]\n\n                if hasattr(source, 'to_dict'):\n                    # If source is already a config object, merge its dict representation\n                    source_dict = source.to_dict()\n                elif isinstance(source, (str, Path)):\n                    # If source is a file path, load from file\n                    source_dict = self.source_manager.load_from_file(source)\n                elif isinstance(source, dict):\n                    # If source is a dict, use it directly\n                    source_dict = source\n                else:\n                    raise ValueError(f"Invalid source type: {type(source)}")\n\n                # Merge this source into the resolved config\n                resolved_dict = self.merge_configs(resolved_config, source_dict)\n                resolved_config = Qwen3VLConfig.from_dict(resolved_dict)\n\n        return resolved_config\n\n\nclass UnifiedConfigManager:\n    """Centralized configuration manager for all optimization techniques."""\n\n    def __init__(self, base_config: Optional['Qwen3VLConfig'] = None):\n        self.validator = ConfigValidator()\n        self.source_manager = ConfigSourceManager()\n\n        # Initialize with base config or default\n        self.base_config = base_config or self._get_default_config()\n\n        # Register default configurations\n        self._register_default_configs()\n\n    def _get_default_config(self) -> 'Qwen3VLConfig':\n        """Get the default configuration."""\nfrom core.config import Qwen3VLConfig\n        return Qwen3VLConfig()\n\n    def _register_default_configs(self):\n        """Register default configurations for different optimization levels."""\nfrom core.config import Qwen3VLConfig\n\n        # Minimal optimization config\n        minimal_config = Qwen3VLConfig(\n            use_memory_pooling=False,\n            use_hierarchical_memory_compression=False,\n            use_memory_efficient_attention=False,\n            use_kv_cache_optimization=False,\n            use_cross_layer_parameter_sharing=False,\n            use_sparsity=False,\n            use_dynamic_sparse_attention=False,\n            use_adaptive_precision=False,\n            use_moe=False,\n            use_flash_attention_2=False,\n            use_adaptive_depth=False,\n            use_gradient_checkpointing=False,\n            use_context_adaptive_positional_encoding=False,\n            use_conditional_feature_extraction=False,\n            use_cross_modal_compression=False,\n            use_cross_layer_memory_sharing=False,\n            use_hierarchical_vision=False,\n            use_learned_activation_routing=False,\n            use_adaptive_batch_processing=False,\n            use_adaptive_sequence_packing=False,\n            use_memory_efficient_grad_accumulation=False,\n            use_faster_rotary_embeddings=False,\n            use_distributed_pipeline_parallelism=False,\n            use_hardware_specific_kernels=False,\n            performance_improvement_threshold=0.0,\n            accuracy_preservation_threshold=0.9,\n            optimization_level="minimal"\n        )\n\n        # Balanced optimization config\n        balanced_config = Qwen3VLConfig(\n            use_memory_pooling=True,\n            use_hierarchical_memory_compression=True,\n            use_memory_efficient_attention=True,\n            use_kv_cache_optimization=True,\n            use_cross_layer_parameter_sharing=True,\n            use_sparsity=True,\n            sparsity_ratio=0.3,\n            use_dynamic_sparse_attention=True,\n            use_adaptive_precision=True,\n            use_moe=True,\n            moe_num_experts=2,\n            moe_top_k=1,\n            use_flash_attention_2=True,\n            use_adaptive_depth=True,\n            use_gradient_checkpointing=True,\n            use_context_adaptive_positional_encoding=True,\n            use_conditional_feature_extraction=True,\n            use_cross_modal_compression=True,\n            use_cross_layer_memory_sharing=True,\n            use_hierarchical_vision=True,\n            use_learned_activation_routing=True,\n            use_adaptive_batch_processing=True,\n            use_adaptive_sequence_packing=True,\n            use_memory_efficient_grad_accumulation=True,\n            use_faster_rotary_embeddings=True,\n            use_distributed_pipeline_parallelism=False,\n            use_hardware_specific_kernels=True,\n            performance_improvement_threshold=0.05,\n            accuracy_preservation_threshold=0.95,\n            optimization_level="balanced"\n        )\n\n        # Aggressive optimization config\n        aggressive_config = Qwen3VLConfig(\n            use_memory_pooling=True,\n            use_hierarchical_memory_compression=True,\n            use_memory_efficient_attention=True,\n            use_kv_cache_optimization=True,\n            use_cross_layer_parameter_sharing=True,\n            use_sparsity=True,\n            sparsity_ratio=0.6,\n            use_dynamic_sparse_attention=True,\n            use_adaptive_precision=True,\n            use_moe=True,\n            moe_num_experts=4,\n            moe_top_k=2,\n            use_flash_attention_2=True,\n            use_adaptive_depth=True,\n            use_gradient_checkpointing=True,\n            use_context_adaptive_positional_encoding=True,\n            use_conditional_feature_extraction=True,\n            use_cross_modal_compression=True,\n            use_cross_layer_memory_sharing=True,\n            use_hierarchical_vision=True,\n            use_learned_activation_routing=True,\n            use_adaptive_batch_processing=True,\n            use_adaptive_sequence_packing=True,\n            use_memory_efficient_grad_accumulation=True,\n            use_faster_rotary_embeddings=True,\n            use_distributed_pipeline_parallelism=False,\n            use_hardware_specific_kernels=True,\n            performance_improvement_threshold=0.1,\n            accuracy_preservation_threshold=0.9,\n            optimization_level="aggressive"\n        )\n\n        # Register configurations\n        self.source_manager.add_source("minimal", minimal_config)\n        self.source_manager.add_source("balanced", balanced_config)\n        self.source_manager.add_source("aggressive", aggressive_config)\n\n    def load_config_from_file(self, file_path: Union[str, Path], name: str = "file_config") -> bool:\n        """Load configuration from file."""\n        try:\nfrom core.config import Qwen3VLConfig\n            config_dict = self.source_manager.load_from_file(file_path)\n            config = Qwen3VLConfig.from_dict(config_dict)\n            self.source_manager.add_source(name, config)\n            logger.info(f"Loaded configuration from {file_path} as {name}")\n            return True\n        except Exception as e:\n            logger.error(f"Failed to load configuration from {file_path}: {e}")\n            return False\n\n    def load_config_from_env(self, name: str = "env_config", prefix: str = "QWEN3_") -> bool:\n        """Load configuration from environment variables."""\n        try:\nfrom core.config import Qwen3VLConfig\n            env_config = self.source_manager.load_from_env(prefix)\n            config = Qwen3VLConfig.from_dict(env_config)\n            self.source_manager.add_source(name, config)\n            logger.info(f"Loaded configuration from environment variables with prefix {prefix} as {name}")\n            return True\n        except Exception as e:\n            logger.error(f"Failed to load configuration from environment: {e}")\n            return False\n\n    def get_config(self, optimization_level: str = "balanced") -> 'Qwen3VLConfig':\n        """Get a configuration with the specified optimization level."""\n        if optimization_level in self.source_manager.sources:\n            config = self.source_manager.sources[optimization_level]\n            if isinstance(config, dict):\nfrom core.config import Qwen3VLConfig\n                return Qwen3VLConfig.from_dict(config)\n            else:\n                return config\n        else:\n            # Create a config based on the level\nfrom core.config import Qwen3VLConfig\n            if optimization_level == "minimal":\n                return Qwen3VLConfig(\n                    use_memory_pooling=False,\n                    use_hierarchical_memory_compression=False,\n                    use_memory_efficient_attention=False,\n                    use_kv_cache_optimization=False,\n                    use_cross_layer_parameter_sharing=False,\n                    use_sparsity=False,\n                    use_dynamic_sparse_attention=False,\n                    use_adaptive_precision=False,\n                    use_moe=False,\n                    use_flash_attention_2=False,\n                    use_adaptive_depth=False,\n                    use_gradient_checkpointing=False,\n                    use_context_adaptive_positional_encoding=False,\n                    use_conditional_feature_extraction=False,\n                    use_cross_modal_compression=False,\n                    use_cross_layer_memory_sharing=False,\n                    use_hierarchical_vision=False,\n                    use_learned_activation_routing=False,\n                    use_adaptive_batch_processing=False,\n                    use_adaptive_sequence_packing=False,\n                    use_memory_efficient_grad_accumulation=False,\n                    use_faster_rotary_embeddings=False,\n                    use_distributed_pipeline_parallelism=False,\n                    use_hardware_specific_kernels=False,\n                    performance_improvement_threshold=0.0,\n                    accuracy_preservation_threshold=0.9,\n                    optimization_level="minimal"\n                )\n            elif optimization_level == "aggressive":\n                return Qwen3VLConfig(\n                    use_memory_pooling=True,\n                    use_hierarchical_memory_compression=True,\n                    use_memory_efficient_attention=True,\n                    use_kv_cache_optimization=True,\n                    use_cross_layer_parameter_sharing=True,\n                    use_sparsity=True,\n                    sparsity_ratio=0.6,\n                    use_dynamic_sparse_attention=True,\n                    use_adaptive_precision=True,\n                    use_moe=True,\n                    moe_num_experts=4,\n                    moe_top_k=2,\n                    use_flash_attention_2=True,\n                    use_adaptive_depth=True,\n                    use_gradient_checkpointing=True,\n                    use_context_adaptive_positional_encoding=True,\n                    use_conditional_feature_extraction=True,\n                    use_cross_modal_compression=True,\n                    use_cross_layer_memory_sharing=True,\n                    use_hierarchical_vision=True,\n                    use_learned_activation_routing=True,\n                    use_adaptive_batch_processing=True,\n                    use_adaptive_sequence_packing=True,\n                    use_memory_efficient_grad_accumulation=True,\n                    use_faster_rotary_embeddings=True,\n                    use_distributed_pipeline_parallelism=False,\n                    use_hardware_specific_kernels=True,\n                    performance_improvement_threshold=0.1,\n                    accuracy_preservation_threshold=0.9,\n                    optimization_level="aggressive"\n                )\n            else:  # Default to balanced\n                return Qwen3VLConfig(\n                    use_memory_pooling=True,\n                    use_hierarchical_memory_compression=True,\n                    use_memory_efficient_attention=True,\n                    use_kv_cache_optimization=True,\n                    use_cross_layer_parameter_sharing=True,\n                    use_sparsity=True,\n                    sparsity_ratio=0.3,\n                    use_dynamic_sparse_attention=True,\n                    use_adaptive_precision=True,\n                    use_moe=True,\n                    moe_num_experts=2,\n                    moe_top_k=1,\n                    use_flash_attention_2=True,\n                    use_adaptive_depth=True,\n                    use_gradient_checkpointing=True,\n                    use_context_adaptive_positional_encoding=True,\n                    use_conditional_feature_extraction=True,\n                    use_cross_modal_compression=True,\n                    use_cross_layer_memory_sharing=True,\n                    use_hierarchical_vision=True,\n                    use_learned_activation_routing=True,\n                    use_adaptive_batch_processing=True,\n                    use_adaptive_sequence_packing=True,\n                    use_memory_efficient_grad_accumulation=True,\n                    use_faster_rotary_embeddings=True,\n                    use_distributed_pipeline_parallelism=False,\n                    use_hardware_specific_kernels=True,\n                    performance_improvement_threshold=0.05,\n                    accuracy_preservation_threshold=0.95,\n                    optimization_level="balanced"\n                )\n\n        # Default return in case all conditions fail (should not happen under normal circumstances)\nfrom core.config import Qwen3VLConfig\n        return Qwen3VLConfig(optimization_level="balanced")  # Default config\n\n    def validate_config(self, config: 'Qwen3VLConfig') -> bool:\n        """Validate configuration and return True if valid."""\n        errors = self.validator.validate_config(config)\n        optimization_errors = self.validator.validate_optimization_compatibility(config)\n        all_errors = errors + optimization_errors\n\n        if all_errors:\n            logger.error(f"Configuration validation failed with {len(all_errors)} errors:")\n            for error in all_errors:\n                logger.error(f"  - {error}")\n            return False\n        return True\n\n    def update_config(self, config: 'Qwen3VLConfig', updates: Dict[str, Any]) -> 'Qwen3VLConfig':\n        """Update configuration with new values."""\n        try:\n            # Create updated config\n            updated_dict = config.to_dict()\n\n            # Apply updates\n            for key, value in updates.items():\n                if key in updated_dict:\n                    updated_dict[key] = value\n                else:\n                    logger.warning(f"Key '{key}' not found in configuration")\n\n            # Create new config from updated dict\nfrom core.config import Qwen3VLConfig\n            new_config = Qwen3VLConfig.from_dict(updated_dict)\n\n            # Validate the new config\n            if self.validate_config(new_config):\n                logger.info("Configuration updated successfully")\n                return new_config\n            else:\n                logger.error("Updated configuration failed validation")\n                return config  # Return original config if validation fails\n        except Exception as e:\n            logger.error(f"Failed to update configuration: {e}")\n            return config\n\n    def get_hardware_optimized_config(self, hardware_specs: Dict[str, Any]) -> 'Qwen3VLConfig':\n        """Get configuration optimized for specific hardware."""\nfrom core.config import Qwen3VLConfig\n\n        # Start with the base balanced config\n        config = self.get_config("balanced")\n\n        # Update based on hardware specs\n        if "gpu_memory" in hardware_specs:\n            gpu_memory_gb = hardware_specs["gpu_memory"] / (1024**3)\n            if hasattr(config, 'gpu_memory_size'):\n                config.gpu_memory_size = hardware_specs["gpu_memory"]\n\n            # Adjust precision based on available GPU memory\n            if gpu_memory_gb < 4:\n                if hasattr(config, 'use_mixed_precision'):\n                    config.use_mixed_precision = True\n                if hasattr(config, 'torch_dtype'):\n                    config.torch_dtype = "float16"\n            elif gpu_memory_gb < 8:\n                if hasattr(config, 'use_mixed_precision'):\n                    config.use_mixed_precision = True\n                if hasattr(config, 'torch_dtype'):\n                    config.torch_dtype = "bfloat16"\n\n        if "cpu_cores" in hardware_specs:\n            if hasattr(config, 'num_threads'):\n                config.num_threads = hardware_specs["cpu_cores"]\n            if hasattr(config, 'num_workers'):\n                config.num_workers = hardware_specs["cpu_cores"]\n\n        if "memory_gb" in hardware_specs:\n            memory_gb = hardware_specs["memory_gb"]\n            if hasattr(config, 'memory_pool_size'):\n                config.memory_pool_size = int(memory_gb * 0.5 * 1024**3)  # 50% of RAM\n\n            # Adjust optimization aggressiveness based on memory\n            if memory_gb < 8:\n                if hasattr(config, 'sparsity_ratio'):\n                    config.sparsity_ratio = 0.5\n                if hasattr(config, 'moe_num_experts'):\n                    config.moe_num_experts = 2\n            elif memory_gb < 16:\n                if hasattr(config, 'sparsity_ratio'):\n                    config.sparsity_ratio = 0.3\n                if hasattr(config, 'moe_num_experts'):\n                    config.moe_num_experts = 3\n            else:\n                if hasattr(config, 'sparsity_ratio'):\n                    config.sparsity_ratio = 0.2\n                if hasattr(config, 'moe_num_experts'):\n                    config.moe_num_experts = 4\n\n        if "storage_type" in hardware_specs:\n            storage_type = hardware_specs["storage_type"].lower()\n            if "nvme" in storage_type:\n                if hasattr(config, 'enable_memory_swapping'):\n                    config.enable_memory_swapping = True\n                if hasattr(config, 'ssd_memory_size'):\n                    config.ssd_memory_size = 100 * 1024**3  # 100GB for NVMe\n            elif "ssd" in storage_type:\n                if hasattr(config, 'enable_memory_swapping'):\n                    config.enable_memory_swapping = True\n                if hasattr(config, 'ssd_memory_size'):\n                    config.ssd_memory_size = 50 * 1024**3  # 50GB for SSD\n            else:\n                if hasattr(config, 'enable_memory_swapping'):\n                    config.enable_memory_swapping = False\n                if hasattr(config, 'ssd_memory_size'):\n                    config.ssd_memory_size = 20 * 1024**3  # 20GB for HDD\n\n        # Validate the optimized config\n        self.validate_config(config)\n\n        return config\n\n\ndef get_default_config() -> 'Qwen3VLConfig':\n    """Get the default unified configuration."""\n    config_manager = UnifiedConfigManager()\n    return config_manager.get_config()\n\n\ndef create_unified_config_manager() -> UnifiedConfigManager:\n    """Create and return a unified configuration manager instance."""\n    return UnifiedConfigManager()\n\n\n# Backward compatibility functions\ndef get_legacy_config() -> Dict[str, Any]:\n    """Return a legacy-style configuration dictionary for backward compatibility."""\n    config = get_default_config()\n    if config is not None:\n        return config.to_dict() if hasattr(config, 'to_dict') else {}\n    else:\n        # Return a default configuration as dictionary if get_default_config returns None\n        default_manager = UnifiedConfigManager()\n        default_config = default_manager.get_config()\n        return default_config.to_dict() if hasattr(default_config, 'to_dict') else {}\n\n\ndef update_legacy_config(config_dict: Dict[str, Any], updates: Dict[str, Any]) -> Dict[str, Any]:\n    """Update legacy configuration dictionary for backward compatibility."""\n    # Convert to unified config, update, then convert back\nfrom core.config import Qwen3VLConfig\n    try:\n        unified_config = Qwen3VLConfig.from_dict(config_dict)\n        config_manager = UnifiedConfigManager()\n        updated_config = config_manager.update_config(unified_config, updates)\n        return updated_config.to_dict() if hasattr(updated_config, 'to_dict') else config_dict\n    except Exception:\n        # If conversion fails, return original config with updates applied directly\n        result = config_dict.copy()\n        result.update(updates)\n        return result\n\n\n# Export classes and functions\n__all__ = [\n    "ConfigValidator", "ConfigSourceManager", "UnifiedConfigManager",\n    "get_default_config", "create_unified_config_manager",\n    "get_legacy_config", "update_legacy_config"\n]