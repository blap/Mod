"""\nVision encoder component for Qwen3-VL.\n\nThis module implements the vision processing component of the Qwen3-VL architecture.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\nfrom qwen3_vl.config.base_config import Qwen3VLConfig\n\n\nclass Qwen3VLVisionEncoder(nn.Module):\n    """\n    Vision encoder component of Qwen3-VL.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n\n        # Initialize vision model parameters based on config\n        self.hidden_size = config.vision_hidden_size\n        self.num_hidden_layers = config.vision_num_hidden_layers\n        self.num_attention_heads = config.vision_num_attention_heads\n        self.patch_size = config.vision_patch_size\n        self.image_size = config.vision_image_size\n        self.num_channels = config.vision_num_channels\n\n        # Calculate number of patches\n        self.num_patches = (self.image_size // self.patch_size) ** 2\n        \n        # Patch embedding layer\n        self.patch_embedding = nn.Conv2d(\n            self.num_channels,\n            self.hidden_size,\n            kernel_size=self.patch_size,\n            stride=self.patch_size,\n            bias=False\n        )\n        \n        # Positional embedding for patches\n        self.position_embedding = nn.Embedding(self.num_patches + 1, self.hidden_size)  # +1 for CLS token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_size))\n        \n        # Layer normalization\n        self.layernorm = nn.LayerNorm(self.hidden_size, eps=config.layer_norm_eps)\n        \n        # Transformer layers for vision processing\n        self.vision_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=self.hidden_size,\n                nhead=self.num_attention_heads,\n                dim_feedforward=self.hidden_size * 4,\n                dropout=0.0,\n                activation='gelu',\n                batch_first=True\n            ) for _ in range(self.num_hidden_layers)\n        ])\n\n    def forward(\n        self,\n        pixel_values: torch.FloatTensor,\n        **kwargs\n    ):\n        """\n        Forward pass of the vision encoder.\n\n        Args:\n            pixel_values: Input pixel values\n            **kwargs: Additional arguments\n\n        Returns:\n            Vision encoder outputs\n        """\n        batch_size = pixel_values.size(0)\n        \n        # Extract patches\n        patches = self.patch_embedding(pixel_values)  # [batch_size, hidden_size, height, width]\n        patches = patches.flatten(2).transpose(1, 2)  # [batch_size, num_patches, hidden_size]\n        \n        # Add CLS token\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        patches = torch.cat([cls_tokens, patches], dim=1)  # [batch_size, num_patches + 1, hidden_size]\n        \n        # Add positional embeddings\n        positions = torch.arange(0, patches.size(1), device=patches.device).unsqueeze(0).expand(batch_size, -1)\n        position_embeddings = self.position_embedding(positions)\n        patches = patches + position_embeddings\n        \n        # Apply layer norm\n        patches = self.layernorm(patches)\n        \n        # Apply transformer layers\n        for layer in self.vision_layers:\n            patches = layer(patches)\n        \n        return patches