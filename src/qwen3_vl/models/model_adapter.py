"""\nModel adapter layer that provides a unified interface across different models.\n\nThis module provides a model adapter layer that offers a unified interface across different models.\n"""\n\nimport abc\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, Any, Optional, Union, List, Tuple\nimport logging\nfrom transformers import PreTrainedModel, PretrainedConfig\n\n\nclass IModelAdapter(abc.ABC):\n    """Interface for model adapters that provide a unified interface."""\n    \n    @property\n    @abc.abstractmethod\n    def model(self) -> nn.Module:\n        """Get the underlying model."""\n        pass\n    \n    @abc.abstractmethod\n    def forward(self, *args, **kwargs):\n        """Forward pass of the model."""\n        pass\n    \n    @abc.abstractmethod\n    def generate(self, *args, **kwargs):\n        """Generate text using the model."""\n        pass\n    \n    @abc.abstractmethod\n    def load_model(self, model_path: str, config: Optional[Dict[str, Any]] = None) -> 'IModelAdapter':\n        """Load the model from a path."""\n        pass\n    \n    @abc.abstractmethod\n    def get_config(self) -> Dict[str, Any]:\n        """Get the model configuration."""\n        pass\n    \n    @abc.abstractmethod\n    def get_tokenizer(self):\n        """Get the tokenizer associated with the model."""\n        pass\n\n\nclass BaseModelAdapter(IModelAdapter):\n    """Base implementation of model adapter."""\n    \n    def __init__(self, model: nn.Module, config: Optional[Dict[str, Any]] = None, tokenizer=None):\n        self._model = model\n        self._config = config or {}\n        self._tokenizer = tokenizer\n        self._logger = logging.getLogger(f"{self.__class__.__module__}.{self.__class__.__name__}")\n    \n    @property\n    def model(self) -> nn.Module:\n        """Get the underlying model."""\n        return self._model\n    \n    def forward(self, *args, **kwargs):\n        """Forward pass implementation."""\n        return self._model(*args, **kwargs)\n    \n    def generate(self, *args, **kwargs):\n        """Generation implementation."""\n        # Default implementation - may need to be overridden by specific adapters\n        if hasattr(self._model, 'generate'):\n            return self._model.generate(*args, **kwargs)\n        else:\n            raise AttributeError("The underlying model does not have a 'generate' method. "\n                               "This model adapter cannot perform text generation.")\n    \n    def load_model(self, model_path: str, config: Optional[Dict[str, Any]] = None) -> 'BaseModelAdapter':\n        """Load model implementation - should be overridden."""\n        raise RuntimeError("BaseModelAdapter.load_model() is an abstract method that must be "\n                         "implemented by subclasses. Use a specific model adapter instead.")\n    \n    def get_config(self) -> Dict[str, Any]:\n        """Get configuration implementation."""\n        return self._config\n    \n    def get_tokenizer(self):\n        """Get the tokenizer."""\n        return self._tokenizer\n    \n    def to(self, device: Union[str, torch.device]):\n        """Move model to specified device."""\n        self._model = self._model.to(device)\n        return self\n    \n    def eval(self):\n        """Set model to evaluation mode."""\n        self._model.eval()\n        return self\n    \n    def train(self):\n        """Set model to training mode."""\n        self._model.train()\n        return self\n\n\nclass Qwen3VLAdapter(BaseModelAdapter):\n    """Adapter for Qwen3-VL models."""\n    \n    def load_model(self, model_path: str, config: Optional[Dict[str, Any]] = None) -> 'Qwen3VLAdapter':\n        """Load Qwen3-VL model from path."""\nfrom models.base_model import create_model_from_pretrained\nfrom qwen3_vl.config.base_config import Qwen3VLConfig\n        \n        # Create config if not provided\n        model_config = None\n        if config:\n            model_config = Qwen3VLConfig(**config)\n        \n        model = create_model_from_pretrained(model_path, model_config)\n        \n        # For now, we're not handling tokenizer in this simplified version\n        return Qwen3VLAdapter(model, config)\n    \n    def generate(\n        self,\n        input_ids: torch.LongTensor,\n        max_length: int = 512,\n        temperature: float = 1.0,\n        do_sample: bool = True,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        **kwargs\n    ):\n        """Generate text using Qwen3-VL model."""\n        if hasattr(self._model, 'generate'):\n            return self._model.generate(\n                input_ids=input_ids,\n                max_length=max_length,\n                temperature=temperature,\n                do_sample=do_sample,\n                pad_token_id=pad_token_id,\n                eos_token_id=eos_token_id,\n                **kwargs\n            )\n        else:\n            # Fallback to base implementation if generate method doesn't exist\n            raise AttributeError("The Qwen3-VL model does not have a 'generate' method or it's not accessible. "\n                               "Check that the model was loaded correctly.")\n\n\nclass HuggingFaceAdapter(BaseModelAdapter):\n    """Adapter for Hugging Face models."""\n    \n    def load_model(self, model_path: str, config: Optional[Dict[str, Any]] = None) -> 'HuggingFaceAdapter':\n        """Load Hugging Face model from path."""\n        from transformers import AutoModel, AutoConfig, AutoTokenizer\n        \n        config = config or {}\n        \n        # Load configuration\n        model_config = AutoConfig.from_pretrained(model_path, **config)\n        \n        # Load model\n        model = AutoModel.from_pretrained(model_path, config=model_config)\n        \n        # Load tokenizer\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(model_path)\n        except:\n            tokenizer = None  # Tokenizer might not be available\n        \n        return HuggingFaceAdapter(model, config, tokenizer)\n    \n    def generate(\n        self,\n        input_ids: torch.LongTensor,\n        max_length: int = 512,\n        temperature: float = 1.0,\n        do_sample: bool = True,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        **kwargs\n    ):\n        """Generate text using Hugging Face model."""\n        if hasattr(self._model, 'generate'):\n            generation_kwargs = {\n                'max_length': max_length,\n                'temperature': temperature,\n                'do_sample': do_sample,\n                **kwargs\n            }\n            \n            if pad_token_id is not None:\n                generation_kwargs['pad_token_id'] = pad_token_id\n            if eos_token_id is not None:\n                generation_kwargs['eos_token_id'] = eos_token_id\n            \n            return self._model.generate(input_ids, **generation_kwargs)\n        else:\n            raise AttributeError("The Hugging Face model does not have a 'generate' method or it's not accessible. "\n                               "Check that the model was loaded correctly.")\n\n\nclass ModelAdapterManager:\n    """Manager for model adapters."""\n    \n    def __init__(self):\n        self._logger = logging.getLogger(f"{self.__class__.__module__}.{self.__class__.__name__}")\n        self._adapters: Dict[str, type] = {\n            "qwen3-vl": Qwen3VLAdapter,\n            "huggingface": HuggingFaceAdapter,\n            "default": BaseModelAdapter\n        }\n    \n    def register_adapter(self, name: str, adapter_class: type) -> bool:\n        """\n        Register a new adapter type.\n        \n        Args:\n            name: Name of the adapter type\n            adapter_class: Adapter class to register\n            \n        Returns:\n            True if registration was successful, False otherwise\n        """\n        try:\n            self._adapters[name.lower()] = adapter_class\n            self._logger.info(f"Registered adapter: {name}")\n            return True\n        except Exception as e:\n            self._logger.error(f"Error registering adapter {name}: {e}")\n            return False\n    \n    def create_adapter(\n        self,\n        model: nn.Module,\n        adapter_type: str = "default",\n        config: Optional[Dict[str, Any]] = None,\n        tokenizer = None\n    ) -> IModelAdapter:\n        """\n        Create an adapter for a model.\n        \n        Args:\n            model: Model to create adapter for\n            adapter_type: Type of adapter to create\n            config: Configuration for the adapter\n            tokenizer: Tokenizer to associate with the adapter\n            \n        Returns:\n            IModelAdapter instance\n        """\n        adapter_type = adapter_type.lower()\n        \n        if adapter_type not in self._adapters:\n            self._logger.warning(f"Unknown adapter type: {adapter_type}, using default")\n            adapter_type = "default"\n        \n        adapter_class = self._adapters[adapter_type]\n        return adapter_class(model, config, tokenizer)\n    \n    def get_adapter_class(self, adapter_type: str) -> Optional[type]:\n        """\n        Get the class for a specific adapter type.\n        \n        Args:\n            adapter_type: Type of adapter\n            \n        Returns:\n            Adapter class if found, None otherwise\n        """\n        return self._adapters.get(adapter_type.lower())\n    \n    def get_supported_adapters(self) -> List[str]:\n        """\n        Get list of supported adapter types.\n        \n        Returns:\n            List of supported adapter types\n        """\n        return list(self._adapters.keys())\n\n\nclass UnifiedModelInterface:\n    """Unified interface that works with any model through adapters."""\n    \n    def __init__(self, adapter: IModelAdapter):\n        self.adapter = adapter\n        self._logger = logging.getLogger(f"{self.__class__.__module__}.{self.__class__.__name__}")\n    \n    def forward(self, *args, **kwargs):\n        """Forward pass through the model."""\n        return self.adapter.forward(*args, **kwargs)\n    \n    def generate(self, *args, **kwargs):\n        """Generate output from the model."""\n        return self.adapter.generate(*args, **kwargs)\n    \n    def get_model(self) -> nn.Module:\n        """Get the underlying model."""\n        return self.adapter.model\n    \n    def get_config(self) -> Dict[str, Any]:\n        """Get the model configuration."""\n        return self.adapter.get_config()\n    \n    def to(self, device: Union[str, torch.device]):\n        """Move model to specified device."""\n        self.adapter.to(device)\n        return self\n    \n    def eval(self):\n        """Set model to evaluation mode."""\n        self.adapter.eval()\n        return self\n    \n    def train(self):\n        """Set model to training mode."""\n        self.adapter.train()\n        return self\n    \n    def save(self, save_path: str):\n        """Save the model."""\n        torch.save(self.adapter.model.state_dict(), save_path)\n        self._logger.info(f"Model saved to {save_path}")\n    \n    def load(self, load_path: str):\n        """Load the model."""\n        self.adapter.model.load_state_dict(torch.load(load_path))\n        self._logger.info(f"Model loaded from {load_path}")\n\n\n# Global adapter manager instance\nadapter_manager = ModelAdapterManager()\n\n\ndef get_adapter_manager() -> ModelAdapterManager:\n    """\n    Get the global adapter manager instance.\n    \n    Returns:\n        ModelAdapterManager instance\n    """\n    return adapter_manager\n\n\ndef create_unified_interface(\n    model: nn.Module,\n    model_type: str = "default",\n    config: Optional[Dict[str, Any]] = None\n) -> UnifiedModelInterface:\n    """\n    Create a unified interface for a model.\n    \n    Args:\n        model: Model to create interface for\n        model_type: Type of model ("qwen3-vl", "huggingface", "default")\n        config: Configuration for the model\n        \n    Returns:\n        UnifiedModelInterface instance\n    """\n    adapter = adapter_manager.create_adapter(model, model_type, config)\n    return UnifiedModelInterface(adapter)