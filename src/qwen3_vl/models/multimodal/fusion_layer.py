"""\nMultimodal fusion layer for Qwen3-VL.\n\nThis module implements the fusion mechanism that combines vision and language modalities.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\nfrom qwen3_vl.config.base_config import Qwen3VLConfig\n\n\nclass Qwen3VLMultimodalFusion(nn.Module):\n    """\n    Multimodal fusion layer for Qwen3-VL.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n\n        # Projection dimensions\n        self.vision_projection_dim = config.vision_projection_dim\n        self.language_projection_dim = config.language_projection_dim\n        self.hidden_size = config.hidden_size\n        \n        # Number of query tokens for vision-language fusion\n        self.num_query_tokens = config.num_query_tokens\n        \n        # Query tokens for multimodal fusion\n        self.query_tokens = nn.Parameter(torch.zeros(1, self.num_query_tokens, self.hidden_size))\n        \n        # Linear projections for vision and language features\n        self.vision_projection = nn.Linear(config.vision_hidden_size, self.hidden_size, bias=False)\n        self.language_projection = nn.Linear(config.hidden_size, self.hidden_size, bias=False)\n        \n        # Transformer layer for multimodal fusion\n        self.fusion_layer = nn.TransformerEncoderLayer(\n            d_model=self.hidden_size,\n            nhead=config.num_attention_heads // 4,  # Using fewer heads for fusion\n            dim_feedforward=self.hidden_size * 2,\n            dropout=0.0,\n            activation='gelu',\n            batch_first=True\n        )\n        \n        # Layer normalization\n        self.layernorm = nn.LayerNorm(self.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        vision_features: torch.FloatTensor,\n        language_features: torch.FloatTensor,\n        **kwargs\n    ):\n        """\n        Forward pass of the multimodal fusion layer.\n\n        Args:\n            vision_features: Features from vision encoder\n            language_features: Features from language model\n            **kwargs: Additional arguments\n\n        Returns:\n            Fused multimodal features\n        """\n        batch_size = vision_features.size(0)\n        \n        # Project vision features to common space\n        projected_vision = self.vision_projection(vision_features)\n        \n        # Project language features to common space\n        projected_language = self.language_projection(language_features)\n        \n        # Expand query tokens to batch size\n        query_tokens = self.query_tokens.expand(batch_size, -1, -1)\n        \n        # Concatenate all features: [query_tokens, projected_vision, projected_language]\n        fused_features = torch.cat([\n            query_tokens,\n            projected_vision,\n            projected_language\n        ], dim=1)\n        \n        # Apply fusion transformer layer\n        fused_features = self.fusion_layer(fused_features)\n        \n        # Apply layer norm\n        fused_features = self.layernorm(fused_features)\n        \n        return fused_features