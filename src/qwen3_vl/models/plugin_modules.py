"""\nPlug-in modules for efficient fine-tuning of Qwen3-VL model.\nThese modules provide modular, pluggable components for adaptation.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, Optional, Any, List, Union, Callable, Tuple\nfrom dataclasses import dataclass\nimport os\nimport json\nimport copy\nfrom models.adapter_layers import AdapterConfig, BottleneckAdapter, LoraLinear, AdapterLayer\nfrom config.hardware_config import HardwareConfig\nfrom models.device_specific_adapters import DeviceSpecificAdapter\n\n\n@dataclass\nclass PluginConfig:\n    """\n    Configuration for plug-in modules.\n    """\n    # Adapter configuration\n    adapter_config: Optional[AdapterConfig] = None\n    \n    # Hardware configuration\n    hardware_config: Optional[HardwareConfig] = None\n    \n    # Plugin-specific configuration\n    plugin_type: str = "adapter"  # "adapter", "lora", "prefix", "ia3", etc.\n    plugin_name: str = "default"\n    is_enabled: bool = True\n    is_trainable: bool = True\n    \n    # Integration settings\n    layer_positions: List[str] = None  # Where to insert the plugin ("attn", "mlp", "both")\n    scale_factor: float = 1.0  # Scaling factor for plugin output\n\n\nclass PluginBase(nn.Module):\n    """\n    Base class for all plug-in modules.\n    Provides common functionality for different types of parameter-efficient modules.\n    """\n    def __init__(self, config: PluginConfig):\n        super().__init__()\n        self.config = config\n        self.plugin_type = config.plugin_type\n        self.plugin_name = config.plugin_name\n        self.is_enabled = config.is_enabled\n        self.is_trainable = config.is_trainable\n        self.scale_factor = config.scale_factor\n        \n        # Initialize the specific plugin implementation\n        self._init_plugin()\n    \n    def _init_plugin(self):\n        """Initialize the specific plugin implementation."""\n        # Default implementation - subclasses should override\n        pass\n\n    def forward(self, *args, **kwargs):\n        """Forward pass for the plugin."""\n        # Default implementation - subclasses should override\n        if args:\n            return args[0]  # Return first argument as default\n        return None\n    \n    def enable(self):\n        """Enable the plugin."""\n        self.is_enabled = True\n    \n    def disable(self):\n        """Disable the plugin."""\n        self.is_enabled = False\n    \n    def freeze(self):\n        """Freeze plugin parameters."""\n        for param in self.parameters():\n            param.requires_grad = False\n        self.is_trainable = False\n    \n    def unfreeze(self):\n        """Unfreeze plugin parameters."""\n        for param in self.parameters():\n            param.requires_grad = True\n        self.is_trainable = True\n\n\nclass AdapterPlugin(PluginBase):\n    """\n    Adapter plug-in module that wraps adapter functionality.\n    """\n    def __init__(self, config: PluginConfig, input_dim: int):\n        super().__init__(config)\n        self.input_dim = input_dim\n        self._init_plugin()  # Call initialization after parent init\n    \n    def _init_plugin(self):\n        """Initialize the adapter plugin."""\n        adapter_config = self.config.adapter_config or AdapterConfig()\n        \n        # Use device-specific adapter if hardware config is provided\n        if self.config.hardware_config:\n            self.adapter = DeviceSpecificAdapter(\n                adapter_config, \n                self.config.hardware_config, \n                self.input_dim\n            )\n        else:\n            self.adapter = BottleneckAdapter(adapter_config, self.input_dim)\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass through the adapter plugin."""\n        if not self.is_enabled:\n            return hidden_states\n        \n        output = self.adapter(hidden_states)\n        \n        # Apply scaling factor\n        if self.scale_factor != 1.0:\n            output = hidden_states + (output - hidden_states) * self.scale_factor\n        \n        return output\n\n\nclass LoraPlugin(PluginBase):\n    """\n    LoRA (Low-Rank Adaptation) plug-in module.\n    """\n    def __init__(self, config: PluginConfig, base_layer: nn.Linear):\n        super().__init__(config)\n        self.base_layer = base_layer\n        self.original_forward = base_layer.forward  # Store original forward method\n\n        # Initialize LoRA components after parent init\n        adapter_config = self.config.adapter_config or AdapterConfig()\n        self.lora = LoraLinear(self.base_layer, adapter_config)\n\n        # Replace the forward method of the base layer to use LoRA\n        self.base_layer.forward = self._lora_forward\n    \n    def _init_plugin(self):\n        """Initialize the LoRA plugin."""\n        # PluginBase handles the main initialization\n        # This method is kept for compatibility but LoraPlugin is initialized in __init__\n        pass\n    \n    def _lora_forward(self, x: torch.Tensor) -> torch.Tensor:\n        """Forward method that applies LoRA transformation."""\n        if not self.is_enabled:\n            return self.original_forward(x)\n        \n        # Apply LoRA transformation\n        return self.lora(x)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """Forward pass - for compatibility, this calls the modified base layer."""\n        if not self.is_enabled:\n            return self.original_forward(x)\n        \n        return self.lora(x)\n\n\nclass PrefixTuningPlugin(PluginBase):\n    """\n    Prefix tuning plug-in module.\n    """\n    def __init__(self, config: PluginConfig, num_layers: int, num_heads: int, head_dim: int):\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.head_dim = head_dim\n        super().__init__(config)\n    \n    def _init_plugin(self):\n        """Initialize the prefix tuning plugin."""\n        # Initialize prefix parameters\n        prefix_length = 10  # Common prefix length\n        self.prefix_key = nn.Parameter(torch.randn(self.num_layers, 1, prefix_length, self.num_heads, self.head_dim))\n        self.prefix_value = nn.Parameter(torch.randn(self.num_layers, 1, prefix_length, self.num_heads, self.head_dim))\n        \n        # Initialize with small random values\n        nn.init.normal_(self.prefix_key, std=0.02)\n        nn.init.normal_(self.prefix_value, std=0.02)\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass for prefix tuning."""\n        if not self.is_enabled:\n            return hidden_states\n        \n        # In a real implementation, this would prepend the prefix to the attention mechanism\n        # This is a simplified version\n        return hidden_states\n\n\nclass IA3Plugin(PluginBase):\n    """\n    IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations) plug-in.\n    """\n    def __init__(self, config: PluginConfig, input_dim: int):\n        self.input_dim = input_dim\n        super().__init__(config)\n    \n    def _init_plugin(self):\n        """Initialize the IA3 plugin."""\n        # IA3 learns activation vectors that scale the internal activations\n        self.ia3_key = nn.Parameter(torch.ones(self.input_dim))\n        self.ia3_value = nn.Parameter(torch.ones(self.input_dim))\n        self.ia3_intermediate = nn.Parameter(torch.ones(self.input_dim))\n        \n        # Initialize with small values close to 1\n        nn.init.ones_(self.ia3_key)\n        nn.init.ones_(self.ia3_value)\n        nn.init.ones_(self.ia3_intermediate)\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass for IA3."""\n        if not self.is_enabled:\n            return hidden_states\n        \n        # Apply IA3 scaling to hidden states\n        # This is a simplified version - in practice, IA3 applies to specific operations\n        scaled_states = hidden_states * self.ia3_key\n        return scaled_states\n\n\nclass PluginManager:\n    """\n    Manager for multiple plug-in modules.\n    Handles registration, activation, and coordination of different plugins.\n    """\n    def __init__(self):\n        self.plugins: Dict[str, PluginBase] = {}\n        self.active_plugins: List[str] = []\n        self.plugin_configs: Dict[str, PluginConfig] = {}\n    \n    def register_plugin(self, name: str, plugin: PluginBase):\n        """Register a new plugin."""\n        self.plugins[name] = plugin\n        self.plugin_configs[name] = plugin.config\n    \n    def get_plugin(self, name: str) -> Optional[PluginBase]:\n        """Get a registered plugin by name."""\n        return self.plugins.get(name)\n    \n    def activate_plugin(self, name: str):\n        """Activate a plugin."""\n        if name in self.plugins and name not in self.active_plugins:\n            self.plugins[name].enable()\n            self.active_plugins.append(name)\n    \n    def deactivate_plugin(self, name: str):\n        """Deactivate a plugin."""\n        if name in self.active_plugins:\n            self.plugins[name].disable()\n            self.active_plugins.remove(name)\n    \n    def apply_plugins(self, hidden_states: torch.Tensor, plugin_names: List[str] = None) -> torch.Tensor:\n        """Apply active plugins to the hidden states."""\n        output = hidden_states\n        \n        # Determine which plugins to apply\n        plugins_to_apply = plugin_names if plugin_names else self.active_plugins\n        \n        for plugin_name in plugins_to_apply:\n            if plugin_name in self.plugins:\n                plugin = self.plugins[plugin_name]\n                if plugin.is_enabled:\n                    output = plugin(output)\n        \n        return output\n    \n    def freeze_all_plugins(self):\n        """Freeze all registered plugins."""\n        for plugin in self.plugins.values():\n            plugin.freeze()\n    \n    def unfreeze_all_plugins(self):\n        """Unfreeze all registered plugins."""\n        for plugin in self.plugins.values():\n            plugin.unfreeze()\n    \n    def get_trainable_parameters(self) -> List[nn.Parameter]:\n        """Get all trainable parameters from active plugins."""\n        trainable_params = []\n        for plugin in self.plugins.values():\n            if plugin.is_trainable:\n                trainable_params.extend([p for p in plugin.parameters() if p.requires_grad])\n        return trainable_params\n    \n    def save_plugins(self, save_dir: str):\n        """Save plugin states to a directory."""\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # Save plugin configurations\n        config_path = os.path.join(save_dir, "plugin_configs.json")\n        config_dict = {}\n        for name, config in self.plugin_configs.items():\n            config_dict[name] = {\n                'plugin_type': config.plugin_type,\n                'plugin_name': config.plugin_name,\n                'is_enabled': config.is_enabled,\n                'is_trainable': config.is_trainable,\n                'scale_factor': config.scale_factor,\n                'layer_positions': config.layer_positions\n            }\n        \n        with open(config_path, 'w') as f:\n            json.dump(config_dict, f, indent=2)\n        \n        # Save plugin parameters\n        for name, plugin in self.plugins.items():\n            plugin_path = os.path.join(save_dir, f"{name}_weights.pt")\n            torch.save(plugin.state_dict(), plugin_path)\n    \n    def load_plugins(self, load_dir: str):\n        """Load plugin states from a directory."""\n        # Load plugin configurations\n        config_path = os.path.join(load_dir, "plugin_configs.json")\n        if os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                config_dict = json.load(f)\n            \n            for name, config_data in config_dict.items():\n                # Update plugin configurations\n                if name in self.plugin_configs:\n                    config = self.plugin_configs[name]\n                    config.is_enabled = config_data['is_enabled']\n                    config.is_trainable = config_data['is_trainable']\n                    config.scale_factor = config_data['scale_factor']\n                    config.layer_positions = config_data['layer_positions']\n        \n        # Load plugin parameters\n        for name, plugin in self.plugins.items():\n            plugin_path = os.path.join(load_dir, f"{name}_weights.pt")\n            if os.path.exists(plugin_path):\n                plugin.load_state_dict(torch.load(plugin_path))\n\n\nclass ModularAdapterLayer(nn.Module):\n    """\n    A modular adapter layer that can incorporate different types of plugins.\n    """\n    def __init__(self, config: PluginConfig, input_dim: int):\n        super().__init__()\n        self.config = config\n        self.input_dim = input_dim\n        \n        # Create the main adapter plugin\n        self.main_plugin = self._create_plugin(config, input_dim)\n        \n        # Create a plugin manager for additional plugins\n        self.plugin_manager = PluginManager()\n        self.plugin_manager.register_plugin("main", self.main_plugin)\n    \n    def _create_plugin(self, config: PluginConfig, input_dim: int) -> PluginBase:\n        """Create the appropriate plugin based on configuration."""\n        if config.plugin_type == "adapter":\n            return AdapterPlugin(config, input_dim)\n        elif config.plugin_type == "lora":\n            # For LoRA, we need a base layer, so we create a dummy one\n            dummy_layer = nn.Linear(input_dim, input_dim)\n            return LoraPlugin(config, dummy_layer)\n        elif config.plugin_type == "prefix_tuning":\n            # This would need additional parameters for a real implementation\n            return PrefixTuningPlugin(config, num_layers=1, num_heads=8, head_dim=input_dim//8)\n        elif config.plugin_type == "ia3":\n            return IA3Plugin(config, input_dim)\n        else:\n            # Default to adapter\n            return AdapterPlugin(config, input_dim)\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass through the modular adapter layer."""\n        return self.main_plugin(hidden_states)\n\n\nclass TaskAdapter(nn.Module):\n    """\n    Task-specific adapter that can switch between different task configurations.\n    """\n    def __init__(self, base_config: PluginConfig, task_configs: Dict[str, PluginConfig]):\n        super().__init__()\n        self.base_config = base_config\n        \n        # Create adapters for each task\n        self.task_adapters = nn.ModuleDict()\n        \n        # Create base adapter\n        self.base_adapter = ModularAdapterLayer(base_config, base_config.adapter_config.hidden_size if hasattr(base_config.adapter_config, 'hidden_size') else 2048)\n        \n        # Create task-specific adapters\n        for task_name, task_config in task_configs.items():\n            self.task_adapters[task_name] = ModularAdapterLayer(task_config, \n                                                              task_config.adapter_config.hidden_size if hasattr(task_config.adapter_config, 'hidden_size') else 2048)\n        \n        self.active_task = None\n    \n    def set_active_task(self, task_name: str):\n        """Set the active task."""\n        self.active_task = task_name\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass with task-specific adaptation."""\n        if self.active_task and self.active_task in self.task_adapters:\n            return self.task_adapters[self.active_task](hidden_states)\n        else:\n            return self.base_adapter(hidden_states)\n\n\nclass DownstreamTaskAdapter(nn.Module):\n    """\n    Adapter specifically designed for downstream task adaptation.\n    """\n    def __init__(self, config: PluginConfig, task_type: str = "classification"):\n        super().__init__()\n        self.config = config\n        self.task_type = task_type\n        \n        # Create task-specific head based on task type\n        if task_type == "classification":\n            self.task_head = nn.Linear(config.adapter_config.hidden_size if hasattr(config.adapter_config, 'hidden_size') else 2048, \n                                     config.num_labels if hasattr(config, 'num_labels') else 2)\n        elif task_type == "regression":\n            self.task_head = nn.Linear(config.adapter_config.hidden_size if hasattr(config.adapter_config, 'hidden_size') else 2048, 1)\n        elif task_type == "generation":\n            self.task_head = nn.Linear(config.adapter_config.hidden_size if hasattr(config.adapter_config, 'hidden_size') else 2048, \n                                     config.vocab_size if hasattr(config, 'vocab_size') else 50257)\n        else:\n            # Default to classification\n            self.task_head = nn.Linear(config.adapter_config.hidden_size if hasattr(config.adapter_config, 'hidden_size') else 2048, 2)\n        \n        # Initialize task head\n        nn.init.normal_(self.task_head.weight, std=0.02)\n        if self.task_head.bias is not None:\n            nn.init.zeros_(self.task_head.bias)\n        \n        # Create the main adapter\n        self.main_adapter = ModularAdapterLayer(config, \n                                              config.adapter_config.hidden_size if hasattr(config.adapter_config, 'hidden_size') else 2048)\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass for downstream task adaptation."""\n        # Apply main adapter\n        adapted_states = self.main_adapter(hidden_states)\n        \n        # Apply task-specific head\n        task_output = self.task_head(adapted_states)\n        \n        return task_output\n\n\nclass FusionAdapter(nn.Module):\n    """\n    Adapter that fuses multiple adaptation techniques.\n    """\n    def __init__(self, configs: List[PluginConfig], input_dim: int):\n        super().__init__()\n        self.adapters = nn.ModuleList()\n        \n        for config in configs:\n            adapter = ModularAdapterLayer(config, input_dim)\n            self.adapters.append(adapter)\n        \n        # Learnable fusion weights\n        self.fusion_weights = nn.Parameter(torch.ones(len(configs)) / len(configs))\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass with fused adaptations."""\n        outputs = []\n        \n        for i, adapter in enumerate(self.adapters):\n            output = adapter(hidden_states)\n            # Only take the delta from the original\n            outputs.append(output - hidden_states)\n        \n        # Apply fusion weights\n        weighted_outputs = []\n        for i, output in enumerate(outputs):\n            weighted_outputs.append(output * self.fusion_weights[i])\n        \n        # Sum all weighted outputs and add back to original\n        fused_output = hidden_states\n        for weighted_output in weighted_outputs:\n            fused_output = fused_output + weighted_output\n        \n        return fused_output\n\n\ndef create_plugin_from_config(config: PluginConfig, **kwargs) -> PluginBase:\n    """\n    Factory function to create a plugin from configuration.\n    """\n    if config.plugin_type == "adapter":\n        input_dim = kwargs.get('input_dim', 2048)\n        return AdapterPlugin(config, input_dim)\n    elif config.plugin_type == "lora":\n        base_layer = kwargs.get('base_layer')\n        if base_layer is None:\n            raise ValueError("base_layer is required for LoRA plugin")\n        return LoraPlugin(config, base_layer)\n    elif config.plugin_type == "prefix_tuning":\n        num_layers = kwargs.get('num_layers', 1)\n        num_heads = kwargs.get('num_heads', 8)\n        head_dim = kwargs.get('head_dim', 64)\n        return PrefixTuningPlugin(config, num_layers, num_heads, head_dim)\n    elif config.plugin_type == "ia3":\n        input_dim = kwargs.get('input_dim', 2048)\n        return IA3Plugin(config, input_dim)\n    else:\n        raise ValueError(f"Unknown plugin type: {config.plugin_type}")