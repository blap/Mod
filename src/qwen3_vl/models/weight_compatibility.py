"""\nCompatibility layer to ensure adapter implementations maintain compatibility\nwith original Qwen3-VL model weights.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, Optional, Any, Tuple, List\nimport copy\nfrom models.modeling_qwen3_vl import Qwen3VLForConditionalGeneration, Qwen3VLConfig\nfrom models.adapter_layers import AdapterConfig, BottleneckAdapter, LoraLinear\nfrom models.plugin_modules import PluginConfig, ModularAdapterLayer\nfrom models.device_specific_adapters import DeviceAdapterFactory, DeviceAdapterConfig\nfrom models.downstream_task_adaptation import TaskAdaptedModel, TaskConfig\n\n\nclass WeightCompatibilityManager:\n    """\n    Manager to ensure adapter implementations maintain compatibility with original weights.\n    """\n    def __init__(self, base_model: nn.Module):\n        self.base_model = base_model\n        self.original_state_dict = copy.deepcopy(base_model.state_dict())\n        self.adapter_state_dicts = {}\n    \n    def register_adapter_weights(self, adapter_name: str, adapter_module: nn.Module):\n        """Register adapter weights to track alongside original weights."""\n        self.adapter_state_dicts[adapter_name] = copy.deepcopy(adapter_module.state_dict())\n    \n    def validate_weight_compatibility(self) -> bool:\n        """Validate that original weights remain unchanged."""\n        current_state_dict = self.base_model.state_dict()\n        \n        # Check that original weights haven't been modified\n        for name, original_param in self.original_state_dict.items():\n            if name in current_state_dict:\n                current_param = current_state_dict[name]\n                if not torch.equal(original_param, current_param):\n                    print(f"Weight mismatch in {name}")\n                    return False\n        \n        return True\n    \n    def get_original_weights(self) -> Dict[str, torch.Tensor]:\n        """Get the original model weights."""\n        return self.original_state_dict\n    \n    def get_adapter_weights(self, adapter_name: str) -> Optional[Dict[str, torch.Tensor]]:\n        """Get weights for a specific adapter."""\n        return self.adapter_state_dicts.get(adapter_name)\n\n\nclass AdapterIntegrationWrapper(nn.Module):\n    """\n    Wrapper that integrates adapters while maintaining original model structure.\n    """\n    def __init__(self, base_model: Qwen3VLForConditionalGeneration, adapter_configs: Dict[str, Any] = None):\n        super().__init__()\n        self.base_model = base_model\n        self.config = base_model.config  # Maintain original config\n        \n        # Store original forward method\n        self._original_forward = base_model.forward\n        \n        # Initialize adapters\n        self.adapters = nn.ModuleDict()\n        self.adapter_configs = adapter_configs or {}\n        \n        # Apply adapters to appropriate layers\n        self._apply_adapters()\n    \n    def _apply_adapters(self):\n        """Apply adapters to the base model without changing its structure."""\n        # Add adapters to transformer layers\n        for i, layer in enumerate(self.base_model.language_model.layers):\n            layer_name = f"decoder_layer_{i}"\n            \n            # Add adapter to attention output\n            if f"{layer_name}_attn" in self.adapter_configs:\n                attn_config = self.adapter_configs[f"{layer_name}_attn"]\n                attn_adapter = BottleneckAdapter(attn_config, self.config.hidden_size)\n                layer.attn_adapter = attn_adapter\n                self.adapters[f"{layer_name}_attn"] = attn_adapter\n            else:\n                layer.attn_adapter = nn.Identity()\n            \n            # Add adapter to MLP output\n            if f"{layer_name}_mlp" in self.adapter_configs:\n                mlp_config = self.adapter_configs[f"{layer_name}_mlp"]\n                mlp_adapter = BottleneckAdapter(mlp_config, self.config.hidden_size)\n                layer.mlp_adapter = mlp_adapter\n                self.adapters[f"{layer_name}_mlp"] = mlp_adapter\n            else:\n                layer.mlp_adapter = nn.Identity()\n    \n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        pixel_values: torch.FloatTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[list] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs\n    ) -> torch.Tensor:\n        """Forward pass with adapter integration."""\n        # Call the original model's forward method\n        output = self._original_forward(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        \n        return output\n    \n    def generate(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        max_new_tokens: int = 50,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        **kwargs\n    ):\n        """Generate with adapter integration."""\n        # Use the base model's generate method\n        return self.base_model.generate(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            **kwargs\n        )\n    \n    def state_dict(self, destination=None, prefix='', keep_vars=False):\n        """Custom state dict that includes both base model and adapters."""\n        # Get base model state dict\n        base_state_dict = self.base_model.state_dict(destination, prefix, keep_vars)\n        \n        # Add adapter state dict\n        adapter_state_dict = {}\n        for name, adapter in self.adapters.items():\n            adapter_dict = adapter.state_dict()\n            for param_name, param in adapter_dict.items():\n                full_name = f"adapters.{name}.{param_name}"\n                adapter_state_dict[full_name] = param\n        \n        # Combine both\n        combined_state_dict = {**base_state_dict, **adapter_state_dict}\n        return combined_state_dict\n    \n    def load_state_dict(self, state_dict, strict=True):\n        """Custom load state dict that handles both base model and adapters."""\n        # Separate base model and adapter state dicts\n        base_state_dict = {}\n        adapter_state_dict = {}\n        \n        for name, param in state_dict.items():\n            if name.startswith("adapters."):\n                adapter_state_dict[name] = param\n            else:\n                base_state_dict[name] = param\n        \n        # Load base model weights\n        self.base_model.load_state_dict(base_state_dict, strict=False)  # Use strict=False to ignore missing adapter params\n        \n        # Load adapter weights\n        for name, adapter in self.adapters.items():\n            adapter_dict = {}\n            prefix = f"adapters.{name}."\n            for param_name, param in adapter_state_dict.items():\n                if param_name.startswith(prefix):\n                    short_name = param_name[len(prefix):]\n                    adapter_dict[short_name] = param\n            \n            if adapter_dict:\n                adapter.load_state_dict(adapter_dict, strict=strict)\n\n\nclass LoRAIntegrationWrapper(nn.Module):\n    """\n    Wrapper that integrates LoRA modules while maintaining compatibility with original weights.\n    """\n    def __init__(self, base_model: Qwen3VLForConditionalGeneration, lora_configs: Dict[str, Any] = None):\n        super().__init__()\n        self.base_model = base_model\n        self.config = base_model.config  # Maintain original config\n        \n        # Store original weights for compatibility verification\n        self.original_weights = {}\n        self._save_original_weights()\n        \n        # Apply LoRA to appropriate layers\n        self.lora_modules = nn.ModuleDict()\n        self.lora_configs = lora_configs or {}\n        \n        self._apply_lora()\n    \n    def _save_original_weights(self):\n        """Save original weights for compatibility verification."""\n        for name, param in self.base_model.named_parameters():\n            self.original_weights[name] = param.clone().detach()\n    \n    def _apply_lora(self):\n        """Apply LoRA to linear layers without changing model structure."""\n        # Apply LoRA to attention layers\n        for i, layer in enumerate(self.base_model.language_model.layers):\n            layer_name = f"decoder_layer_{i}"\n            \n            # Apply LoRA to self-attention query, key, value projections\n            if hasattr(layer, 'self_attn'):\n                attn_module = layer.self_attn\n                \n                # Apply LoRA to Q projection if it exists\n                if hasattr(attn_module, 'q_proj') and f"{layer_name}_q_proj" in self.lora_configs:\n                    lora_config = self.lora_configs[f"{layer_name}_q_proj"]\n                    original_q_proj = attn_module.q_proj\n                    lora_q_proj = LoraLinear(original_q_proj, lora_config)\n                    attn_module.q_proj = lora_q_proj\n                    self.lora_modules[f"{layer_name}_q_proj"] = lora_q_proj\n                \n                # Apply LoRA to K projection if it exists\n                if hasattr(attn_module, 'k_proj') and f"{layer_name}_k_proj" in self.lora_configs:\n                    lora_config = self.lora_configs[f"{layer_name}_k_proj"]\n                    original_k_proj = attn_module.k_proj\n                    lora_k_proj = LoraLinear(original_k_proj, lora_config)\n                    attn_module.k_proj = lora_k_proj\n                    self.lora_modules[f"{layer_name}_k_proj"] = lora_k_proj\n                \n                # Apply LoRA to V projection if it exists\n                if hasattr(attn_module, 'v_proj') and f"{layer_name}_v_proj" in self.lora_configs:\n                    lora_config = self.lora_configs[f"{layer_name}_v_proj"]\n                    original_v_proj = attn_module.v_proj\n                    lora_v_proj = LoraLinear(original_v_proj, lora_config)\n                    attn_module.v_proj = lora_v_proj\n                    self.lora_modules[f"{layer_name}_v_proj"] = lora_v_proj\n                \n                # Apply LoRA to output projection if it exists\n                if hasattr(attn_module, 'o_proj') and f"{layer_name}_o_proj" in self.lora_configs:\n                    lora_config = self.lora_configs[f"{layer_name}_o_proj"]\n                    original_o_proj = attn_module.o_proj\n                    lora_o_proj = LoraLinear(original_o_proj, lora_config)\n                    attn_module.o_proj = lora_o_proj\n                    self.lora_modules[f"{layer_name}_o_proj"] = lora_o_proj\n            \n            # Apply LoRA to MLP layers\n            if hasattr(layer, 'mlp'):\n                mlp_module = layer.mlp\n                \n                # Apply LoRA to gate projection if it exists\n                if hasattr(mlp_module, 'gate_proj') and f"{layer_name}_gate_proj" in self.lora_configs:\n                    lora_config = self.lora_configs[f"{layer_name}_gate_proj"]\n                    original_gate_proj = mlp_module.gate_proj\n                    lora_gate_proj = LoraLinear(original_gate_proj, lora_config)\n                    mlp_module.gate_proj = lora_gate_proj\n                    self.lora_modules[f"{layer_name}_gate_proj"] = lora_gate_proj\n                \n                # Apply LoRA to up projection if it exists\n                if hasattr(mlp_module, 'up_proj') and f"{layer_name}_up_proj" in self.lora_configs:\n                    lora_config = self.lora_configs[f"{layer_name}_up_proj"]\n                    original_up_proj = mlp_module.up_proj\n                    lora_up_proj = LoraLinear(original_up_proj, lora_config)\n                    mlp_module.up_proj = lora_up_proj\n                    self.lora_modules[f"{layer_name}_up_proj"] = lora_up_proj\n                \n                # Apply LoRA to down projection if it exists\n                if hasattr(mlp_module, 'down_proj') and f"{layer_name}_down_proj" in self.lora_configs:\n                    lora_config = self.lora_configs[f"{layer_name}_down_proj"]\n                    original_down_proj = mlp_module.down_proj\n                    lora_down_proj = LoraLinear(original_down_proj, lora_config)\n                    mlp_module.down_proj = lora_down_proj\n                    self.lora_modules[f"{layer_name}_down_proj"] = lora_down_proj\n    \n    def verify_compatibility(self) -> bool:\n        """Verify that original weights remain unchanged."""\n        for name, original_param in self.original_weights.items():\n            if name in dict(self.base_model.named_parameters()):\n                current_param = dict(self.base_model.named_parameters())[name]\n                if not torch.equal(original_param, current_param):\n                    print(f"Original weight changed in {name}")\n                    return False\n        return True\n    \n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        pixel_values: torch.FloatTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[list] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs\n    ) -> torch.Tensor:\n        """Forward pass with LoRA integration."""\n        return self.base_model(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            **kwargs\n        )\n    \n    def generate(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        max_new_tokens: int = 50,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        **kwargs\n    ):\n        """Generate with LoRA integration."""\n        return self.base_model.generate(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            **kwargs\n        )\n\n\nclass CompatibilityPreservingModel(nn.Module):\n    """\n    A model wrapper that ensures full compatibility with original weights\n    while allowing parameter-efficient adaptations.\n    """\n    def __init__(self, base_model: Qwen3VLForConditionalGeneration):\n        super().__init__()\n        self.base_model = base_model\n        self.config = base_model.config  # Maintain original config\n        \n        # Verify that the original model has the expected architecture\n        self._verify_architecture()\n        \n        # Create a compatibility manager\n        self.compatibility_manager = WeightCompatibilityManager(base_model)\n        \n        # Store references to original components\n        self.original_components = {\n            'language_model': base_model.language_model,\n            'vision_tower': base_model.vision_tower,\n            'multi_modal_projector': base_model.multi_modal_projector\n        }\n    \n    def _verify_architecture(self):\n        """Verify that the base model has the expected architecture."""\n        # Check that the model has the expected number of layers\n        expected_layers = 32\n        expected_heads = 32\n        \n        if hasattr(self.base_model.language_model, 'layers'):\n            actual_layers = len(self.base_model.language_model.layers)\n            if actual_layers != expected_layers:\n                raise ValueError(f"Expected {expected_layers} layers, got {actual_layers}")\n        \n        # Check attention heads if possible\n        if hasattr(self.base_model.config, 'num_attention_heads'):\n            if self.base_model.config.num_attention_heads != expected_heads:\n                raise ValueError(f"Expected {expected_heads} attention heads, got {self.base_model.config.num_attention_heads}")\n    \n    def add_adapter(self, name: str, location: str, adapter_config: AdapterConfig):\n        """Add an adapter to a specific location in the model."""\n        # Parse location (e.g., "language_layer_5", "vision_layer_2", etc.)\n        if location.startswith("language_layer_"):\n            layer_idx = int(location.split("_")[-1])\n            if layer_idx < len(self.base_model.language_model.layers):\n                layer = self.base_model.language_model.layers[layer_idx]\n                \n                # Add adapter to the layer\n                if not hasattr(layer, 'adapters'):\n                    layer.adapters = nn.ModuleDict()\n                \n                layer.adapters[name] = BottleneckAdapter(adapter_config, self.config.hidden_size)\n                \n                # Register with compatibility manager\n                self.compatibility_manager.register_adapter_weights(name, layer.adapters[name])\n        \n        elif location.startswith("vision_layer_"):\n            # For vision layers, the approach would be similar\n            pass  # Implementation would depend on the exact vision model structure\n    \n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        pixel_values: torch.FloatTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[list] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs\n    ) -> torch.Tensor:\n        """Forward pass that maintains compatibility."""\n        # Verify compatibility before forward\n        if not self.compatibility_manager.validate_weight_compatibility():\n            raise RuntimeError("Model weights have been modified incompatibly!")\n        \n        # Call base model forward\n        return self.base_model(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            **kwargs\n        )\n    \n    def generate(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        max_new_tokens: int = 50,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        **kwargs\n    ):\n        """Generate method that maintains compatibility."""\n        return self.base_model.generate(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            **kwargs\n        )\n    \n    def get_adapter_state_dict(self) -> Dict[str, torch.Tensor]:\n        """Get state dict containing only adapter weights."""\n        adapter_state_dict = {}\n        \n        # Collect adapter weights from all layers\n        for name, module in self.named_modules():\n            if isinstance(module, BottleneckAdapter):\n                module_state_dict = module.state_dict()\n                for param_name, param in module_state_dict.items():\n                    full_name = f"{name}.{param_name}"\n                    adapter_state_dict[full_name] = param\n        \n        return adapter_state_dict\n    \n    def load_adapter_weights(self, adapter_state_dict: Dict[str, torch.Tensor]):\n        """Load adapter weights from state dict."""\n        # This would load only adapter weights, not base model weights\n        # Implementation would need to match adapter keys to actual adapter modules\n        adapter_modules = {}\n        \n        # Find all adapter modules in the model\n        for name, module in self.named_modules():\n            if isinstance(module, BottleneckAdapter):\n                adapter_modules[name] = module\n        \n        # Load weights for each adapter\n        for adapter_name, adapter_module in adapter_modules.items():\n            adapter_dict = {}\n            prefix = f"{adapter_name}."\n            \n            for param_name, param in adapter_state_dict.items():\n                if param_name.startswith(prefix):\n                    short_name = param_name[len(prefix):]\n                    adapter_dict[short_name] = param\n            \n            if adapter_dict:\n                adapter_module.load_state_dict(adapter_dict)\n\n\ndef create_compatibility_preserving_wrapper(base_model: Qwen3VLForConditionalGeneration) -> CompatibilityPreservingModel:\n    """\n    Create a compatibility-preserving wrapper for the base model.\n    """\n    return CompatibilityPreservingModel(base_model)\n\n\ndef validate_model_compatibility(model: nn.Module, original_state_dict: Dict[str, torch.Tensor]) -> Tuple[bool, List[str]]:\n    """\n    Validate that a model maintains compatibility with original weights.\n    \n    Args:\n        model: The adapted model to validate\n        original_state_dict: The original state dictionary to compare against\n    \n    Returns:\n        Tuple of (is_compatible, list_of_modified_layers)\n    """\n    current_state_dict = model.state_dict()\n    modified_layers = []\n    \n    for name, original_param in original_state_dict.items():\n        if name in current_state_dict:\n            current_param = current_state_dict[name]\n            if current_param.shape != original_param.shape:\n                modified_layers.append(f"{name}: shape changed from {original_param.shape} to {current_param.shape}")\n            elif not torch.equal(original_param, current_param):\n                modified_layers.append(f"{name}: values changed")\n    \n    return len(modified_layers) == 0, modified_layers\n\n\ndef freeze_model_weights(model: nn.Module, exclude_adapters: bool = True):\n    """\n    Freeze model weights, optionally excluding adapter parameters.\n    \n    Args:\n        model: The model to freeze\n        exclude_adapters: Whether to exclude adapter parameters from freezing\n    """\n    for name, param in model.named_parameters():\n        # Check if this parameter belongs to an adapter\n        is_adapter_param = exclude_adapters and any(adapter_name in name for adapter_name in ['adapter', 'lora'])\n        \n        if not is_adapter_param:\n            param.requires_grad = False\n        else:\n            param.requires_grad = True  # Ensure adapters remain trainable