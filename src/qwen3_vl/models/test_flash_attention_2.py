"""\nTests for FlashAttention 2 implementation in Qwen3-VL model.\nTests memory efficiency, correctness, and hardware compatibility.\n"""\nimport pytest\nimport torch\nimport torch.nn.functional as F\nimport math\nfrom models.flash_attention_2 import FlashAttention2, HardwareSpecificFlashAttention2, FlashAttention2TransformerLayer, create_flash_attention_2\n    FlashAttention2,\n    HardwareSpecificFlashAttention2,\n    FlashAttention2TransformerLayer,\n    create_flash_attention_2\n)\nfrom qwen3_vl.core.config import Qwen3VLConfig\n\n\ndef test_flash_attention_2_basic():\n    """Test basic functionality of FlashAttention 2."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_attention_heads = 8\n    config.max_position_embeddings = 2048\n    config.rope_theta = 10000.0\n    \n    # Create attention module\n    attention = FlashAttention2(config, layer_idx=0)\n    \n    # Create test inputs\n    batch_size = 2\n    seq_len = 128\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Forward pass\n    output, attn_weights, past_key_value = attention(\n        hidden_states=hidden_states,\n        output_attentions=True\n    )\n    \n    # Check output shape\n    assert output.shape == hidden_states.shape\n    assert attn_weights is not None\n    assert attn_weights.shape == (batch_size, config.num_attention_heads, seq_len, seq_len)\n    \n    print("✓ Basic FlashAttention 2 test passed")\n\n\ndef test_flash_attention_2_with_mask():\n    """Test FlashAttention 2 with attention mask."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_attention_heads = 4\n    config.max_position_embeddings = 1024\n    \n    attention = FlashAttention2(config, layer_idx=0)\n    \n    batch_size = 1\n    seq_len = 64\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Create causal mask\n    mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)\n    mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, seq_len, seq_len)\n    \n    # Forward pass with mask\n    output, attn_weights, _ = attention(\n        hidden_states=hidden_states,\n        attention_mask=mask,\n        output_attentions=True\n    )\n    \n    # Check that output shape is correct\n    assert output.shape == hidden_states.shape\n    \n    # Check that causal mask was applied (upper triangle should be near zero)\n    if attn_weights is not None:\n        # Check that upper triangular part (excluding diagonal) has very small values due to mask\n        for i in range(seq_len):\n            for j in range(i+1, seq_len):\n                assert attn_weights[0, 0, i, j] < 1e-6, f"Mask not properly applied at position ({i}, {j})"\n    \n    print("✓ FlashAttention 2 with mask test passed")\n\n\ndef test_flash_attention_2_32_heads():\n    """Test FlashAttention 2 with 32 attention heads as required by Qwen3-VL."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 4096  # 32 heads * 128 head_dim\n    config.num_attention_heads = 32  # Required for Qwen3-VL\n    config.max_position_embeddings = 2048\n    \n    attention = FlashAttention2(config, layer_idx=0)\n    \n    batch_size = 1\n    seq_len = 32\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Forward pass\n    output, attn_weights, _ = attention(\n        hidden_states=hidden_states,\n        output_attentions=True\n    )\n    \n    # Check output shape\n    assert output.shape == hidden_states.shape\n    assert attn_weights.shape == (batch_size, config.num_attention_heads, seq_len, seq_len)\n    \n    print("✓ FlashAttention 2 with 32 heads test passed")\n\n\ndef test_hardware_specific_flash_attention():\n    """Test hardware-specific FlashAttention 2 implementation."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_attention_heads = 8\n    config.max_position_embeddings = 1024\n    \n    attention = HardwareSpecificFlashAttention2(config, layer_idx=0)\n    \n    batch_size = 2\n    seq_len = 64\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Forward pass\n    output, attn_weights, _ = attention(\n        hidden_states=hidden_states,\n        output_attentions=True\n    )\n    \n    # Check output shape\n    assert output.shape == hidden_states.shape\n    assert attn_weights is not None\n    \n    print("✓ Hardware-specific FlashAttention test passed")\n\n\ndef test_flash_attention_2_memory_efficiency():\n    """Test memory efficiency of FlashAttention 2 with larger sequences."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_attention_heads = 4\n    config.max_position_embeddings = 2048\n    \n    attention = FlashAttention2(config, layer_idx=0)\n    \n    # Test with longer sequence to verify memory efficiency\n    batch_size = 1\n    seq_len = 256  # Longer sequence\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Forward pass\n    output, attn_weights, _ = attention(\n        hidden_states=hidden_states,\n        output_attentions=False  # Don't compute weights to save memory\n    )\n    \n    # Check output shape\n    assert output.shape == hidden_states.shape\n    \n    print("✓ FlashAttention 2 memory efficiency test passed")\n\n\ndef test_flash_attention_2_with_cache():\n    """Test FlashAttention 2 with key-value caching."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_attention_heads = 4\n    config.max_position_embeddings = 1024\n    \n    attention = FlashAttention2(config, layer_idx=0)\n    \n    batch_size = 1\n    seq_len = 32\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Mock past key-value state (simplified)\n    class MockPastKeyValue:\n        def update(self, key_states, value_states, layer_idx, cache_position):\n            return key_states, value_states\n    \n    past_key_value = MockPastKeyValue()\n    \n    # Forward pass with cache\n    output, attn_weights, present_key_value = attention(\n        hidden_states=hidden_states,\n        past_key_value=past_key_value,\n        use_cache=True,\n        output_attentions=True\n    )\n    \n    # Check output shape\n    assert output.shape == hidden_states.shape\n    assert present_key_value is not None\n    \n    print("✓ FlashAttention 2 with cache test passed")\n\n\ndef test_flash_attention_2_transformer_layer():\n    """Test FlashAttention 2 within a complete transformer layer."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_attention_heads = 8\n    config.intermediate_size = 2048\n    config.max_position_embeddings = 1024\n    config.layer_norm_eps = 1e-6\n    \n    layer = FlashAttention2TransformerLayer(config, layer_idx=0)\n    \n    batch_size = 2\n    seq_len = 64\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Forward pass through transformer layer\n    output = layer(\n        hidden_states=hidden_states,\n        output_attentions=True\n    )\n    \n    # Check output shape\n    assert len(output) >= 1\n    assert output[0].shape == hidden_states.shape\n    \n    print("✓ FlashAttention 2 transformer layer test passed")\n\n\ndef test_flash_attention_2_numerical_stability():\n    """Test numerical stability of FlashAttention 2."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_attention_heads = 4\n    config.max_position_embeddings = 512\n    \n    attention = FlashAttention2(config, layer_idx=0)\n    \n    batch_size = 1\n    seq_len = 32\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Forward pass\n    output, attn_weights, _ = attention(\n        hidden_states=hidden_states,\n        output_attentions=True\n    )\n    \n    # Check that attention weights sum to approximately 1 (softmax property)\n    if attn_weights is not None:\n        attn_sum = attn_weights.sum(dim=-1)\n        assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-5), \\n            f"Attention weights don't sum to 1: {attn_sum[:5]}"\n    \n    # Check that output is finite (no NaN or inf)\n    assert torch.all(torch.isfinite(output)), "Output contains NaN or Inf values"\n    \n    print("✓ FlashAttention 2 numerical stability test passed")\n\n\ndef test_create_flash_attention_2_factory():\n    """Test the factory function for creating FlashAttention 2."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_attention_heads = 4\n    \n    # Test default creation\n    attention1 = create_flash_attention_2(config, 0)\n    assert isinstance(attention1, FlashAttention2)\n    \n    # Test hardware-specific creation\n    config.hardware_specific_attention = True\n    attention2 = create_flash_attention_2(config, 0)\n    assert isinstance(attention2, HardwareSpecificFlashAttention2)\n    \n    print("✓ FlashAttention 2 factory function test passed")\n\n\ndef test_flash_attention_2_backward_compatibility():\n    """Test that FlashAttention 2 maintains compatibility with standard attention."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 128\n    config.num_attention_heads = 2\n    config.max_position_embeddings = 512\n    \n    # Create FlashAttention\n    flash_attention = FlashAttention2(config, layer_idx=0)\n    \n    batch_size = 1\n    seq_len = 16\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size, requires_grad=True)\n    \n    # Forward pass\n    output, attn_weights, _ = flash_attention(\n        hidden_states=hidden_states,\n        output_attentions=True\n    )\n    \n    # Backward pass\n    loss = output.sum()\n    loss.backward()\n    \n    # Check that gradients exist\n    assert hidden_states.grad is not None\n    assert hidden_states.grad.shape == hidden_states.shape\n    \n    print("✓ FlashAttention 2 backward compatibility test passed")\n\n\ndef test_flash_attention_2_rotary_embeddings():\n    """Test that FlashAttention 2 properly handles rotary embeddings."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_attention_heads = 4\n    config.max_position_embeddings = 512\n    config.rope_theta = 10000.0\n    \n    attention = FlashAttention2(config, layer_idx=0)\n    \n    batch_size = 1\n    seq_len = 32\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Create position IDs\n    position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n    \n    # Forward pass with position IDs\n    output, attn_weights, _ = attention(\n        hidden_states=hidden_states,\n        position_ids=position_ids,\n        output_attentions=True\n    )\n    \n    # Check output shape\n    assert output.shape == hidden_states.shape\n    \n    print("✓ FlashAttention 2 rotary embeddings test passed")\n\n\ndef run_all_tests():\n    """Run all FlashAttention 2 tests."""\n    print("Running FlashAttention 2 tests...")\n    \n    test_flash_attention_2_basic()\n    test_flash_attention_2_with_mask()\n    test_flash_attention_2_32_heads()\n    test_hardware_specific_flash_attention()\n    test_flash_attention_2_memory_efficiency()\n    test_flash_attention_2_with_cache()\n    test_flash_attention_2_transformer_layer()\n    test_flash_attention_2_numerical_stability()\n    test_create_flash_attention_2_factory()\n    test_flash_attention_2_backward_compatibility()\n    test_flash_attention_2_rotary_embeddings()\n    \n    print("\n✅ All FlashAttention 2 tests passed!")\n\n\nif __name__ == "__main__":\n    run_all_tests()