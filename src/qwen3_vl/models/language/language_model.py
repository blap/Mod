"""\nLanguage model component for Qwen3-VL.\n\nThis module implements the language modeling component of the Qwen3-VL architecture.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\nfrom qwen3_vl.config.base_config import Qwen3VLConfig\n\n\nclass Qwen3VLLanguageModel(nn.Module):\n    """\n    Language model component of Qwen3-VL.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n\n        # Initialize model parameters based on config\n        self.hidden_size = config.hidden_size\n        self.num_hidden_layers = config.num_hidden_layers\n        self.num_attention_heads = config.num_attention_heads\n\n        # Create embedding layers\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n        self.embed_positions = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n\n        # Create transformer layers\n        self.layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=config.hidden_size,\n                nhead=config.num_attention_heads,\n                dim_feedforward=config.intermediate_size,\n                dropout=0.0,\n                activation='gelu',  # Using gelu instead of silu to avoid issues\n                batch_first=True\n            ) for _ in range(config.num_hidden_layers)\n        ])\n\n        # Final layer norm\n        self.final_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        # Output projection\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights\n        self._init_weights()\n\n    def _init_weights(self):\n        """Initialize model weights."""\n        with torch.no_grad():\n            self.embed_tokens.weight.normal_(mean=0.0, std=self.config.initializer_range)\n            self.embed_positions.weight.normal_(mean=0.0, std=self.config.initializer_range)\n            self.lm_head.weight.normal_(mean=0.0, std=self.config.initializer_range)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        **kwargs\n    ):\n        """\n        Forward pass of the language model.\n\n        Args:\n            input_ids: Input token IDs\n            attention_mask: Attention mask\n            position_ids: Position IDs\n            **kwargs: Additional arguments\n\n        Returns:\n            Language model outputs\n        """\n        # Get input embeddings\n        inputs_embeds = self.embed_tokens(input_ids)\n\n        # Add position embeddings\n        if position_ids is None:\n            position_ids = torch.arange(\n                0, input_ids.size(1), dtype=torch.long, device=input_ids.device\n            ).unsqueeze(0)\n        position_embeds = self.embed_positions(position_ids)\n\n        hidden_states = inputs_embeds + position_embeds\n\n        # Apply transformer layers\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, src_key_padding_mask=attention_mask)\n\n        # Apply final layer norm\n        hidden_states = self.final_layernorm(hidden_states)\n\n        # Generate logits\n        logits = self.lm_head(hidden_states)\n\n        return logits