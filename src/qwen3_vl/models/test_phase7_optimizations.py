"""\nComprehensive tests for Phase 7 Advanced Architecture Optimizations\n"""\nimport torch\nimport pytest\nfrom torch.testing import assert_close\nimport numpy as np\n\nfrom config import Qwen3VLConfig\nfrom models.dynamic_sparse_attention import DynamicSparseAttention, VisionDynamicSparseAttention\nfrom models.adaptive_depth import InputComplexityAssessor, AdaptiveDepthController\nfrom models.cross_modal_memory_compression import CrossModalMemoryCompressor, CrossModalMemoryBank\nfrom models.cross_layer_memory_sharing import CrossLayerMemoryManager, LayerMemoryBank\nfrom models.context_adaptive_positional_encoding import ContextAdaptivePositionalEncoding, VisionContextAdaptivePositionalEncoding\nfrom models.conditional_feature_extraction import ConditionalFeatureExtractor\nfrom models.adaptive_precision import AdaptivePrecisionController, LayerWisePrecisionSelector, PrecisionAdaptiveLayer\nfrom models.moe_flash_attention import MoeLayer\n\n\ndef test_dynamic_sparse_attention():\n    """Test dynamic sparse attention with learned routing."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_attention_heads = 8\n    config.max_position_embeddings = 512\n    config.rope_theta = 10000\n    \n    # Initialize with sparse attention enabled\n    config.use_dynamic_sparse_attention = True\n    config.sparse_attention_sparsity_ratio = 0.5\n\n    attention = DynamicSparseAttention(config, layer_idx=0)\n    \n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n    \n    # Test forward pass\n    output, attn_weights, past_key_value = attention(\n        hidden_states=hidden_states,\n        position_ids=position_ids\n    )\n    \n    assert output.shape == hidden_states.shape\n    assert attn_weights is None  # Not returned when output_attentions=False\n    print("✓ Dynamic sparse attention test passed")\n\n\ndef test_vision_dynamic_sparse_attention():\n    """Test vision dynamic sparse attention."""\n    config = Qwen3VLConfig()\n    config.vision_hidden_size = 768\n    config.vision_num_attention_heads = 12\n    \n    attention = VisionDynamicSparseAttention(config)\n    \n    # Create test inputs\n    batch_size, seq_len = 2, 196  # 14x14 patches\n    hidden_states = torch.randn(batch_size, seq_len, config.vision_hidden_size)\n    \n    # Test forward pass\n    output = attention(hidden_states=hidden_states)\n    \n    assert output.shape == hidden_states.shape\n    print("✓ Vision dynamic sparse attention test passed")\n\n\ndef test_input_complexity_assessor():\n    """Test input complexity assessment."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.vision_hidden_size = 768\n    config.vocab_size = 32000\n\n    assessor = InputComplexityAssessor(config)\n    \n    # Test text complexity assessment\n    batch_size, seq_len = 2, 32\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n    text_complexity = assessor.assess_text_complexity(input_ids)\n    assert text_complexity.shape == () or text_complexity.shape == (1,)\n    assert 0.0 <= text_complexity.item() <= 1.0\n    \n    # Test image complexity assessment\n    pixel_values = torch.randn(batch_size, 3, 224, 224)\n    image_complexity = assessor.assess_image_complexity(pixel_values)\n    assert 0.0 <= image_complexity.item() <= 1.0\n    \n    # Test multimodal complexity assessment\n    multimodal_complexity = assessor.assess_multimodal_complexity(input_ids, pixel_values)\n    assert 0.0 <= multimodal_complexity.item() <= 1.0\n    \n    # Test forward pass\n    complexity_score = assessor(input_ids=input_ids)\n    assert 0.0 <= complexity_score.item() <= 1.0\n    \n    complexity_score = assessor(pixel_values=pixel_values)\n    assert 0.0 <= complexity_score.item() <= 1.0\n    \n    complexity_score = assessor(input_ids=input_ids, pixel_values=pixel_values)\n    assert 0.0 <= complexity_score.item() <= 1.0\n    \n    print("✓ Input complexity assessor test passed")\n\n\ndef test_adaptive_depth_controller():\n    """Test adaptive depth controller."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_hidden_layers = 32\n    config.vocab_size = 32000\n\n    assessor = InputComplexityAssessor(config)\n    controller = AdaptiveDepthController(config, assessor)\n    \n    # Test with text input\n    batch_size, seq_len = 2, 32\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n    num_layers, complexity = controller(input_ids=input_ids)\n    \n    assert isinstance(num_layers, int)\n    assert 1 <= num_layers <= config.num_hidden_layers\n    assert 0.0 <= complexity <= 1.0\n    \n    # Test with image input\n    pixel_values = torch.randn(batch_size, 3, 224, 224)\n    num_layers, complexity = controller(pixel_values=pixel_values)\n    \n    assert isinstance(num_layers, int)\n    assert 1 <= num_layers <= config.num_hidden_layers\n    assert 0.0 <= complexity <= 1.0\n    \n    print("✓ Adaptive depth controller test passed")\n\n\ndef test_cross_modal_memory_compressor():\n    """Test cross-modal memory compression."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.vision_hidden_size = 768  # Set vision hidden size as well\n    config.compression_ratio = 0.5\n    config.enable_cross_modal_compression = True\n\n    compressor = CrossModalMemoryCompressor(config)\n\n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    text_features = torch.randn(batch_size, seq_len, config.hidden_size)\n    vision_features = torch.randn(batch_size, seq_len, config.vision_hidden_size)\n\n    # Test forward pass\n    compressed_text, compressed_vision = compressor(text_features, vision_features)\n\n    assert compressed_text.shape == text_features.shape\n    assert compressed_vision.shape == vision_features.shape\n\n    # Test with compression disabled\n    config.enable_cross_modal_compression = False\n    compressor_disabled = CrossModalMemoryCompressor(config)\n    text_out, vision_out = compressor_disabled(text_features, vision_features)\n\n    # Should return original features when disabled\n    assert torch.allclose(text_out, text_features, atol=1e-5)\n    assert torch.allclose(vision_out, vision_features, atol=1e-5)\n\n    print("✓ Cross-modal memory compressor test passed")\n\n\ndef test_cross_layer_memory_manager():\n    """Test cross-layer memory sharing."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_hidden_layers = 32\n\n    manager = CrossLayerMemoryManager(config)\n    \n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Test forward pass through multiple layers\n    for layer_idx in range(3):\n        updated_states = manager(hidden_states, layer_idx)\n        assert updated_states.shape == hidden_states.shape\n        \n        # Reset memory between tests\n        manager.reset_memory()\n    \n    print("✓ Cross-layer memory manager test passed")\n\n\ndef test_context_adaptive_positional_encoding():\n    """Test context-adaptive positional encoding."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.max_position_embeddings = 512\n\n    encoder = ContextAdaptivePositionalEncoding(\n        hidden_size=config.hidden_size,\n        max_seq_len=config.max_position_embeddings\n    )\n    \n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n    \n    # Test forward pass\n    output = encoder(hidden_states=hidden_states, position_ids=position_ids)\n    assert output.shape == hidden_states.shape\n    \n    # Test without position IDs\n    output = encoder(hidden_states=hidden_states)\n    assert output.shape == hidden_states.shape\n    \n    print("✓ Context-adaptive positional encoding test passed")\n\n\ndef test_vision_context_adaptive_positional_encoding():\n    """Test vision context-adaptive positional encoding."""\n    config = Qwen3VLConfig()\n    config.vision_hidden_size = 768\n    config.vision_image_size = 224\n    config.vision_patch_size = 16\n\n    encoder = VisionContextAdaptivePositionalEncoding(\n        hidden_size=config.vision_hidden_size,\n        num_patches_per_dim=config.vision_image_size // config.vision_patch_size\n    )\n    \n    # Create test inputs\n    batch_size, num_patches = 2, 196  # 14x14 patches\n    hidden_states = torch.randn(batch_size, num_patches, config.vision_hidden_size)\n    \n    # Test forward pass\n    output = encoder(hidden_states=hidden_states)\n    assert output.shape == hidden_states.shape\n    \n    print("✓ Vision context-adaptive positional encoding test passed")\n\n\ndef test_conditional_feature_extractor():\n    """Test conditional feature extraction."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.vocab_size = 32000\n    config.vision_hidden_size = 768\n    config.vision_num_channels = 3\n    config.vision_image_size = 224\n    config.vision_patch_size = 16\n\n    extractor = ConditionalFeatureExtractor(config)\n    \n    # Test text extraction\n    batch_size, seq_len = 2, 16\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n    text_features, text_info = extractor(text_input=input_ids)\n    assert text_features.shape[0] == batch_size\n    assert text_features.shape[1] == seq_len\n    assert text_features.shape[2] == config.hidden_size\n    assert text_info['modality'] == 'text'\n    \n    # Test vision extraction\n    pixel_values = torch.randn(batch_size, 3, 224, 224)\n    vision_features, vision_info = extractor(image_input=pixel_values)\n    assert vision_features.shape[0] == batch_size\n    # Vision features should match vision_hidden_size, not hidden_size\n    assert vision_features.shape[2] == config.vision_hidden_size\n    assert vision_info['modality'] == 'vision'\n    \n    # Test multimodal extraction\n    multimodal_features, multimodal_info = extractor(\n        text_input=input_ids,\n        image_input=pixel_values\n    )\n    assert multimodal_features.shape[0] == batch_size\n    assert multimodal_info['modality'] == 'multimodal'\n    \n    print("✓ Conditional feature extractor test passed")\n\n\ndef test_adaptive_precision_controller():\n    """Test adaptive precision controller."""\n    config = Qwen3VLConfig()\n    config.num_hidden_layers = 4\n\n    controller = AdaptivePrecisionController(config)\n    \n    # Test precision selection\n    requirements = {\n        'computation_intensity': 0.5,\n        'sensitivity_to_precision': 0.3,\n        'memory_footprint': 0.7,\n        'accuracy_importance': 0.8\n    }\n    \n    for layer_idx in range(config.num_hidden_layers):\n        precision = controller.select_optimal_precision(layer_idx, requirements)\n        assert precision in ['fp32', 'fp16', 'int8', 'mixed']\n    \n    # Test current precisions\n    precisions = controller.get_current_precisions()\n    assert len(precisions) == config.num_hidden_layers\n    \n    print("✓ Adaptive precision controller test passed")\n\n\ndef test_layer_wise_precision_selector():\n    """Test layer-wise precision selector."""\n    config = Qwen3VLConfig()\n    config.num_hidden_layers = 4\n\n    selector = LayerWisePrecisionSelector(config)\n    \n    # Create test inputs\n    batch_size, seq_len, hidden_size = 2, 16, 512\n    inputs = torch.randn(batch_size, seq_len, hidden_size)\n    \n    for layer_idx in range(config.num_hidden_layers):\n        requirements = selector.estimate_layer_requirements(layer_idx, inputs)\n        assert 'computation_intensity' in requirements\n        assert 'sensitivity_to_precision' in requirements\n        \n        precision = selector.select_precision(layer_idx, requirements)\n        assert precision in ['fp32', 'fp16', 'int8', 'mixed']\n    \n    print("✓ Layer-wise precision selector test passed")\n\n\ndef test_moe_layer():\n    """Test Mixture of Experts layer."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.intermediate_size = 2048\n\n    moe = MoeLayer(config, num_experts=4, top_k=2)\n    \n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    x = torch.randn(batch_size, seq_len, config.hidden_size)\n    \n    # Test forward pass\n    output = moe(x)\n    assert output.shape == x.shape\n    \n    print("✓ Mixture of Experts layer test passed")\n\n\ndef test_integration_of_all_components():\n    """Test integration of all optimization components."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_hidden_layers = 32\n    config.num_attention_heads = 8\n    config.vocab_size = 32000\n    config.vision_hidden_size = 768\n    config.vision_num_attention_heads = 12\n    config.vision_num_channels = 3\n    config.vision_image_size = 224\n    config.vision_patch_size = 16\n    config.max_position_embeddings = 512\n    config.rope_theta = 10000\n\n    # Enable various optimizations\n    config.use_dynamic_sparse_attention = True\n    config.use_adaptive_precision = True\n    config.use_context_adaptive_positional_encoding = True\n    config.use_conditional_feature_extraction = True\n    config.enable_cross_modal_compression = True\n    config.enable_cross_layer_memory_sharing = True\n    config.use_adaptive_depth = True\n    config.use_moe = True\n    config.moe_num_experts = 4\n    config.moe_top_k = 2\n\n    # Test that all components can be instantiated together\nfrom models.modeling_qwen3_vl import Qwen3VLForConditionalGeneration\n    \n    model = Qwen3VLForConditionalGeneration(config)\n    \n    # Test with text input\n    batch_size, seq_len = 1, 16\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n    outputs = model(input_ids=input_ids)\n    assert outputs.shape[0] == batch_size\n    \n    # Test with image input\n    pixel_values = torch.randn(batch_size, 3, 224, 224)\n    outputs = model(pixel_values=pixel_values)\n    assert outputs.shape[0] == batch_size\n    \n    # Test with multimodal input\n    outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n    assert outputs.shape[0] == batch_size\n    \n    print("✓ Integration of all components test passed")\n\n\ndef run_all_tests():\n    """Run all tests for Phase 7 optimizations."""\n    print("Running Phase 7 Advanced Architecture Optimizations tests...")\n    \n    test_dynamic_sparse_attention()\n    test_vision_dynamic_sparse_attention()\n    test_input_complexity_assessor()\n    test_adaptive_depth_controller()\n    test_cross_modal_memory_compressor()\n    test_cross_layer_memory_manager()\n    test_context_adaptive_positional_encoding()\n    test_vision_context_adaptive_positional_encoding()\n    test_conditional_feature_extraction()\n    test_adaptive_precision_controller()\n    test_layer_wise_precision_selector()\n    test_moe_layer()\n    test_integration_of_all_components()\n    \n    print("\n✓ All Phase 7 optimization tests passed successfully!")\n\n\nif __name__ == "__main__":\n    run_all_tests()