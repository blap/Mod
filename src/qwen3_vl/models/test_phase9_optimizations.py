"""\nComprehensive tests for Phase 9 Advanced Performance Optimizations\n"""\nimport torch\nimport pytest\nfrom torch.testing import assert_close\nimport numpy as np\nimport time\n\nfrom models.config import Qwen3VLConfig\nfrom models.block_sparse_attention import BlockSparseAttention, VisionBlockSparseAttention\nfrom models.cross_modal_token_merging import CrossModalTokenMerger, HierarchicalCrossModalFusion\nfrom models.hierarchical_memory_compression import HierarchicalMemoryCompressor, AdaptiveMemoryBank, MemoryEfficientCrossAttention\nfrom models.learned_activation_routing import LearnedActivationRouter, ContextAdaptiveActivation, MultiModalActivationRouter, AdaptiveMLPWithLearnedRouting\nfrom models.adaptive_batch_processing import AdaptiveBatchProcessor, DynamicBatchScheduler, HeterogeneousBatchProcessor\nfrom models.cross_layer_parameter_recycling import CrossLayerParameterRecycler, ParameterRecyclingAdapter, RecycledAttention, RecycledMLP, HierarchicalParameterRecycling\nfrom models.adaptive_sequence_packing import SequencePacker, DynamicSequencePacker, MultimodalSequencePacker\nfrom models.memory_efficient_gradient_accumulation import GradientAccumulationScheduler, LayerWiseGradientScheduler, DynamicGradientAccumulator\nfrom models.kv_cache_optimization_multi_strategy import MultiStrategyKVCache\nfrom models.faster_rotary_embedding import OptimizedRotaryEmbedding, FastRotaryAttention\nfrom models.distributed_pipeline_parallelism import PipelineParallelModel, OptimizedPipelineParallelModel\nfrom models.hardware_specific_optimization import HardwareOptimizedAttention, HardwareOptimizedMLP, SM61OptimizedTransformerLayer\n\n\ndef test_block_sparse_attention():\n    """Test block-sparse attention with learned routing."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_attention_heads = 8\n    config.max_position_embeddings = 512\n    config.rope_theta = 10000\n    config.block_sparse_sparsity_ratio = 0.5\n    config.block_sparse_block_size = 32\n\n    attention = BlockSparseAttention(config, layer_idx=0)\n\n    # Create test inputs\n    batch_size, seq_len = 2, 64\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n\n    # Test forward pass\n    output, attn_weights, past_key_value = attention(\n        hidden_states=hidden_states,\n        position_ids=position_ids\n    )\n\n    assert output.shape == hidden_states.shape\n    assert attn_weights is None  # Not returned when output_attentions=False\n    print("PASS: Block-sparse attention test passed")\n\n\ndef test_cross_modal_token_merging():\n    """Test cross-modal token merging functionality."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.vision_hidden_size = 768\n    config.cmtm_similarity_temperature = 0.1\n    config.cmtm_merging_threshold = 0.7\n    config.cmtm_max_merged_tokens = 0.5\n\n    merger = CrossModalTokenMerger(config)\n\n    # Create test inputs\n    batch_size, lang_seq_len, vision_seq_len = 2, 16, 196  # 14x14 patches\n    language_tokens = torch.randn(batch_size, lang_seq_len, config.hidden_size)\n    vision_tokens = torch.randn(batch_size, vision_seq_len, config.vision_hidden_size)\n\n    # Test forward pass\n    merged_lang, merged_vision, merge_info = merger(\n        language_tokens=language_tokens,\n        vision_tokens=vision_tokens\n    )\n\n    assert merged_lang.shape[0] == batch_size\n    assert merged_vision.shape[0] == batch_size\n    assert 'merge_info' in locals()\n    print("PASS: Cross-modal token merging test passed")\n\n\ndef test_hierarchical_memory_compression():\n    """Test hierarchical memory compression system."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.hierarchical_compression_levels = 3\n\n    compressor = HierarchicalMemoryCompressor(config)\n\n    # Create test inputs\n    batch_size, seq_len = 2, 32\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n\n    # Test forward pass\n    compressed_states, compression_info = compressor(hidden_states)\n\n    assert compressed_states.shape == hidden_states.shape\n    assert 'compression_strategies' in compression_info\n    assert 'compression_ratios' in compression_info\n    print("PASS: Hierarchical memory compression test passed")\n\n\ndef test_learned_activation_routing():\n    """Test learned activation routing mechanism."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.learned_activation_count = 5\n\n    router = LearnedActivationRouter(config)\n\n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n\n    # Test forward pass\n    activated_states = router(hidden_states, position_ids, layer_idx=0)\n\n    assert activated_states.shape == hidden_states.shape\n    print("PASS: Learned activation routing test passed")\n\n\ndef test_adaptive_batch_processing():\n    """Test adaptive batch processing with heterogeneous inputs."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.vision_hidden_size = 768\n    config.vision_image_size = 224\n    config.vision_patch_size = 16\n\n    processor = AdaptiveBatchProcessor(config)\n\n    # Create test inputs\n    batch_size, seq_len = 4, 32\n    input_ids = torch.randint(0, 1000, (batch_size, seq_len))\n    pixel_values = torch.randn(batch_size, 3, 224, 224)\n\n    # Test forward pass\n    processed_output, batch_info = processor(\n        input_ids=input_ids,\n        pixel_values=pixel_values\n    )\n\n    assert processed_output.shape[0] >= batch_size  # May be expanded due to packing\n    assert 'input_types' in batch_info\n    assert 'scheduling_info' in batch_info\n    print("PASS: Adaptive batch processing test passed")\n\n\ndef test_cross_layer_parameter_recycling():\n    """Test cross-layer parameter recycling."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.parameter_recycling_groups = 4\n    config.parameter_recycling_adapter_factor = 16\n\n    recycler = CrossLayerParameterRecycler(config)\n\n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n\n    # Test forward pass\n    recycled_output = recycler(hidden_states, layer_idx=0)\n\n    assert recycled_output.shape == hidden_states.shape\n    print("PASS: Cross-layer parameter recycling test passed")\n\n\ndef test_adaptive_sequence_packing():\n    """Test adaptive sequence packing."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.max_position_embeddings = 512\n    config.sequence_packing_algorithm = 'greedy'\n    config.sequence_packing_bin_size = 256\n\n    packer = DynamicSequencePacker(config)\n\n    # Create sequences of different lengths\n    seq1 = torch.randn(10, config.hidden_size)\n    seq2 = torch.randn(20, config.hidden_size)\n    seq3 = torch.randn(15, config.hidden_size)\n    sequences = [seq1, seq2, seq3]\n\n    # Test forward pass\n    packed_seq, packed_mask, packing_info = packer(sequences)\n\n    assert packed_seq.dim() == 3  # [num_bins, max_bin_len, hidden_size]\n    assert packed_mask.shape[0] == packed_seq.shape[0]\n    assert 'packing_efficiency' in packing_info\n    print("PASS: Adaptive sequence packing test passed")\n\n\ndef test_memory_efficient_gradient_accumulation():\n    """Test memory-efficient gradient accumulation scheduling."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.gradient_accumulation_steps = 4\n\n    scheduler = GradientAccumulationScheduler(config)\n\n    # Create gradient tensors\n    grad1 = torch.randn(2, 16, config.hidden_size)\n    grad2 = torch.randn(2, 16, config.hidden_size)\n    grad3 = torch.randn(2, 16, config.hidden_size)\n    gradients = [grad1, grad2, grad3]\n\n    # Test forward pass\n    accumulated_grad, scheduling_info = scheduler(gradients, current_step=2, total_steps=4)\n\n    assert accumulated_grad.shape == grad1.shape\n    assert 'strategy' in scheduling_info\n    print("PASS: Memory-efficient gradient accumulation test passed")\n\n\ndef test_kv_cache_optimization_multi_strategy():\n    """Test KV cache optimization with multiple strategies."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_attention_heads = 8\n    config.max_position_embeddings = 512\n\n    cache = MultiStrategyKVCache(config, layer_idx=0)\n\n    # Create test key/value states\n    batch_size, num_heads, seq_len, head_dim = 2, 8, 16, 64\n    key_states = torch.randn(batch_size, num_heads, seq_len, head_dim)\n    value_states = torch.randn(batch_size, num_heads, seq_len, head_dim)\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n\n    # Test forward pass\n    k_out, v_out, cache_info = cache(key_states, value_states, hidden_states)\n\n    assert k_out.shape[0] == batch_size\n    assert v_out.shape[0] == batch_size\n    assert 'strategy' in cache_info\n    print("PASS: KV cache optimization multi-strategy test passed")\n\n\ndef test_faster_rotary_embedding():\n    """Test faster rotary embedding approximations."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_attention_heads = 8\n    head_dim = config.hidden_size // config.num_attention_heads\n\n    rotary_emb = OptimizedRotaryEmbedding(\n        head_dim,\n        max_position_embeddings=512,\n        base=10000,\n        optimization_level="balanced"\n    )\n\n    # Create test inputs\n    batch_size, num_heads, seq_len, head_dim = 2, 8, 16, 64\n    x = torch.randn(batch_size, num_heads, seq_len, head_dim)\n    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n\n    # Test forward pass\n    cos, sin = rotary_emb(x, position_ids)\n\n    assert cos.shape == (batch_size, num_heads, seq_len, head_dim)\n    assert sin.shape == (batch_size, num_heads, seq_len, head_dim)\n    print("PASS: Faster rotary embedding test passed")\n\n\ndef test_distributed_pipeline_parallelism():\n    """Test distributed pipeline parallelism."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_hidden_layers = 4\n    config.num_attention_heads = 4\n    config.intermediate_size = 512\n\n    # Test single device pipeline\n    pipeline_model = PipelineParallelModel(config, num_stages=2)\n\n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n\n    # Test forward pass\n    output = pipeline_model(hidden_states)\n\n    assert output.shape == hidden_states.shape\n    print("PASS: Distributed pipeline parallelism test passed")\n\n\ndef test_hardware_specific_optimization():\n    """Test hardware-specific kernel optimizations."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_attention_heads = 4\n    config.max_position_embeddings = 512\n    config.rope_theta = 10000\n    config.use_tensor_cores = True\n    config.use_half_precision = True\n\n    attention = HardwareOptimizedAttention(config, layer_idx=0)\n\n    # Create test inputs\n    batch_size, seq_len = 2, 16\n    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)\n    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n\n    # Test forward pass\n    output, attn_weights, past_key_value = attention(\n        hidden_states=hidden_states,\n        position_ids=position_ids\n    )\n\n    assert output.shape == hidden_states.shape\n    print("PASS: Hardware-specific optimization test passed")\n\n\ndef test_integration_of_all_optimizations():\n    """Test integration of all Phase 9 optimization components."""\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_hidden_layers = 4\n    config.num_attention_heads = 4\n    config.vision_hidden_size = 512\n    config.vision_num_attention_heads = 8\n    config.max_position_embeddings = 256\n    config.rope_theta = 10000\n    \n    # Enable Phase 9 optimizations\n    config.block_sparse_sparsity_ratio = 0.5\n    config.cmtm_similarity_temperature = 0.1\n    config.hierarchical_compression_levels = 2\n    config.learned_activation_count = 4\n    config.parameter_recycling_groups = 2\n    config.sequence_packing_algorithm = 'greedy'\n    config.gradient_accumulation_steps = 2\n    config.use_tensor_cores = True\n    config.use_half_precision = True\n\n    # Test that all components can be instantiated together\n    try:\n        # Test attention components\n        sparse_attention = BlockSparseAttention(config, layer_idx=0)\n        kv_cache = MultiStrategyKVCache(config, layer_idx=0)\n        fast_rotary = FastRotaryAttention(config, layer_idx=0)\n        hardware_attention = HardwareOptimizedAttention(config, layer_idx=0)\n        \n        # Test other components\n        token_merger = CrossModalTokenMerger(config)\n        memory_compressor = HierarchicalMemoryCompressor(config)\n        activation_router = LearnedActivationRouter(config)\n        parameter_recycler = CrossLayerParameterRecycler(config)\n        sequence_packer = DynamicSequencePacker(config)\n        grad_scheduler = GradientAccumulationScheduler(config)\n        \n        print("PASS: Integration of all Phase 9 optimization components test passed")\n    except Exception as e:\n        print(f"FAIL: Integration test failed: {e}")\n        raise\n\n\ndef run_phase9_tests():\n    """Run all Phase 9 optimization tests."""\n    print("Running Phase 9 Advanced Performance Optimizations tests...\n")\n    \n    test_block_sparse_attention()\n    test_cross_modal_token_merging()\n    test_hierarchical_memory_compression()\n    test_learned_activation_routing()\n    test_adaptive_batch_processing()\n    test_cross_layer_parameter_recycling()\n    test_adaptive_sequence_packing()\n    test_memory_efficient_gradient_accumulation()\n    test_kv_cache_optimization_multi_strategy()\n    test_faster_rotary_embedding()\n    test_distributed_pipeline_parallelism()\n    test_hardware_specific_optimization()\n    test_integration_of_all_optimizations()\n    \n    print("\nPASS: All Phase 9 optimization tests passed successfully!")\n\n\nif __name__ == "__main__":\n    run_phase9_tests()