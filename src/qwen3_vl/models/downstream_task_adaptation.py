"""\nDownstream task adaptation support for Qwen3-VL model.\nThis module provides components for adapting the model to specific downstream tasks\nwithout changing the core architecture.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Dict, Any, List, Union, Tuple\nfrom dataclasses import dataclass\nimport os\nimport json\nimport copy\nfrom models.adapter_layers import AdapterConfig, BottleneckAdapter, LoraLinear, AdapterLayer\nfrom models.plugin_modules import PluginConfig, ModularAdapterLayer, DownstreamTaskAdapter\nfrom models.device_specific_adapters import DeviceAdapterFactory, DeviceAdapterConfig\n\n\n@dataclass\nclass TaskConfig:\n    """\n    Configuration for downstream tasks.\n    """\n    task_type: str = "classification"  # "classification", "regression", "generation", "question_answering", etc.\n    num_labels: int = 2  # Number of output labels for classification tasks\n    task_name: str = "default"\n    \n    # Adapter configuration for this task\n    adapter_config: Optional[AdapterConfig] = None\n    \n    # Task-specific parameters\n    use_task_adapter: bool = True\n    task_adapter_reduction_factor: int = 2  # Factor by which to reduce adapter dimensions for task-specific adapters\n    freeze_backbone: bool = True  # Whether to freeze the main model during task adaptation\n    learning_rate_multiplier: float = 1.0  # Multiplier for task-specific learning rate\n\n\nclass TaskAdapterManager(nn.Module):\n    """\n    Manager for task-specific adapters.\n    Handles registration, activation, and switching between different task adapters.\n    """\n    def __init__(self):\n        super().__init__()\n        self.task_adapters = nn.ModuleDict()\n        self.task_configs = {}\n        self.active_task = None\n        self.is_training = False\n    \n    def register_task(self, task_name: str, config: TaskConfig, input_dim: int):\n        """Register a new task with its adapter."""\n        self.task_configs[task_name] = config\n        \n        # Check if config is a TaskConfig object or a dictionary\n        if hasattr(config, 'use_task_adapter'):\n            # config is a TaskConfig object\n            use_task_adapter = config.use_task_adapter\n            task_adapter_reduction_factor = config.task_adapter_reduction_factor\n            task_adapter_config = config.adapter_config or AdapterConfig()\n        else:\n            # config is a dictionary\n            use_task_adapter = config.get('use_task_adapter', True)\n            task_adapter_reduction_factor = config.get('task_adapter_reduction_factor', 2)\n            task_adapter_config = config.get('adapter_config') or AdapterConfig()\n\n        # Create task-specific adapter\n        if use_task_adapter:\n            # Adjust adapter configuration for task-specific use\n            adapter_config = task_adapter_config or AdapterConfig()\n\n            # Reduce adapter dimensions for task-specific adapters to save parameters\n            adapter_dim = max(\n                adapter_config.adapter_dim // task_adapter_reduction_factor,\n                16\n            )\n\n            # Create a new config with the adjusted adapter_dim\n            task_adapter_config = AdapterConfig(\n                adapter_dim=adapter_dim,\n                adapter_scalar=adapter_config.adapter_scalar,\n                adapter_dropout=adapter_config.adapter_dropout,\n                lora_r=adapter_config.lora_r,\n                lora_alpha=adapter_config.lora_alpha,\n                lora_dropout=adapter_config.lora_dropout,\n                device_specific=adapter_config.device_specific,\n                hardware_config=adapter_config.hardware_config,\n                task_name=task_name,\n                is_trainable=adapter_config.is_trainable\n            )\n\n            # Create the task adapter\n            task_adapter = AdapterLayer(task_adapter_config, input_dim)\n            self.task_adapters[task_name] = task_adapter\n        else:\n            # If not using task adapters, create a simple placeholder\n            self.task_adapters[task_name] = nn.Identity()\n    \n    def set_active_task(self, task_name: str):\n        """Set the active task."""\n        if task_name not in self.task_adapters:\n            raise ValueError(f"Task '{task_name}' not registered")\n        \n        self.active_task = task_name\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Apply the active task adapter."""\n        if self.active_task is None:\n            # If no active task, return unchanged\n            return hidden_states\n        \n        return self.task_adapters[self.active_task](hidden_states)\n    \n    def get_task_parameters(self, task_name: str) -> List[nn.Parameter]:\n        """Get parameters for a specific task."""\n        if task_name not in self.task_adapters:\n            return []\n        \n        return list(self.task_adapters[task_name].parameters())\n    \n    def get_all_trainable_parameters(self) -> List[nn.Parameter]:\n        """Get all trainable parameters across all task adapters."""\n        all_params = []\n        for adapter in self.task_adapters.values():\n            all_params.extend([p for p in adapter.parameters() if p.requires_grad])\n        return all_params\n\n\nclass DownstreamTaskHead(nn.Module):\n    """\n    Task-specific head for downstream tasks.\n    """\n    def __init__(self, config: TaskConfig, input_dim: int):\n        super().__init__()\n        self.config = config\n        self.task_type = config.task_type\n        \n        # Create task-specific head based on task type\n        if config.task_type == "classification":\n            self.head = nn.Linear(input_dim, config.num_labels)\n            self.activation = nn.Softmax(dim=-1) if config.num_labels > 1 else nn.Sigmoid()\n        elif config.task_type == "regression":\n            self.head = nn.Linear(input_dim, 1)\n            self.activation = nn.Identity()\n        elif config.task_type == "generation":\n            # For generation, we might just use the language model head\n            self.head = nn.Linear(input_dim, config.vocab_size if hasattr(config, 'vocab_size') else 50257)\n            self.activation = nn.Softmax(dim=-1)\n        elif config.task_type == "question_answering":\n            # For QA, we might predict start and end positions\n            self.head = nn.Linear(input_dim, 2)  # Start and end logits\n            self.activation = nn.Softmax(dim=-1)\n        elif config.task_type == "token_classification":  # NER, POS tagging, etc.\n            self.head = nn.Linear(input_dim, config.num_labels)\n            self.activation = nn.Softmax(dim=-1)\n        else:\n            # Default to binary classification\n            self.head = nn.Linear(input_dim, 2)\n            self.activation = nn.Softmax(dim=-1)\n        \n        # Initialize head weights\n        nn.init.normal_(self.head.weight, std=0.02)\n        if self.head.bias is not None:\n            nn.init.zeros_(self.head.bias)\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass through task head."""\n        # Apply task head\n        logits = self.head(hidden_states)\n        \n        # Apply activation\n        output = self.activation(logits)\n        \n        return output\n\n\nclass TaskAdaptedModel(nn.Module):\n    """\n    Wrapper that adds task adaptation capabilities to a base model.\n    """\n    def __init__(self, base_model: nn.Module, task_configs: Dict[str, TaskConfig]):\n        super().__init__()\n        self.base_model = base_model\n        self.task_configs = task_configs\n        \n        # Get input dimension from base model config\n        if hasattr(base_model.config, 'hidden_size'):\n            input_dim = base_model.config.hidden_size\n        else:\n            input_dim = 2048  # Default fallback\n        \n        # Create task adapter manager\n        self.task_adapter_manager = TaskAdapterManager()\n        \n        # Register all tasks\n        for task_name, config in task_configs.items():\n            self.task_adapter_manager.register_task(task_name, config, input_dim)\n        \n        # Create task-specific heads\n        self.task_heads = nn.ModuleDict()\n        for task_name, config in task_configs.items():\n            self.task_heads[task_name] = DownstreamTaskHead(config, input_dim)\n        \n        # Freeze backbone if specified in configs\n        self._freeze_backbone_if_needed()\n    \n    def _freeze_backbone_if_needed(self):\n        """Freeze the base model parameters if specified in task configs."""\n        should_freeze = any(config.freeze_backbone for config in self.task_configs.values())\n        \n        if should_freeze:\n            for param in self.base_model.parameters():\n                param.requires_grad = False\n    \n    def set_active_task(self, task_name: str):\n        """Set the active task for adaptation."""\n        self.task_adapter_manager.set_active_task(task_name)\n        self.active_task = task_name\n    \n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        **kwargs\n    ) -> torch.Tensor:\n        """Forward pass with task-specific adaptation."""\n        # Get base model output\n        base_output = self.base_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            pixel_values=pixel_values,\n            **kwargs\n        )\n        \n        # Apply task-specific adapter\n        adapted_output = self.task_adapter_manager(base_output)\n        \n        # Apply task-specific head if active task is set\n        if hasattr(self, 'active_task') and self.active_task in self.task_heads:\n            task_output = self.task_heads[self.active_task](adapted_output)\n            return task_output\n        \n        return adapted_output\n    \n    def get_trainable_parameters(self) -> List[nn.Parameter]:\n        """Get all trainable parameters (only task adapters and heads, not backbone)."""\n        trainable_params = []\n        \n        # Add task adapter parameters\n        trainable_params.extend(self.task_adapter_manager.get_all_trainable_parameters())\n        \n        # Add task head parameters\n        for head in self.task_heads.values():\n            trainable_params.extend([p for p in head.parameters() if p.requires_grad])\n        \n        return trainable_params\n    \n    def save_adapters(self, save_dir: str):\n        """Save task adapters to a directory."""\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # Save task configurations\n        config_path = os.path.join(save_dir, "task_configs.json")\n        config_dict = {}\n        for task_name, config in self.task_configs.items():\n            config_dict[task_name] = {\n                'task_type': config.task_type,\n                'num_labels': config.num_labels,\n                'task_name': config.task_name,\n                'use_task_adapter': config.use_task_adapter,\n                'task_adapter_reduction_factor': config.task_adapter_reduction_factor,\n                'freeze_backbone': config.freeze_backbone,\n                'learning_rate_multiplier': config.learning_rate_multiplier\n            }\n        \n        with open(config_path, 'w') as f:\n            json.dump(config_dict, f, indent=2)\n        \n        # Save task adapters\n        for task_name, adapter in self.task_adapter_manager.task_adapters.items():\n            adapter_path = os.path.join(save_dir, f"{task_name}_adapter.pt")\n            torch.save(adapter.state_dict(), adapter_path)\n        \n        # Save task heads\n        for task_name, head in self.task_heads.items():\n            head_path = os.path.join(save_dir, f"{task_name}_head.pt")\n            torch.save(head.state_dict(), head_path)\n    \n    def load_adapters(self, load_dir: str):\n        """Load task adapters from a directory."""\n        # Load task configurations\n        config_path = os.path.join(load_dir, "task_configs.json")\n        if os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                config_dict = json.load(f)\n            \n            # Update task configs\n            for task_name, config_data in config_dict.items():\n                if task_name in self.task_configs:\n                    config = self.task_configs[task_name]\n                    config.task_type = config_data['task_type']\n                    config.num_labels = config_data['num_labels']\n                    config.task_name = config_data['task_name']\n                    config.use_task_adapter = config_data['use_task_adapter']\n                    config.task_adapter_reduction_factor = config_data['task_adapter_reduction_factor']\n                    config.freeze_backbone = config_data['freeze_backbone']\n                    config.learning_rate_multiplier = config_data['learning_rate_multiplier']\n        \n        # Load task adapters\n        for task_name, adapter in self.task_adapter_manager.task_adapters.items():\n            adapter_path = os.path.join(load_dir, f"{task_name}_adapter.pt")\n            if os.path.exists(adapter_path):\n                adapter.load_state_dict(torch.load(adapter_path))\n        \n        # Load task heads\n        for task_name, head in self.task_heads.items():\n            head_path = os.path.join(load_dir, f"{task_name}_head.pt")\n            if os.path.exists(head_path):\n                head.load_state_dict(torch.load(head_path))\n\n\nclass MultiTaskAdapter(nn.Module):\n    """\n    Adapter that can handle multiple tasks simultaneously.\n    """\n    def __init__(self, task_configs: Dict[str, TaskConfig], input_dim: int):\n        super().__init__()\n        self.task_configs = task_configs\n        \n        # Create adapters for each task\n        self.task_adapters = nn.ModuleDict()\n        for task_name, config in task_configs.items():\n            if config.use_task_adapter:\n                # Adjust adapter configuration for task-specific use\n                task_adapter_config = config.adapter_config or AdapterConfig()\n                \n                # Reduce adapter dimensions for task-specific adapters\n                task_adapter_config.adapter_dim = max(\n                    task_adapter_config.adapter_dim // config.task_adapter_reduction_factor, \n                    16\n                )\n                \n                # Set task name in adapter config\n                task_adapter_config.task_name = task_name\n                \n                self.task_adapters[task_name] = AdapterLayer(task_adapter_config, input_dim)\n            else:\n                self.task_adapters[task_name] = nn.Identity()\n        \n        # Create a fusion mechanism for multi-task scenarios\n        self.task_weights = nn.ParameterDict({\n            name: nn.Parameter(torch.ones(1)) for name in task_configs.keys()\n        })\n    \n    def forward(self, hidden_states: torch.Tensor, active_tasks: List[str] = None) -> torch.Tensor:\n        """Forward pass with multiple active tasks."""\n        if active_tasks is None:\n            # If no specific tasks specified, return unchanged\n            return hidden_states\n        \n        # Apply each active task adapter\n        outputs = []\n        for task_name in active_tasks:\n            if task_name in self.task_adapters:\n                task_output = self.task_adapters[task_name](hidden_states)\n                # Only add the delta from original\n                outputs.append(task_output - hidden_states)\n        \n        if not outputs:\n            return hidden_states\n        \n        # Weighted combination of task outputs\n        combined_output = hidden_states\n        for i, (task_name, task_output) in enumerate(zip(active_tasks, outputs)):\n            weight = torch.tanh(self.task_weights[task_name])  # Use tanh to constrain weights\n            combined_output = combined_output + weight * task_output\n        \n        return combined_output\n\n\nclass ProgressiveTaskAdapter(nn.Module):\n    """\n    Adapter that progressively adapts to new tasks without forgetting previous ones.\n    """\n    def __init__(self, initial_task_config: TaskConfig, input_dim: int):\n        super().__init__()\n        self.input_dim = input_dim\n        self.task_configs = {"initial": initial_task_config}\n        \n        # Create initial adapter\n        initial_config = initial_task_config.adapter_config or AdapterConfig()\n        initial_config.task_name = "initial"\n        self.adapters = nn.ModuleDict({\n            "initial": AdapterLayer(initial_config, input_dim)\n        })\n        \n        # Create replay buffers for previous tasks (simplified)\n        self.replay_buffers = {}\n    \n    def add_task(self, task_name: str, task_config: TaskConfig):\n        """Add a new task while preserving knowledge from previous tasks."""\n        self.task_configs[task_name] = task_config\n        \n        # Create adapter for new task\n        new_config = task_config.adapter_config or AdapterConfig()\n        new_config.task_name = task_name\n        self.adapters[task_name] = AdapterLayer(new_config, self.input_dim)\n    \n    def forward(self, hidden_states: torch.Tensor, task_name: str) -> torch.Tensor:\n        """Forward pass for a specific task."""\n        if task_name not in self.adapters:\n            raise ValueError(f"Task '{task_name}' not registered")\n        \n        return self.adapters[task_name](hidden_states)\n\n\nclass TaskRoutingAdapter(nn.Module):\n    """\n    Adapter that routes inputs to appropriate task-specific adapters based on content.\n    """\n    def __init__(self, task_configs: Dict[str, TaskConfig], input_dim: int):\n        super().__init__()\n        self.task_configs = task_configs\n        \n        # Create task-specific adapters\n        self.task_adapters = nn.ModuleDict()\n        for task_name, config in task_configs.items():\n            if config.use_task_adapter:\n                task_adapter_config = config.adapter_config or AdapterConfig()\n                task_adapter_config.task_name = task_name\n                self.task_adapters[task_name] = AdapterLayer(task_adapter_config, input_dim)\n            else:\n                self.task_adapters[task_name] = nn.Identity()\n        \n        # Create a router network that predicts which task to use\n        self.router = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, len(task_configs))\n        )\n        \n        # Task names list for router output mapping\n        self.task_names = list(task_configs.keys())\n    \n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        """Forward pass with content-based task routing."""\n        # Get routing probabilities\n        routing_logits = self.router(hidden_states.mean(dim=1))  # Use sequence mean for routing\n        routing_weights = torch.softmax(routing_logits, dim=-1)\n        \n        # Apply all task adapters and weight by routing probabilities\n        output = hidden_states\n        for i, task_name in enumerate(self.task_names):\n            task_output = self.task_adapters[task_name](hidden_states)\n            # Weight by routing probability\n            weight = routing_weights[:, i:i+1].unsqueeze(1)  # Shape: [batch, 1, 1]\n            output = output + (task_output - hidden_states) * weight\n        \n        return output\n\n\ndef create_task_adapted_model(base_model: nn.Module, task_definitions: List[Tuple[str, str, int]]) -> TaskAdaptedModel:\n    """\n    Create a task-adapted model from a base model and task definitions.\n    \n    Args:\n        base_model: The base Qwen3-VL model to adapt\n        task_definitions: List of (task_name, task_type, num_labels) tuples\n    \n    Returns:\n        TaskAdaptedModel with specified tasks\n    """\n    # Create task configurations\n    task_configs = {}\n    \n    for task_name, task_type, num_labels in task_definitions:\n        # Create default adapter config\n        adapter_config = AdapterConfig(\n            adapter_dim=64,  # Reasonable default for task-specific adapters\n            adapter_scalar=1.0,\n            adapter_dropout=0.1,\n            lora_r=8,\n            lora_alpha=16,\n            lora_dropout=0.05,\n            task_name=task_name\n        )\n        \n        # Create task config\n        task_config = TaskConfig(\n            task_type=task_type,\n            num_labels=num_labels,\n            task_name=task_name,\n            adapter_config=adapter_config,\n            use_task_adapter=True,\n            task_adapter_reduction_factor=2,\n            freeze_backbone=True,\n            learning_rate_multiplier=1.0\n        )\n        \n        task_configs[task_name] = task_config\n    \n    # Create and return task adapted model\n    return TaskAdaptedModel(base_model, task_configs)\n\n\nclass EfficientTaskAdapter(nn.Module):\n    """\n    Memory and computation efficient task adapter using shared parameters.\n    """\n    def __init__(self, task_configs: Dict[str, TaskConfig], input_dim: int, shared_adapter_dim: int = 32):\n        super().__init__()\n        self.task_configs = task_configs\n        \n        # Create a shared base adapter\n        shared_config = AdapterConfig(\n            adapter_dim=shared_adapter_dim,\n            adapter_scalar=1.0,\n            adapter_dropout=0.05\n        )\n        self.shared_adapter = BottleneckAdapter(shared_config, input_dim)\n        \n        # Create task-specific projections that are much smaller\n        self.task_projections = nn.ModuleDict()\n        for task_name, config in task_configs.items():\n            # Each task has its own small projection layer\n            task_specific_dim = max(shared_adapter_dim // 2, 16)\n            self.task_projections[task_name] = nn.Sequential(\n                nn.Linear(input_dim + shared_adapter_dim, task_specific_dim),\n                nn.ReLU(),\n                nn.Linear(task_specific_dim, input_dim)\n            )\n    \n    def forward(self, hidden_states: torch.Tensor, task_name: str) -> torch.Tensor:\n        """Forward pass with shared + task-specific adaptation."""\n        # Apply shared adapter\n        shared_output = self.shared_adapter(hidden_states)\n        \n        # Concatenate original and shared output\n        combined_input = torch.cat([hidden_states, shared_output], dim=-1)\n        \n        # Apply task-specific projection\n        if task_name in self.task_projections:\n            task_output = self.task_projections[task_name](combined_input)\n            return hidden_states + task_output  # Add residual\n        else:\n            # If task not found, just return shared adaptation\n            return shared_output\n\n\ndef get_default_task_configs() -> Dict[str, TaskConfig]:\n    """\n    Get default configurations for common downstream tasks.\n    """\n    default_configs = {}\n    \n    # Classification task\n    cls_config = TaskConfig(\n        task_type="classification",\n        num_labels=2,\n        task_name="classification",\n        adapter_config=AdapterConfig(\n            adapter_dim=64,\n            adapter_scalar=1.0,\n            adapter_dropout=0.1,\n            lora_r=8,\n            lora_alpha=16,\n            lora_dropout=0.05\n        ),\n        use_task_adapter=True,\n        task_adapter_reduction_factor=2,\n        freeze_backbone=True,\n        learning_rate_multiplier=1.0\n    )\n    default_configs["classification"] = cls_config\n    \n    # Question answering task\n    qa_config = TaskConfig(\n        task_type="question_answering",\n        num_labels=2,  # Start and end positions\n        task_name="question_answering",\n        adapter_config=AdapterConfig(\n            adapter_dim=64,\n            adapter_scalar=1.0,\n            adapter_dropout=0.1,\n            lora_r=8,\n            lora_alpha=16,\n            lora_dropout=0.05\n        ),\n        use_task_adapter=True,\n        task_adapter_reduction_factor=2,\n        freeze_backbone=True,\n        learning_rate_multiplier=1.0\n    )\n    default_configs["question_answering"] = qa_config\n    \n    # Generation task\n    gen_config = TaskConfig(\n        task_type="generation",\n        num_labels=50257,  # Default vocab size\n        task_name="generation",\n        adapter_config=AdapterConfig(\n            adapter_dim=64,\n            adapter_scalar=1.0,\n            adapter_dropout=0.1,\n            lora_r=8,\n            lora_alpha=16,\n            lora_dropout=0.05\n        ),\n        use_task_adapter=True,\n        task_adapter_reduction_factor=2,\n        freeze_backbone=True,\n        learning_rate_multiplier=1.0\n    )\n    default_configs["generation"] = gen_config\n    \n    return default_configs