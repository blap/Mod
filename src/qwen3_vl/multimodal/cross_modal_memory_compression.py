"""\nCross-modal memory compression with semantic integrity maintenance\nfor the Qwen3-VL architecture.\n"""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict, Any\nfrom config.config import Qwen3VLConfig\n\n\nclass SemanticPreservingCompressor(nn.Module):\n    """\n    Compresses cross-modal representations while maintaining semantic integrity.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.vision_hidden_size = config.vision_hidden_size\n        self.compression_ratio = getattr(config, 'compression_ratio', 0.5)  # Default to 50% compression\n\n        # Projection layers for text compression\n        text_compressed_size = int(self.hidden_size * self.compression_ratio)\n        self.text_compress_proj = nn.Linear(self.hidden_size, text_compressed_size)\n        self.text_expand_proj = nn.Linear(text_compressed_size, self.hidden_size)\n\n        # Projection layers for vision compression\n        vision_compressed_size = int(self.vision_hidden_size * self.compression_ratio)\n        self.vision_compress_proj = nn.Linear(self.vision_hidden_size, vision_compressed_size)\n        self.vision_expand_proj = nn.Linear(vision_compressed_size, self.vision_hidden_size)\n\n        # For cross-modal attention, we need a common dimension\n        common_compressed_dim_raw = min(text_compressed_size, vision_compressed_size)\n\n        # Cross-modal attention for semantic alignment\n        # Ensure embed_dim is divisible by num_heads\n        num_heads = max(1, min(8, common_compressed_dim_raw // 16))  # Use smaller heads to ensure divisibility\n        # Adjust common_compressed_dim to be divisible by num_heads\n        self.common_compressed_dim = (common_compressed_dim_raw // num_heads) * num_heads\n        if self.common_compressed_dim == 0:\n            self.common_compressed_dim = num_heads * 16  # Minimum viable size\n\n        # Project to common dimension for cross-modal attention (after fixing the dimension)\n        self.text_to_common_proj = nn.Linear(text_compressed_size, self.common_compressed_dim)\n        self.vision_to_common_proj = nn.Linear(vision_compressed_size, self.common_compressed_dim)\n        self.text_from_common_proj = nn.Linear(self.common_compressed_dim, text_compressed_size)\n        self.vision_from_common_proj = nn.Linear(self.common_compressed_dim, vision_compressed_size)\n\n        self.cross_modal_attention = nn.MultiheadAttention(\n            embed_dim=self.common_compressed_dim,\n            num_heads=num_heads,\n            batch_first=True\n        )\n\n        # Layer normalization for text\n        self.text_norm_compress = nn.LayerNorm(text_compressed_size)\n        self.text_norm_expand = nn.LayerNorm(self.hidden_size)\n\n        # Layer normalization for vision\n        self.vision_norm_compress = nn.LayerNorm(vision_compressed_size)\n        self.vision_norm_expand = nn.LayerNorm(self.vision_hidden_size)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(\n        self,\n        text_features: torch.Tensor,\n        vision_features: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Compress text and vision features while maintaining semantic alignment.\n\n        Args:\n            text_features: Text features of shape (batch_size, text_seq_len, hidden_size)\n            vision_features: Vision features of shape (batch_size, vision_seq_len, vision_hidden_size)\n            attention_mask: Optional attention mask\n\n        Returns:\n            Tuple of (compressed_text_features, compressed_vision_features) with original dimensions\n        """\n        batch_size_text, seq_len_text, hidden_size_text = text_features.shape\n        batch_size_vision, seq_len_vision, hidden_size_vision = vision_features.shape\n\n        # Compress text features\n        compressed_text = self.text_compress_proj(text_features)\n        compressed_text = self.text_norm_compress(compressed_text)\n        compressed_text = F.gelu(compressed_text)\n\n        # Compress vision features\n        compressed_vision = self.vision_compress_proj(vision_features)\n        compressed_vision = self.vision_norm_compress(compressed_vision)\n        compressed_vision = F.gelu(compressed_vision)\n\n        # Project to common dimension for cross-modal attention\n        projected_text = self.text_to_common_proj(compressed_text)\n        projected_vision = self.vision_to_common_proj(compressed_vision)\n\n        # Apply cross-modal attention to maintain semantic alignment\n        # Concatenate compressed features for cross-attention\n        combined_features = torch.cat([projected_text, projected_vision], dim=1)\n\n        # Apply cross-modal attention\n        attended_features, _ = self.cross_modal_attention(\n            query=combined_features,\n            key=combined_features,\n            value=combined_features,\n            attn_mask=attention_mask\n        )\n\n        # Split back into text and vision components\n        attended_text = attended_features[:, :seq_len_text, :]\n        attended_vision = attended_features[:, seq_len_text:, :]\n\n        # Project back from common dimension\n        text_from_common = self.text_from_common_proj(attended_text)\n        vision_from_common = self.vision_from_common_proj(attended_vision)\n\n        # Expand back to original dimensions\n        expanded_text = self.text_expand_proj(text_from_common)\n        expanded_vision = self.vision_expand_proj(vision_from_common)\n\n        expanded_text = self.text_norm_expand(expanded_text)\n        expanded_vision = self.vision_norm_expand(expanded_vision)\n\n        # Apply dropout\n        expanded_text = self.dropout(expanded_text)\n        expanded_vision = self.dropout(expanded_vision)\n\n        return expanded_text, expanded_vision\n\n\nclass CrossModalMemoryBank(nn.Module):\n    """\n    Memory bank for storing and retrieving cross-modal representations efficiently.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.vision_hidden_size = config.vision_hidden_size\n        self.memory_size = getattr(config, 'memory_bank_size', 1024)\n        self.compression_ratio = getattr(config, 'compression_ratio', 0.5)\n\n        # Memory bank for compressed representations - use common dimension\n        text_compressed_size = int(self.hidden_size * self.compression_ratio)\n        vision_compressed_size = int(self.vision_hidden_size * self.compression_ratio)\n        self.common_compressed_dim = min(text_compressed_size, vision_compressed_size)\n\n        self.memory_bank = nn.Parameter(\n            torch.randn(self.memory_size, self.common_compressed_dim) * 0.02,\n            requires_grad=True\n        )\n\n        # Projections for text memory operations\n        self.text_query_proj = nn.Linear(self.hidden_size, self.common_compressed_dim)\n        self.text_key_proj = nn.Linear(self.hidden_size, self.common_compressed_dim)\n        self.text_value_proj = nn.Linear(self.hidden_size, self.common_compressed_dim)\n        self.text_expand_proj = nn.Linear(self.common_compressed_dim, self.hidden_size)\n\n        # Projections for vision memory operations\n        self.vision_query_proj = nn.Linear(self.vision_hidden_size, self.common_compressed_dim)\n        self.vision_key_proj = nn.Linear(self.vision_hidden_size, self.common_compressed_dim)\n        self.vision_value_proj = nn.Linear(self.vision_hidden_size, self.common_compressed_dim)\n        self.vision_expand_proj = nn.Linear(self.common_compressed_dim, self.vision_hidden_size)\n\n        # Layer normalization for text\n        self.text_norm = nn.LayerNorm(self.hidden_size)\n\n        # Layer normalization for vision\n        self.vision_norm = nn.LayerNorm(self.vision_hidden_size)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(\n        self,\n        text_features: torch.Tensor,\n        vision_features: torch.Tensor,\n        retrieve_only: bool = False\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Store or retrieve cross-modal representations from the memory bank.\n\n        Args:\n            text_features: Text features of shape (batch_size, seq_len, hidden_size)\n            vision_features: Vision features of shape (batch_size, vision_seq_len, vision_hidden_size)\n            retrieve_only: If True, only retrieve from memory without storing\n\n        Returns:\n            Tuple of (enhanced_text_features, enhanced_vision_features)\n        """\n        batch_size, text_seq_len, text_hidden_size = text_features.shape\n        batch_size_v, vision_seq_len, vision_hidden_size = vision_features.shape\n\n        # Project text and vision features to common compressed space separately\n        text_queries = self.text_query_proj(text_features)\n        text_keys = self.text_key_proj(text_features)\n        text_values = self.text_value_proj(text_features)\n\n        vision_queries = self.vision_query_proj(vision_features)\n        vision_keys = self.vision_key_proj(vision_features)\n        vision_values = self.vision_value_proj(vision_features)\n\n        # Combine queries for attention with memory bank\n        combined_queries = torch.cat([text_queries, vision_queries], dim=1)\n\n        # Compute attention scores with memory bank\n        memory_attn_scores = torch.matmul(combined_queries, self.memory_bank.t())  # [batch, combined_seq_len, memory_size]\n        memory_attn_weights = F.softmax(memory_attn_scores, dim=-1)\n\n        # Retrieve from memory\n        retrieved_memory = torch.matmul(memory_attn_weights, self.memory_bank)  # [batch, combined_seq_len, common_compressed_dim]\n\n        # Split back into text and vision components\n        retrieved_text = retrieved_memory[:, :text_seq_len, :]\n        retrieved_vision = retrieved_memory[:, text_seq_len:, :]\n\n        # Expand back to original dimensions\n        expanded_text = self.text_expand_proj(retrieved_text)\n        expanded_vision = self.vision_expand_proj(retrieved_vision)\n\n        # Apply normalization\n        expanded_text = self.text_norm(expanded_text)\n        expanded_vision = self.vision_norm(expanded_vision)\n\n        # Apply dropout\n        expanded_text = self.dropout(expanded_text)\n        expanded_vision = self.dropout(expanded_vision)\n\n        # Add residual connections\n        enhanced_text = text_features + expanded_text\n        enhanced_vision = vision_features + expanded_vision\n\n        return enhanced_text, enhanced_vision\n\n\nclass CrossModalSemanticIntegrityModule(nn.Module):\n    """\n    Maintains semantic integrity during cross-modal memory compression.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.vision_hidden_size = config.vision_hidden_size\n\n        # Semantic alignment networks for text\n        self.text_alignment = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size // 2),\n            nn.ReLU(),\n            nn.Linear(self.hidden_size // 2, self.hidden_size),\n            nn.Sigmoid()\n        )\n\n        # Semantic alignment networks for vision\n        self.vision_alignment = nn.Sequential(\n            nn.Linear(self.vision_hidden_size, self.vision_hidden_size // 2),\n            nn.ReLU(),\n            nn.Linear(self.vision_hidden_size // 2, self.vision_hidden_size),\n            nn.Sigmoid()\n        )\n\n        # Semantic consistency checker - uses smaller dimension\n        self.consistency_checker = nn.Sequential(\n            nn.Linear(self.hidden_size + self.vision_hidden_size, (self.hidden_size + self.vision_hidden_size) // 2),\n            nn.ReLU(),\n            nn.Linear((self.hidden_size + self.vision_hidden_size) // 2, 1),\n            nn.Sigmoid()\n        )\n\n        # Layer normalization for text\n        self.text_norm = nn.LayerNorm(self.hidden_size)\n\n        # Layer normalization for vision\n        self.vision_norm = nn.LayerNorm(self.vision_hidden_size)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(\n        self,\n        text_features: torch.Tensor,\n        vision_features: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        """\n        Apply semantic integrity maintenance to cross-modal features.\n\n        Args:\n            text_features: Text features of shape (batch_size, seq_len, hidden_size)\n            vision_features: Vision features of shape (batch_size, seq_len, vision_hidden_size)\n\n        Returns:\n            Tuple of (aligned_text_features, aligned_vision_features, consistency_score)\n        """\n        batch_size, seq_len, text_hidden_size = text_features.shape\n        batch_size_v, seq_len_v, vision_hidden_size = vision_features.shape\n\n        # Apply alignment transformations\n        text_alignment_weights = self.text_alignment(text_features)\n        vision_alignment_weights = self.vision_alignment(vision_features)\n\n        # Align features based on cross-modal context\n        aligned_text = text_features * text_alignment_weights\n        aligned_vision = vision_features * vision_alignment_weights\n\n        # Compute semantic consistency score\n        # Use mean pooled representations for consistency check\n        text_pooled = aligned_text.mean(dim=1)  # [batch, hidden_size]\n        vision_pooled = aligned_vision.mean(dim=1)  # [batch, vision_hidden_size]\n        combined_pooled = torch.cat([text_pooled, vision_pooled], dim=-1)  # [batch, hidden_size + vision_hidden_size]\n\n        consistency_score = self.consistency_checker(combined_pooled)  # [batch, 1]\n\n        # Apply normalization and dropout\n        aligned_text = self.text_norm(aligned_text)\n        aligned_vision = self.vision_norm(aligned_vision)\n        aligned_text = self.dropout(aligned_text)\n        aligned_vision = self.dropout(aligned_vision)\n\n        return aligned_text, aligned_vision, consistency_score\n\n\nclass CrossModalMemoryCompressor(nn.Module):\n    """\n    Main cross-modal memory compression module that integrates all components.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n\n        # Initialize components\n        self.semantic_preserving_compressor = SemanticPreservingCompressor(config)\n        self.memory_bank = CrossModalMemoryBank(config)\n        self.semantic_integrity_module = CrossModalSemanticIntegrityModule(config)\n\n        # Compression ratio control\n        self.compression_enabled = getattr(config, 'enable_cross_modal_compression', True)\n        self.compression_ratio = getattr(config, 'compression_ratio', 0.5)\n\n    def forward(\n        self,\n        text_features: torch.Tensor,\n        vision_features: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Compress cross-modal features while maintaining semantic integrity.\n\n        Args:\n            text_features: Text features of shape (batch_size, text_seq_len, hidden_size)\n            vision_features: Vision features of shape (batch_size, vision_seq_len, hidden_size)\n            attention_mask: Optional attention mask\n\n        Returns:\n            Tuple of (compressed_text_features, compressed_vision_features) with original dimensions\n        """\n        if not self.compression_enabled:\n            # Return original features if compression is disabled\n            return text_features, vision_features\n\n        # Ensure both features have the same sequence length for processing\n        # If they don't, we'll use the shorter one or pad appropriately\n        min_seq_len = min(text_features.size(1), vision_features.size(1))\n        text_features = text_features[:, :min_seq_len, :]\n        vision_features = vision_features[:, :min_seq_len, :]\n\n        # Apply semantic-preserving compression\n        compressed_text, compressed_vision = self.semantic_preserving_compressor(\n            text_features, vision_features, attention_mask\n        )\n\n        # Apply memory bank operations\n        enhanced_text, enhanced_vision = self.memory_bank(\n            compressed_text, compressed_vision\n        )\n\n        # Apply semantic integrity maintenance\n        final_text, final_vision, consistency_score = self.semantic_integrity_module(\n            enhanced_text, enhanced_vision\n        )\n\n        return final_text, final_vision\n\n    def get_compression_ratio(self) -> float:\n        """Get the current compression ratio."""\n        return self.compression_ratio\n\n    def update_compression_ratio(self, new_ratio: float):\n        """Update the compression ratio."""\n        self.compression_ratio = max(0.1, min(1.0, new_ratio))  # Clamp between 0.1 and 1.0