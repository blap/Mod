"""Enhanced Specialized Tensor Pools for Qwen3-VL with Intel i5-10210U Optimizations."""\n\nimport torch\nimport torch.nn as nn\nfrom collections import OrderedDict\nfrom typing import Dict, List, Optional, Tuple, Any, Union\nimport time\nimport math\nimport psutil\nfrom dataclasses import dataclass\nimport threading\nfrom utils.tensor_memory_pool import LRUTensorPool, TensorInfo\n\n\nclass AttentionTensorPool(LRUTensorPool):\n    """\n    Specialized pool for attention tensors with optimizations for Intel i5-10210U.\n    Attention tensors are typically large and accessed frequently during attention computation.\n    """\n\n    def __init__(self,\n                 max_capacity_bytes: int = 1024 * 1024 * 1024,  # 1GB default\n                 cache_line_size: int = 64,\n                 l3_cache_size: int = 6 * 1024 * 1024,  # 6MB for i5-10210U\n                 dtype: torch.dtype = torch.float16,\n                 device: Optional[torch.device] = None):\n        super().__init__(max_capacity_bytes, cache_line_size, l3_cache_size, dtype, device)\n        self.tensor_type = "attention"\n\n        # Attention-specific optimizations\n        self._setup_attention_optimizations()\n\n    def _setup_attention_optimizations(self):\n        """Set up optimizations specific to attention tensors."""\n        # For attention computation, align to cache-friendly dimensions\n        # Intel i5-10210U has 64-byte cache lines, optimize for this\n        self.alignment_factor = 64\n        \n        # For attention weights, consider common shapes in transformer models\n        self.common_shapes = [\n            (8, 1024, 1024),  # Standard attention weights\n            (8, 512, 512),    # Half-sized attention\n            (16, 256, 256),   # Multi-head attention\n            (1, 2048, 2048),  # Large sequence attention\n        ]\n\n    def _align_shape_for_attention(self, shape: Tuple[int, ...]) -> Tuple[int, ...]:\n        """Align shape dimensions for optimal attention computation on i5-10210U."""\n        shape_list = list(shape)\n        \n        # Align the last dimension (head dimension) to 64-byte boundaries\n        if len(shape_list) >= 1:\n            last_dim = shape_list[-1]\n            element_size = torch.tensor([], dtype=self.dtype).element_size()\n            elements_per_cache_line = 64 // element_size  # Usually 16 for float16, 8 for float32\n            aligned_last_dim = ((last_dim + elements_per_cache_line - 1) // elements_per_cache_line) * elements_per_cache_line\n            shape_list[-1] = aligned_last_dim\n        \n        # For multi-dimensional attention tensors, align sequence dimensions too\n        if len(shape_list) >= 2:\n            seq_dim = shape_list[-2]\n            aligned_seq_dim = ((seq_dim + 31) // 32) * 32  # Align to 32 for SIMD\n            shape_list[-2] = aligned_seq_dim\n            \n        return tuple(shape_list)\n\n    def get_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """Get attention tensor with hardware-optimized alignment."""\n        dtype = dtype or self.dtype\n        \n        # Apply attention-specific alignment\n        aligned_shape = self._align_shape_for_attention(shape)\n        \n        # Call parent method with aligned shape\n        tensor = super().get_tensor(aligned_shape, dtype)\n        \n        # If original shape is different, return a view with the correct shape\n        if tensor.shape != shape:\n            # Create a new tensor with the exact requested shape\n            original_tensor = tensor\n            tensor = torch.empty(shape, dtype=dtype, device=self.device)\n            \n            # Copy data if possible (only if shapes are compatible)\n            min_elements = min(original_tensor.numel(), tensor.numel())\n            if min_elements > 0:\n                tensor_flat = tensor.view(-1)\n                original_flat = original_tensor.view(-1)\n                tensor_flat[:min_elements] = original_flat[:min_elements]\n        \n        return tensor\n\n\nclass KVCachePool(LRUTensorPool):\n    """\n    Specialized pool for KV cache tensors with optimizations for Intel i5-10210U.\n    KV cache tensors are accessed sequentially during generation, optimize for this pattern.\n    """\n    \n    def __init__(self, \n                 max_capacity_bytes: int = 1024 * 1024 * 768,  # 768MB default for KV cache\n                 cache_line_size: int = 128,  # Larger for sequential access\n                 l3_cache_size: int = 6 * 1024 * 1024,  # 6MB for i5-10210U\n                 dtype: torch.dtype = torch.float16,\n                 device: Optional[torch.device] = None):\n        super().__init__(max_capacity_bytes, cache_line_size, l3_cache_size, dtype, device)\n        self.tensor_type = "kv_cache"\n        \n        # KV cache-specific optimizations\n        self._setup_kv_cache_optimizations()\n\n    def _setup_kv_cache_optimizations(self):\n        """Set up optimizations specific to KV cache tensors."""\n        # For KV cache, use larger cache line alignment for sequential access\n        self.alignment_factor = 128  # Larger alignment for sequential access pattern\n        \n        # Common KV cache shapes for transformer models\n        self.common_shapes = [\n            (1, 32, 2048, 128),  # Standard KV cache (batch=1, heads=32, seq=2048, head_dim=128)\n            (1, 16, 1024, 64),  # Smaller KV cache\n            (2, 16, 512, 64),   # Batch=2 KV cache\n            (1, 8, 4096, 128),  # Long sequence KV cache\n        ]\n\n    def _align_shape_for_kv_cache(self, shape: Tuple[int, ...]) -> Tuple[int, ...]:\n        """Align shape dimensions for optimal KV cache usage on i5-10210U."""\n        shape_list = list(shape)\n        \n        # For KV cache, align the sequence length dimension (dim 2) for better cache line utilization\n        if len(shape_list) >= 3:\n            seq_len = shape_list[2]  # Assuming format [batch, heads, seq_len, head_dim]\n            aligned_seq_len = ((seq_len + 63) // 64) * 64  # Align to 64 for cache efficiency\n            shape_list[2] = aligned_seq_len\n            \n        # Align head dimension (dim 3) to cache line boundaries\n        if len(shape_list) >= 4:\n            head_dim = shape_list[3]\n            element_size = torch.tensor([], dtype=self.dtype).element_size()\n            elements_per_cache_line = 64 // element_size\n            aligned_head_dim = ((head_dim + elements_per_cache_line - 1) // elements_per_cache_line) * elements_per_cache_line\n            shape_list[3] = aligned_head_dim\n            \n        return tuple(shape_list)\n\n    def get_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """Get KV cache tensor with hardware-optimized alignment."""\n        dtype = dtype or self.dtype\n        \n        # Apply KV cache-specific alignment\n        aligned_shape = self._align_shape_for_kv_cache(shape)\n        \n        # Call parent method with aligned shape\n        tensor = super().get_tensor(aligned_shape, dtype)\n        \n        # If original shape is different, return a view with the correct shape\n        if tensor.shape != shape:\n            # Create a new tensor with the exact requested shape\n            original_tensor = tensor\n            tensor = torch.empty(shape, dtype=dtype, device=self.device)\n            \n            # Copy data if possible (only if shapes are compatible)\n            min_elements = min(original_tensor.numel(), tensor.numel())\n            if min_elements > 0:\n                tensor_flat = tensor.view(-1)\n                original_flat = original_tensor.view(-1)\n                tensor_flat[:min_elements] = original_flat[:min_elements]\n        \n        return tensor\n\n\nclass ImageEmbeddingPool(LRUTensorPool):\n    """\n    Specialized pool for image embedding tensors with optimizations for Intel i5-10210U.\n    Image embeddings have specific access patterns in vision encoders.\n    """\n    \n    def __init__(self, \n                 max_capacity_bytes: int = 512 * 1024 * 1024,  # 512MB default for image embeddings\n                 cache_line_size: int = 256,  # Larger for image processing\n                 l3_cache_size: int = 6 * 1024 * 1024,  # 6MB for i5-10210U\n                 dtype: torch.dtype = torch.float16,\n                 device: Optional[torch.device] = None):\n        super().__init__(max_capacity_bytes, cache_line_size, l3_cache_size, dtype, device)\n        self.tensor_type = "image_embeddings"\n        \n        # Image embedding-specific optimizations\n        self._setup_image_embedding_optimizations()\n\n    def _setup_image_embedding_optimizations(self):\n        """Set up optimizations specific to image embedding tensors."""\n        # For image embeddings, use larger alignment for image processing efficiency\n        self.alignment_factor = 256  # Larger alignment for image processing\n        \n        # Common image embedding shapes for vision models\n        self.common_shapes = [\n            (1, 576, 1152),      # Standard vision transformer patches\n            (1, 256, 768),       # Smaller image embeddings\n            (1, 1024, 512),      # Alternative embedding dimension\n            (2, 576, 1152),      # Batch=2 image embeddings\n        ]\n\n    def _align_shape_for_image_embeddings(self, shape: Tuple[int, ...]) -> Tuple[int, ...]:\n        """Align shape dimensions for optimal image embedding usage on i5-10210U."""\n        shape_list = list(shape)\n        \n        # For image embeddings, align the feature dimension (dim 2) for better processing\n        if len(shape_list) >= 3:\n            feat_dim = shape_list[2]  # Assuming format [batch, patches, features]\n            element_size = torch.tensor([], dtype=self.dtype).element_size()\n            elements_per_alignment = 256 // element_size  # Align to 256 bytes for image processing\n            aligned_feat_dim = ((feat_dim + elements_per_alignment - 1) // elements_per_alignment) * elements_per_alignment\n            shape_list[2] = aligned_feat_dim\n            \n        # Align patch dimension (dim 1) to processing-friendly boundaries\n        if len(shape_list) >= 2:\n            patch_dim = shape_list[1]\n            aligned_patch_dim = ((patch_dim + 15) // 16) * 16  # Align to 16 for processing\n            shape_list[1] = aligned_patch_dim\n            \n        return tuple(shape_list)\n\n    def get_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """Get image embedding tensor with hardware-optimized alignment."""\n        dtype = dtype or self.dtype\n        \n        # Apply image embedding-specific alignment\n        aligned_shape = self._align_shape_for_image_embeddings(shape)\n        \n        # Call parent method with aligned shape\n        tensor = super().get_tensor(aligned_shape, dtype)\n        \n        # If original shape is different, return a view with the correct shape\n        if tensor.shape != shape:\n            # Create a new tensor with the exact requested shape\n            original_tensor = tensor\n            tensor = torch.empty(shape, dtype=dtype, device=self.device)\n            \n            # Copy data if possible (only if shapes are compatible)\n            min_elements = min(original_tensor.numel(), tensor.numel())\n            if min_elements > 0:\n                tensor_flat = tensor.view(-1)\n                original_flat = original_tensor.view(-1)\n                tensor_flat[:min_elements] = original_flat[:min_elements]\n        \n        return tensor\n\n\nclass TextEmbeddingPool(LRUTensorPool):\n    """\n    Specialized pool for text embedding tensors with optimizations for Intel i5-10210U.\n    Text embeddings have different access patterns compared to image embeddings.\n    """\n    \n    def __init__(self, \n                 max_capacity_bytes: int = 512 * 1024 * 1024,  # 512MB default for text embeddings\n                 cache_line_size: int = 512,  # Larger for text processing\n                 l3_cache_size: int = 6 * 1024 * 1024,  # 6MB for i5-10210U\n                 dtype: torch.dtype = torch.float16,\n                 device: Optional[torch.device] = None):\n        super().__init__(max_capacity_bytes, cache_line_size, l3_cache_size, dtype, device)\n        self.tensor_type = "text_embeddings"\n        \n        # Text embedding-specific optimizations\n        self._setup_text_embedding_optimizations()\n\n    def _setup_text_embedding_optimizations(self):\n        """Set up optimizations specific to text embedding tensors."""\n        # For text embeddings, use larger alignment for text processing efficiency\n        self.alignment_factor = 512  # Larger alignment for text processing\n        \n        # Common text embedding shapes for language models\n        self.common_shapes = [\n            (1, 512, 4096),      # Standard text embeddings\n            (1, 128, 2048),      # Smaller text embeddings\n            (1, 1024, 2048),     # Longer sequence embeddings\n            (4, 512, 4096),      # Batch=4 text embeddings\n        ]\n\n    def _align_shape_for_text_embeddings(self, shape: Tuple[int, ...]) -> Tuple[int, ...]:\n        """Align shape dimensions for optimal text embedding usage on i5-10210U."""\n        shape_list = list(shape)\n        \n        # For text embeddings, align the embedding dimension (dim 2) for better processing\n        if len(shape_list) >= 3:\n            embed_dim = shape_list[2]  # Assuming format [batch, seq_len, embed_dim]\n            element_size = torch.tensor([], dtype=self.dtype).element_size()\n            elements_per_alignment = 512 // element_size  # Align to 512 bytes for text processing\n            aligned_embed_dim = ((embed_dim + elements_per_alignment - 1) // elements_per_alignment) * elements_per_alignment\n            shape_list[2] = aligned_embed_dim\n            \n        # Align sequence length dimension (dim 1) to processing-friendly boundaries\n        if len(shape_list) >= 2:\n            seq_len = shape_list[1]\n            aligned_seq_len = ((seq_len + 31) // 32) * 32  # Align to 32 for processing\n            shape_list[1] = aligned_seq_len\n            \n        return tuple(shape_list)\n\n    def get_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """Get text embedding tensor with hardware-optimized alignment."""\n        dtype = dtype or self.dtype\n        \n        # Apply text embedding-specific alignment\n        aligned_shape = self._align_shape_for_text_embeddings(shape)\n        \n        # Call parent method with aligned shape\n        tensor = super().get_tensor(aligned_shape, dtype)\n        \n        # If original shape is different, return a view with the correct shape\n        if tensor.shape != shape:\n            # Create a new tensor with the exact requested shape\n            original_tensor = tensor\n            tensor = torch.empty(shape, dtype=dtype, device=self.device)\n            \n            # Copy data if possible (only if shapes are compatible)\n            min_elements = min(original_tensor.numel(), tensor.numel())\n            if min_elements > 0:\n                tensor_flat = tensor.view(-1)\n                original_flat = original_tensor.view(-1)\n                tensor_flat[:min_elements] = original_flat[:min_elements]\n        \n        return tensor\n\n\nclass IntermediateActivationPool(LRUTensorPool):\n    """\n    Specialized pool for intermediate activation tensors with optimizations for Intel i5-10210U.\n    Intermediate activations are temporary results in feed-forward networks.\n    """\n    \n    def __init__(self, \n                 max_capacity_bytes: int = 768 * 1024 * 1024,  # 768MB default for intermediate activations\n                 cache_line_size: int = 64,  # Standard alignment\n                 l3_cache_size: int = 6 * 1024 * 1024,  # 6MB for i5-10210U\n                 dtype: torch.dtype = torch.float16,\n                 device: Optional[torch.device] = None):\n        super().__init__(max_capacity_bytes, cache_line_size, l3_cache_size, dtype, device)\n        self.tensor_type = "intermediate"\n        \n        # Intermediate activation-specific optimizations\n        self._setup_intermediate_optimizations()\n\n    def _setup_intermediate_optimizations(self):\n        """Set up optimizations specific to intermediate activation tensors."""\n        # For intermediate activations, use standard alignment but optimize for reuse\n        self.alignment_factor = 64  # Standard alignment\n        \n        # Common intermediate activation shapes for transformer models\n        self.common_shapes = [\n            (1, 512, 11008),     # Standard FFN intermediate (2816 * 4)\n            (1, 1024, 4096),     # Alternative FFN intermediate\n            (2, 512, 11008),     # Batch=2 intermediate\n            (1, 256, 8192),      # Smaller intermediate\n        ]\n\n    def _align_shape_for_intermediate(self, shape: Tuple[int, ...]) -> Tuple[int, ...]:\n        """Align shape dimensions for optimal intermediate activation usage on i5-10210U."""\n        shape_list = list(shape)\n        \n        # For intermediate activations, align the feature dimension (dim 2) for better processing\n        if len(shape_list) >= 3:\n            feat_dim = shape_list[2]  # Assuming format [batch, seq_len, features]\n            element_size = torch.tensor([], dtype=self.dtype).element_size()\n            elements_per_cache_line = 64 // element_size\n            aligned_feat_dim = ((feat_dim + elements_per_cache_line - 1) // elements_per_cache_line) * elements_per_cache_line\n            shape_list[2] = aligned_feat_dim\n            \n        return tuple(shape_list)\n\n    def get_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """Get intermediate activation tensor with hardware-optimized alignment."""\n        dtype = dtype or self.dtype\n        \n        # Apply intermediate-specific alignment\n        aligned_shape = self._align_shape_for_intermediate(shape)\n        \n        # Call parent method with aligned shape\n        tensor = super().get_tensor(aligned_shape, dtype)\n        \n        # If original shape is different, return a view with the correct shape\n        if tensor.shape != shape:\n            # Create a new tensor with the exact requested shape\n            original_tensor = tensor\n            tensor = torch.empty(shape, dtype=dtype, device=self.device)\n            \n            # Copy data if possible (only if shapes are compatible)\n            min_elements = min(original_tensor.numel(), tensor.numel())\n            if min_elements > 0:\n                tensor_flat = tensor.view(-1)\n                original_flat = original_tensor.view(-1)\n                tensor_flat[:min_elements] = original_flat[:min_elements]\n        \n        return tensor\n\n\nclass SpecializedPoolManager:\n    """\n    Manager for all specialized tensor pools with Intel i5-10210U optimizations.\n    """\n    \n    def __init__(self, \n                 base_capacity: int = 3 * 1024 * 1024 * 1024,  # 3GB base capacity\n                 cache_line_size: int = 64,\n                 l3_cache_size: int = 6 * 1024 * 1024,  # 6MB for i5-10210U\n                 dtype: torch.dtype = torch.float16,\n                 device: Optional[torch.device] = None):\n        """\n        Initialize the specialized pool manager.\n        \n        Args:\n            base_capacity: Base capacity for all pools combined\n            cache_line_size: Size of cache line for alignment\n            l3_cache_size: Size of L3 cache for optimization\n            dtype: Default tensor data type\n            device: Device to allocate tensors on\n        """\n        self.base_capacity = base_capacity\n        self.cache_line_size = cache_line_size\n        self.l3_cache_size = l3_cache_size\n        self.dtype = dtype\n        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        \n        # Initialize all specialized pools\n        self.pools: Dict[str, LRUTensorPool] = {}\n        self._initialize_specialized_pools()\n        \n        # Track tensor-to-pool mapping\n        self.tensor_to_pool: Dict[int, str] = {}\n        \n        # Hardware-specific optimizations\n        self._apply_hardware_optimizations()\n\n    def _initialize_specialized_pools(self):\n        """Initialize all specialized tensor pools."""\n        # Calculate capacity distribution among pools\n        total_capacity = self.base_capacity\n        \n        # Attention pool gets 25% of capacity\n        attention_capacity = int(total_capacity * 0.25)\n        # KV cache gets 30% of capacity (usually larger)\n        kv_cache_capacity = int(total_capacity * 0.30)\n        # Image embeddings get 15% of capacity\n        image_emb_capacity = int(total_capacity * 0.15)\n        # Text embeddings get 15% of capacity\n        text_emb_capacity = int(total_capacity * 0.15)\n        # Intermediate activations get 15% of capacity\n        intermediate_capacity = int(total_capacity * 0.15)\n        \n        # Initialize each specialized pool\n        self.pools['attention'] = AttentionTensorPool(\n            max_capacity_bytes=attention_capacity,\n            cache_line_size=self.cache_line_size,\n            l3_cache_size=self.l3_cache_size,\n            dtype=self.dtype,\n            device=self.device\n        )\n        \n        self.pools['kv_cache'] = KVCachePool(\n            max_capacity_bytes=kv_cache_capacity,\n            cache_line_size=self.cache_line_size,\n            l3_cache_size=self.l3_cache_size,\n            dtype=self.dtype,\n            device=self.device\n        )\n        \n        self.pools['image_embeddings'] = ImageEmbeddingPool(\n            max_capacity_bytes=image_emb_capacity,\n            cache_line_size=self.cache_line_size,\n            l3_cache_size=self.l3_cache_size,\n            dtype=self.dtype,\n            device=self.device\n        )\n        \n        self.pools['text_embeddings'] = TextEmbeddingPool(\n            max_capacity_bytes=text_emb_capacity,\n            cache_line_size=self.cache_line_size,\n            l3_cache_size=self.l3_cache_size,\n            dtype=self.dtype,\n            device=self.device\n        )\n        \n        self.pools['intermediate'] = IntermediateActivationPool(\n            max_capacity_bytes=intermediate_capacity,\n            cache_line_size=self.cache_line_size,\n            l3_cache_size=self.l3_cache_size,\n            dtype=self.dtype,\n            device=self.device\n        )\n        \n        # General pool for other tensor types (fallback)\n        general_capacity = int(total_capacity * 0.10)  # 10% for general use\n        self.pools['general'] = LRUTensorPool(\n            max_capacity_bytes=general_capacity,\n            cache_line_size=self.cache_line_size,\n            l3_cache_size=self.l3_cache_size,\n            dtype=self.dtype,\n            device=self.device\n        )\n\n    def _apply_hardware_optimizations(self):\n        """\n        Apply hardware-specific optimizations for Intel i5-10210U:\n        - 4 cores + 8 threads\n        - 6MB L3 cache\n        - Memory alignment for cache efficiency\n        """\n        # Adjust pool sizes based on L3 cache size to avoid thrashing\n        l3_cache_per_core = self.l3_cache_size // 4  # 1.5MB per core\n        \n        for pool_name, pool in self.pools.items():\n            # Limit individual pool size to avoid cache thrashing\n            max_pool_size = min(pool.max_capacity_bytes, l3_cache_per_core * 2)  # 3MB max per pool\n            pool.max_capacity_bytes = max_pool_size\n\n    def get_tensor(self, shape: Tuple[int, ...], tensor_type: str, dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """\n        Get a tensor from the appropriate specialized pool.\n        \n        Args:\n            shape: Shape of the tensor to allocate\n            tensor_type: Type of tensor (determines which pool to use)\n            dtype: Data type of the tensor (optional)\n            \n        Returns:\n            Allocated tensor from the specialized pool\n        """\n        dtype = dtype or self.dtype\n        \n        # Select appropriate specialized pool\n        if tensor_type in self.pools:\n            pool = self.pools[tensor_type]\n        else:\n            # Use general pool for unknown tensor types\n            pool = self.pools['general']\n        \n        # Get tensor from specialized pool\n        tensor = pool.get_tensor(shape, dtype)\n        \n        # Track tensor-to-pool mapping\n        tensor_id = id(tensor)\n        self.tensor_to_pool[tensor_id] = tensor_type\n        \n        return tensor\n\n    def return_tensor(self, tensor: torch.Tensor, tensor_type: Optional[str] = None):\n        """\n        Return a tensor to the appropriate specialized pool.\n        \n        Args:\n            tensor: Tensor to return to pool\n            tensor_type: Type of tensor (optional, will be inferred if not provided)\n        """\n        tensor_id = id(tensor)\n        \n        # Determine which pool to return to\n        if tensor_type is None:\n            tensor_type = self.tensor_to_pool.get(tensor_id, 'general')\n        \n        # Return to appropriate specialized pool\n        if tensor_type in self.pools:\n            self.pools[tensor_type].return_tensor(tensor)\n        \n        # Remove from tracking\n        if tensor_id in self.tensor_to_pool:\n            del self.tensor_to_pool[tensor_id]\n\n    def get_pool_stats(self) -> Dict[str, Any]:\n        """Get statistics for all specialized pools."""\n        stats = {}\n        for pool_name, pool in self.pools.items():\n            stats[pool_name] = pool.get_stats()\n        \n        # Add aggregate stats\n        total_allocated = sum(s['current_size_bytes'] for s in stats.values())\n        total_max = sum(s['max_capacity_bytes'] for s in stats.values())\n        \n        stats['aggregate'] = {\n            'total_allocated_bytes': total_allocated,\n            'total_max_capacity_bytes': total_max,\n            'total_utilization_percent': (total_allocated / total_max * 100) if total_max > 0 else 0,\n            'total_tensors': sum(s['pool_size'] for s in stats.values())\n        }\n        \n        return stats\n\n    def clear_all_pools(self):\n        """Clear all specialized tensor pools."""\n        for pool in self.pools.values():\n            pool.clear()\n        self.tensor_to_pool.clear()\n\n\n# Example usage and testing\nif __name__ == "__main__":\n    print("Testing Specialized Tensor Pools...")\n    \n    # Create specialized pool manager\n    pool_manager = SpecializedPoolManager(\n        base_capacity=1024 * 1024 * 1024,  # 1GB total\n        dtype=torch.float16,\n        device=torch.device('cpu')  # Use CPU for testing\n    )\n    \n    # Test attention tensor allocation\n    print("\n1. Testing attention tensor allocation...")\n    attention_tensor = pool_manager.get_tensor((8, 1024, 1024), 'attention')\n    print(f"Allocated attention tensor: {attention_tensor.shape}, {attention_tensor.dtype}")\n    \n    # Test KV cache tensor allocation\n    print("\n2. Testing KV cache tensor allocation...")\n    kv_tensor = pool_manager.get_tensor((1, 32, 2048, 128), 'kv_cache')\n    print(f"Allocated KV cache tensor: {kv_tensor.shape}, {kv_tensor.dtype}")\n    \n    # Test image embedding tensor allocation\n    print("\n3. Testing image embedding tensor allocation...")\n    img_tensor = pool_manager.get_tensor((1, 576, 1152), 'image_embeddings')\n    print(f"Allocated image embedding tensor: {img_tensor.shape}, {img_tensor.dtype}")\n    \n    # Test text embedding tensor allocation\n    print("\n4. Testing text embedding tensor allocation...")\n    text_tensor = pool_manager.get_tensor((1, 512, 4096), 'text_embeddings')\n    print(f"Allocated text embedding tensor: {text_tensor.shape}, {text_tensor.dtype}")\n    \n    # Test intermediate activation tensor allocation\n    print("\n5. Testing intermediate activation tensor allocation...")\n    int_tensor = pool_manager.get_tensor((1, 512, 11008), 'intermediate')\n    print(f"Allocated intermediate tensor: {int_tensor.shape}, {int_tensor.dtype}")\n    \n    # Return tensors to pools\n    print("\n6. Returning tensors to pools...")\n    pool_manager.return_tensor(attention_tensor, 'attention')\n    pool_manager.return_tensor(kv_tensor, 'kv_cache')\n    pool_manager.return_tensor(img_tensor, 'image_embeddings')\n    pool_manager.return_tensor(text_tensor, 'text_embeddings')\n    pool_manager.return_tensor(int_tensor, 'intermediate')\n    \n    print("Tensors returned to pools successfully")\n    \n    # Display pool statistics\n    print("\n7. Specialized pool statistics:")\n    stats = pool_manager.get_pool_stats()\n    for pool_name, pool_stats in stats.items():\n        if isinstance(pool_stats, dict) and 'utilization_percent' in pool_stats:\n            if pool_name != 'aggregate':\n                print(f"  {pool_name}: {pool_stats['utilization_percent']:.2f}% utilization, "\n                      f"{pool_stats['hit_rate']:.2f} hit rate")\n    \n    print(f"\n  Aggregate: {stats['aggregate']['total_utilization_percent']:.2f}% total utilization")\n    \n    print("\nSpecialized tensor pools test completed successfully!")