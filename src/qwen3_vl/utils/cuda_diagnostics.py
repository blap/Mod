"""\nCUDA Diagnostic Module for Qwen3-VL-2B-Instruct Project\n\nThis module provides diagnostic tools for debugging CUDA operations,\nincluding detailed error reports, memory diagnostics, and performance analysis.\n"""\nimport torch\nimport logging\nfrom typing import Dict, List, Optional, Tuple\nimport sys\nimport os\nimport subprocess\nimport platform\nfrom datetime import datetime\nimport psutil\nfrom utils.cuda_error_handler import CUDAErrorHandler\n\n\nclass CUDADiagnostics:\n    """\n    Comprehensive diagnostic tools for CUDA operations in the Qwen3-VL-2B-Instruct project.\n    """\n    \n    def __init__(self):\n        self.logger = logging.getLogger(__name__)\n        self.error_handler = CUDAErrorHandler()\n    \n    def generate_system_diagnostic_report(self) -> Dict:\n        """\n        Generate a comprehensive system diagnostic report.\n        \n        Returns:\n            Dictionary containing system diagnostic information\n        """\n        report = {\n            "timestamp": datetime.now().isoformat(),\n            "system_info": {\n                "platform": platform.platform(),\n                "python_version": sys.version,\n                "torch_version": torch.__version__,\n                "cuda_available": torch.cuda.is_available(),\n                "cuda_version": torch.version.cuda if torch.version.cuda else "N/A",\n                "driver_version": self._get_driver_version(),\n            },\n            "gpu_info": [],\n            "memory_info": self._get_memory_info(),\n            "environment_vars": self._get_cuda_environment_vars(),\n        }\n        \n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                gpu_info = {\n                    "device_id": i,\n                    "name": torch.cuda.get_device_name(i),\n                    "compute_capability": torch.cuda.get_device_capability(i),\n                    "total_memory": torch.cuda.get_device_properties(i).total_memory,\n                    "major": torch.cuda.get_device_properties(i).major,\n                    "minor": torch.cuda.get_device_properties(i).minor,\n                }\n                report["gpu_info"].append(gpu_info)\n        \n        return report\n    \n    def _get_driver_version(self) -> str:\n        """\n        Get the CUDA driver version.\n        """\n        try:\n            if torch.cuda.is_available():\n                # Get driver version using nvidia-smi if available\n                result = subprocess.run(['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader,nounits'], \n                                      capture_output=True, text=True, timeout=10)\n                if result.returncode == 0:\n                    return result.stdout.strip().split('\n')[0] if result.stdout.strip() else "N/A"\n        except Exception:\n            pass\n        return "N/A"\n    \n    def _get_memory_info(self) -> Dict:\n        """\n        Get system and GPU memory information.\n        """\n        memory_info = {\n            "system_memory": {\n                "total": psutil.virtual_memory().total,\n                "available": psutil.virtual_memory().available,\n                "percent_used": psutil.virtual_memory().percent,\n            },\n            "gpu_memory": [],\n        }\n        \n        if torch.cuda.is_available():\n            for i in range(torch.cuda.device_count()):\n                try:\n                    allocated = torch.cuda.memory_allocated(i)\n                    reserved = torch.cuda.memory_reserved(i)\n                    total = torch.cuda.get_device_properties(i).total_memory\n                    memory_info["gpu_memory"].append({\n                        "device_id": i,\n                        "allocated": allocated,\n                        "reserved": reserved,\n                        "total": total,\n                        "free": total - allocated,\n                        "utilization": allocated / total if total > 0 else 0,\n                    })\n                except Exception as e:\n                    self.logger.warning(f"Could not get memory info for GPU {i}: {e}")\n        \n        return memory_info\n    \n    def _get_cuda_environment_vars(self) -> Dict:\n        """\n        Get CUDA-related environment variables.\n        """\n        cuda_vars = {}\n        for key, value in os.environ.items():\n            if 'CUDA' in key.upper() or 'NV' in key.upper():\n                cuda_vars[key] = value\n        return cuda_vars\n    \n    def diagnose_cuda_operation(self, operation_name: str, tensors: List[torch.Tensor]) -> Dict:\n        """\n        Diagnose a CUDA operation with detailed information.\n        \n        Args:\n            operation_name: Name of the operation to diagnose\n            tensors: List of tensors involved in the operation\n            \n        Returns:\n            Dictionary with diagnostic information\n        """\n        diagnostic_info = {\n            "operation": operation_name,\n            "timestamp": datetime.now().isoformat(),\n            "tensors": [],\n            "memory_before": self.error_handler.check_memory_usage(),\n        }\n        \n        # Collect tensor information\n        for i, tensor in enumerate(tensors):\n            tensor_info = {\n                "index": i,\n                "shape": list(tensor.shape),\n                "dtype": str(tensor.dtype),\n                "device": str(tensor.device),\n                "requires_grad": tensor.requires_grad,\n                "is_cuda": tensor.is_cuda,\n                "size_bytes": tensor.element_size() * tensor.nelement(),\n            }\n            diagnostic_info["tensors"].append(tensor_info)\n        \n        return diagnostic_info\n    \n    def diagnose_memory_pool(self) -> Dict:\n        """\n        Diagnose the current state of the memory pool.\n        \n        Returns:\n            Dictionary with memory pool diagnostic information\n        """\nfrom utils.cuda_wrapper import cuda_memory_pool\n        \n        diagnostic_info = {\n            "timestamp": datetime.now().isoformat(),\n            "memory_pool_stats": cuda_memory_pool.get_stats(),\n            "torch_memory_stats": {\n                "allocated": torch.cuda.memory_allocated() if torch.cuda.is_available() else 0,\n                "reserved": torch.cuda.memory_reserved() if torch.cuda.is_available() else 0,\n                "max_allocated": torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0,\n                "max_reserved": torch.cuda.max_memory_reserved() if torch.cuda.is_available() else 0,\n            },\n        }\n        \n        return diagnostic_info\n    \n    def log_detailed_error_info(self, operation_name: str, exception: Exception, tensors: Optional[List[torch.Tensor]] = None):\n        """\n        Log detailed error information for debugging.\n        \n        Args:\n            operation_name: Name of the operation that failed\n            exception: The exception that occurred\n            tensors: Optional list of tensors involved in the operation\n        """\n        error_info = {\n            "operation": operation_name,\n            "error_type": type(exception).__name__,\n            "error_message": str(exception),\n            "timestamp": datetime.now().isoformat(),\n        }\n        \n        if tensors:\n            tensor_info = []\n            for i, tensor in enumerate(tensors):\n                if torch.is_tensor(tensor):\n                    tensor_info.append({\n                        "index": i,\n                        "shape": list(tensor.shape),\n                        "dtype": str(tensor.dtype),\n                        "device": str(tensor.device),\n                        "requires_grad": tensor.requires_grad,\n                        "is_cuda": tensor.is_cuda,\n                    })\n            error_info["tensors"] = tensor_info\n        \n        # Get memory info at the time of error\n        try:\n            allocated, reserved, max_memory = self.error_handler.check_memory_usage()\n            error_info["memory_at_error"] = {\n                "allocated_mb": allocated,\n                "reserved_mb": reserved,\n                "max_memory_mb": max_memory,\n            }\n        except Exception as e:\n            error_info["memory_at_error"] = f"Could not get memory info: {e}"\n        \n        # Log the detailed error info\n        self.logger.error(f"CUDA Operation Error Report: {error_info}")\n        \n        # Also print to console for immediate visibility\n        print(f"\n--- CUDA ERROR DIAGNOSTICS ---")\n        print(f"Operation: {operation_name}")\n        print(f"Error Type: {type(exception).__name__}")\n        print(f"Error Message: {str(exception)}")\n        if tensors:\n            print(f"Tensors Involved: {len(tensors)}")\n            for i, tensor in enumerate(tensors):\n                if torch.is_tensor(tensor):\n                    print(f"  Tensor {i}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}")\n        print(f"-----------------------------\n")\n    \n    def check_for_memory_leaks(self) -> Dict:\n        """\n        Check for potential memory leaks by comparing memory usage over time.\n        \n        Returns:\n            Dictionary with memory leak analysis\n        """\n        if not torch.cuda.is_available():\n            return {"error": "CUDA not available"}\n        \n        # Get initial memory stats\n        initial_allocated = torch.cuda.memory_allocated()\n        initial_reserved = torch.cuda.memory_reserved()\n        \n        # Synchronize to ensure all operations are complete\n        torch.cuda.synchronize()\n        \n        # Get final memory stats after synchronization\n        final_allocated = torch.cuda.memory_allocated()\n        final_reserved = torch.cuda.memory_reserved()\n        \n        analysis = {\n            "timestamp": datetime.now().isoformat(),\n            "initial_allocated": initial_allocated,\n            "initial_reserved": initial_reserved,\n            "final_allocated": final_allocated,\n            "final_reserved": final_reserved,\n            "allocated_change": final_allocated - initial_allocated,\n            "reserved_change": final_reserved - initial_reserved,\n            "potential_leak": (final_allocated > initial_allocated) or (final_reserved > initial_reserved),\n        }\n        \n        return analysis\n    \n    def run_comprehensive_diagnostic(self) -> Dict:\n        """\n        Run a comprehensive diagnostic of the CUDA system.\n        \n        Returns:\n            Dictionary with comprehensive diagnostic information\n        """\n        self.logger.info("Running comprehensive CUDA diagnostic...")\n        \n        diagnostic_report = {\n            "system_diagnostic": self.generate_system_diagnostic_report(),\n            "memory_pool_diagnostic": self.diagnose_memory_pool(),\n            "memory_leak_check": self.check_for_memory_leaks(),\n            "cuda_availability": self.error_handler.check_cuda_status(),\n        }\n        \n        self.logger.info("Comprehensive CUDA diagnostic completed")\n        \n        return diagnostic_report\n\n\n# Global instance for use across the application\ncuda_diagnostics = CUDADiagnostics()\n\n\ndef print_diagnostics_report():\n    """\n    Print a formatted diagnostics report to the console.\n    """\n    report = cuda_diagnostics.run_comprehensive_diagnostic()\n    \n    print("\n" + "="*60)\n    print("CUDA DIAGNOSTICS REPORT")\n    print("="*60)\n    \n    # System info\n    sys_info = report["system_diagnostic"]["system_info"]\n    print(f"System: {sys_info['platform']}")\n    print(f"Python: {sys_info['python_version']}")\n    print(f"PyTorch: {sys_info['torch_version']}")\n    print(f"CUDA Available: {sys_info['cuda_available']}")\n    print(f"CUDA Version: {sys_info['cuda_version']}")\n    \n    # GPU info\n    print(f"\nGPUs Detected: {len(report['system_diagnostic']['gpu_info'])}")\n    for gpu in report["system_diagnostic"]["gpu_info"]:\n        print(f"  GPU {gpu['device_id']}: {gpu['name']}")\n        print(f"    Compute Capability: {gpu['compute_capability']}")\n        print(f"    Total Memory: {gpu['total_memory'] / (1024**3):.2f} GB")\n    \n    # Memory info\n    mem_info = report["system_diagnostic"]["memory_info"]\n    print(f"\nSystem Memory: {mem_info['system_memory']['total'] / (1024**3):.2f} GB total, "\n          f"{mem_info['system_memory']['percent_used']:.1f}% used")\n    \n    # Memory pool info\n    pool_stats = report["memory_pool_diagnostic"]["memory_pool_stats"]\n    print(f"\nMemory Pool Stats:")\n    print(f"  Total Size: {pool_stats['total_size']:.2f} MB")\n    print(f"  Allocated: {pool_stats['allocated']:.2f} MB")\n    print(f"  Reserved: {pool_stats['reserved']:.2f} MB")\n    print(f"  Fragmentation: {pool_stats['fragmentation']:.2f}")\n    \n    # Memory leak check\n    leak_check = report["memory_leak_check"]\n    print(f"\nMemory Leak Check:")\n    print(f"  Potential Leak Detected: {leak_check['potential_leak']}")\n    print(f"  Allocated Change: {leak_check['allocated_change']} bytes")\n    print(f"  Reserved Change: {leak_check['reserved_change']} bytes")\n    \n    print("="*60 + "\n")\n\n\n# Export the main components\n__all__ = [\n    'CUDADiagnostics',\n    'cuda_diagnostics',\n    'print_diagnostics_report'\n]