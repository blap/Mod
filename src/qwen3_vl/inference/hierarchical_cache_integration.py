"""\nIntegration module for hierarchical cache with existing Qwen3-VL memory pool systems.\n"""\n\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple, Dict, Any, Union\nfrom inference.hierarchical_cache_manager import HierarchicalCacheManager, CacheConfig\nfrom src.qwen3_vl.optimization.tensor_memory_pool import OptimizedMemoryPoolManager, create_memory_pool_manager\nfrom src.qwen3_vl.optimization.memory_pool_integration import MemoryPoolIntegration\n\n\nclass HierarchicalCacheIntegration:\n    """\n    Integration class to connect hierarchical cache with existing memory pool systems.\n    """\n    \n    def __init__(self, \n                 cache_config: CacheConfig = None,\n                 pool_config: Any = None):\n        """\n        Initialize integration between hierarchical cache and memory pools.\n        \n        Args:\n            cache_config: Configuration for hierarchical cache\n            pool_config: Configuration for existing memory pools\n        """\n        self.cache_config = cache_config or CacheConfig()\n        self.pool_config = pool_config\n        \n        # Initialize hierarchical cache manager\n        self.hierarchical_cache = HierarchicalCacheManager(self.cache_config)\n        \n        # Initialize existing memory pool system\n        self.memory_pool_manager = create_memory_pool_manager(pool_config) if pool_config else None\n        self.memory_pool_integration = MemoryPoolIntegration(pool_config) if pool_config else None\n        \n        # Mapping between cache and pool systems\n        self.tensor_to_cache_level: Dict[str, int] = {}\n        self.tensor_to_pool_type: Dict[str, str] = {}\n        \n        # Stats tracking\n        self.integration_stats = {\n            'cache_hits': 0,\n            'pool_hits': 0,\n            'cache_misses': 0,\n            'pool_misses': 0,\n            'transfers_cache_to_pool': 0,\n            'transfers_pool_to_cache': 0\n        }\n    \n    def get_tensor(self, \n                   shape: Tuple[int, ...], \n                   tensor_type: str = "general",\n                   dtype: torch.dtype = torch.float16,\n                   device: torch.device = None) -> torch.Tensor:\n        """\n        Get tensor using hierarchical cache first, falling back to memory pools.\n        \n        Args:\n            shape: Tensor shape\n            tensor_type: Type of tensor (e.g., 'attention', 'kv_cache', 'image_embeddings')\n            dtype: Tensor data type\n            device: Target device\n        \n        Returns:\n            Tensor from cache or pool system\n        """\n        device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        \n        # First, try to get from hierarchical cache\n        cached_tensor, cache_level = self.hierarchical_cache.get_tensor(shape, dtype, tensor_type, device)\n        \n        if cached_tensor is not None:\n            self.integration_stats['cache_hits'] += 1\n            return cached_tensor\n        \n        # If not in cache, try memory pool\n        if self.memory_pool_manager:\n            self.integration_stats['cache_misses'] += 1\n            \n            # Use memory pool manager to get tensor\n            pooled_tensor = self.memory_pool_manager.get_tensor(shape, tensor_type, dtype)\n            \n            if pooled_tensor is not None:\n                self.integration_stats['pool_hits'] += 1\n                \n                # Consider putting this tensor in hierarchical cache for future access\n                self.hierarchical_cache.put_tensor(pooled_tensor, tensor_type)\n                \n                return pooled_tensor.to(device)\n        \n        # If neither cache nor pool has the tensor, create a new one\n        self.integration_stats['pool_misses'] += 1\n        new_tensor = torch.empty(shape, dtype=dtype, device=device)\n        new_tensor.zero_()\n        \n        # Put the new tensor in hierarchical cache\n        self.hierarchical_cache.put_tensor(new_tensor, tensor_type)\n        \n        return new_tensor\n    \n    def return_tensor(self, \n                      tensor: torch.Tensor, \n                      tensor_type: str = "general",\n                      to_cache: bool = True):\n        """\n        Return tensor to appropriate system (cache or pool).\n        \n        Args:\n            tensor: Tensor to return\n            tensor_type: Type of tensor\n            to_cache: Whether to prioritize cache over pool\n        """\n        if to_cache:\n            # Put in hierarchical cache\n            self.hierarchical_cache.put_tensor(tensor, tensor_type)\n        else:\n            # Return to memory pool if available\n            if self.memory_pool_manager:\n                self.memory_pool_manager.return_tensor(tensor, tensor_type)\n    \n    def get_attention_tensor(self, \n                           batch_size: int, \n                           num_heads: int, \n                           seq_len: int, \n                           head_dim: int,\n                           dtype: torch.dtype = torch.float16) -> torch.Tensor:\n        """Get attention tensor optimized for the hierarchical system."""\n        shape = (batch_size, num_heads, seq_len, head_dim)\n        return self.get_tensor(shape, "attention", dtype)\n    \n    def get_kv_cache_tensor(self, \n                          batch_size: int, \n                          num_heads: int, \n                          seq_len: int, \n                          head_dim: int,\n                          dtype: torch.dtype = torch.float16) -> torch.Tensor:\n        """Get KV cache tensor optimized for the hierarchical system."""\n        shape = (batch_size, num_heads, seq_len, head_dim)\n        return self.get_tensor(shape, "kv_cache", dtype)\n    \n    def get_image_embedding_tensor(self, \n                                 batch_size: int, \n                                 num_patches: int, \n                                 feature_dim: int,\n                                 dtype: torch.dtype = torch.float16) -> torch.Tensor:\n        """Get image embedding tensor optimized for the hierarchical system."""\n        shape = (batch_size, num_patches, feature_dim)\n        return self.get_tensor(shape, "image_embeddings", dtype)\n    \n    def get_text_embedding_tensor(self, \n                                batch_size: int, \n                                seq_len: int, \n                                embed_dim: int,\n                                dtype: torch.dtype = torch.float16) -> torch.Tensor:\n        """Get text embedding tensor optimized for the hierarchical system."""\n        shape = (batch_size, seq_len, embed_dim)\n        return self.get_tensor(shape, "text_embeddings", dtype)\n    \n    def migrate_hot_tensors_to_gpu(self):\n        """Migrate frequently accessed tensors to GPU cache (L1)."""\n        # Identify hot tensors from access patterns\n        hot_tensors = self.hierarchical_cache.access_tracker.get_hot_tensors(n=20)\n        \n        for tensor_id in hot_tensors:\n            # Check if tensor is currently in L2 or L3 and migrate to L1 if beneficial\n            if tensor_id in self.hierarchical_cache.tensor_locations:\n                current_level = self.hierarchical_cache.tensor_locations[tensor_id]\n                \n                if current_level in [2, 3]:  # Currently in L2 or L3\n                    # Get tensor from current location\n                    tensor = None\n                    if current_level == 2:\n                        tensor = self.hierarchical_cache.l2_cache.get(tensor_id)\n                    elif current_level == 3:\n                        tensor = self.hierarchical_cache.l3_cache.get(tensor_id)\n                    \n                    if tensor is not None:\n                        # Remove from current location\n                        if current_level == 2:\n                            self.hierarchical_cache.l2_cache.remove(tensor_id)\n                        elif current_level == 3:\n                            self.hierarchical_cache.l3_cache.remove(tensor_id)\n                        \n                        # Put in L1\n                        metadata = self.hierarchical_cache._get_tensor_metadata(tensor_id, current_level)\n                        success = self.hierarchical_cache.l1_cache.put(tensor_id, tensor, metadata)\n                        \n                        if success:\n                            self.hierarchical_cache.tensor_locations[tensor_id] = 1\n    \n    def migrate_cold_tensors_to_ssd(self):\n        """Migrate infrequently accessed tensors to SSD cache (L3)."""\n        # Check tensors in L1 and L2 for migration to L3\n        for tensor_id in list(self.hierarchical_cache.l1_cache.cache.keys()):\n            metadata = self.hierarchical_cache.l1_cache.metadata.get(tensor_id)\n            if metadata:\n                time_since_access = time.time() - metadata.last_access_time\n                if (time_since_access > self.cache_config.migration_time_threshold and \n                    metadata.access_count < self.cache_config.migration_threshold_medium_freq):\n                    # Migrate from L1 to L3\n                    tensor = self.hierarchical_cache.l1_cache.get(tensor_id)\n                    if tensor is not None:\n                        self.hierarchical_cache.l1_cache.remove(tensor_id)\n                        \n                        # Put in L3\n                        success = self.hierarchical_cache.l3_cache.put(tensor_id, tensor, metadata)\n                        if success:\n                            self.hierarchical_cache.tensor_locations[tensor_id] = 3\n        \n        for tensor_id in list(self.hierarchical_cache.l2_cache.cache.keys()):\n            metadata = self.hierarchical_cache.l2_cache.metadata.get(tensor_id)\n            if metadata:\n                time_since_access = time.time() - metadata.last_access_time\n                if (time_since_access > self.cache_config.migration_time_threshold and \n                    metadata.access_count < self.cache_config.migration_threshold_medium_freq):\n                    # Migrate from L2 to L3\n                    tensor = self.hierarchical_cache.l2_cache.get(tensor_id)\n                    if tensor is not None:\n                        self.hierarchical_cache.l2_cache.remove(tensor_id)\n                        \n                        # Put in L3\n                        success = self.hierarchical_cache.l3_cache.put(tensor_id, tensor, metadata)\n                        if success:\n                            self.hierarchical_cache.tensor_locations[tensor_id] = 3\n    \n    def perform_integrated_migrations(self):\n        """Perform migrations based on both access patterns and pool system needs."""\n        # Perform cache-level migrations\n        self.hierarchical_cache._perform_migrations()\n        \n        # Perform hot/cold tensor migrations\n        self.migrate_hot_tensors_to_gpu()\n        self.migrate_cold_tensors_to_ssd()\n    \n    def get_integration_stats(self) -> Dict[str, Any]:\n        """Get statistics about the integration between cache and pool systems."""\n        cache_stats = self.hierarchical_cache.get_stats()\n        \n        return {\n            'integration_stats': self.integration_stats,\n            'hierarchical_cache_stats': cache_stats,\n            'pool_system_stats': self.memory_pool_manager.get_pool_stats() if self.memory_pool_manager else None,\n            'cache_to_pool_hit_ratio': (\n                self.integration_stats['cache_hits'] / \n                (self.integration_stats['cache_hits'] + self.integration_stats['cache_misses'])\n                if (self.integration_stats['cache_hits'] + self.integration_stats['cache_misses']) > 0 else 0\n            ),\n            'pool_hit_ratio': (\n                self.integration_stats['pool_hits'] / \n                (self.integration_stats['pool_hits'] + self.integration_stats['pool_misses'])\n                if (self.integration_stats['pool_hits'] + self.integration_stats['pool_misses']) > 0 else 0\n            )\n        }\n    \n    def clear_all_caches(self):\n        """Clear both hierarchical cache and memory pools."""\n        self.hierarchical_cache.clear_cache()\n        if self.memory_pool_manager:\n            self.memory_pool_manager.clear_all_pools()\n\n\nclass Qwen3VLHierarchicalCacheAdapter:\n    """\n    Adapter class for Qwen3-VL to use the integrated hierarchical caching system.\n    """\n    \n    def __init__(self, config=None):\n        self.config = config\n        self.cache_config = CacheConfig()\n        \n        # Set cache sizes based on available system resources\n        available_memory = psutil.virtual_memory().available\n        gpu_memory = 0\n        if torch.cuda.is_available():\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory\n        \n        # Set cache sizes proportionally\n        if gpu_memory > 0:\n            self.cache_config.l1_cache_size = min(int(gpu_memory * 0.3), 2 * 1024 * 1024 * 1024)  # 30% of GPU memory or 2GB\n        self.cache_config.l2_cache_size = min(int(available_memory * 0.2), 4 * 1024 * 1024 * 1024)  # 20% of RAM or 4GB\n        self.cache_config.l3_cache_size = min(int(available_memory * 0.5), 10 * 1024 * 1024 * 1024)  # 50% of RAM or 10GB\n        \n        # Create integration\n        self.integration = HierarchicalCacheIntegration(self.cache_config, config)\n    \n    def allocate_attention_weights(self, batch_size: int, num_heads: int,\n                                  seq_len: int, head_dim: int) -> torch.Tensor:\n        """Allocate attention weights tensor using hierarchical cache."""\n        return self.integration.get_attention_tensor(batch_size, num_heads, seq_len, head_dim)\n    \n    def allocate_kv_cache(self, batch_size: int, num_heads: int,\n                         seq_len: int, head_dim: int, is_key: bool = True) -> torch.Tensor:\n        """Allocate KV cache tensor using hierarchical cache."""\n        return self.integration.get_kv_cache_tensor(batch_size, num_heads, seq_len, head_dim)\n    \n    def allocate_image_features(self, batch_size: int, num_patches: int,\n                               feature_dim: int) -> torch.Tensor:\n        """Allocate image feature tensor using hierarchical cache."""\n        return self.integration.get_image_embedding_tensor(batch_size, num_patches, feature_dim)\n    \n    def allocate_text_embeddings(self, batch_size: int, seq_len: int,\n                                embed_dim: int) -> torch.Tensor:\n        """Allocate text embedding tensor using hierarchical cache."""\n        return self.integration.get_text_embedding_tensor(batch_size, seq_len, embed_dim)\n    \n    def perform_cache_maintenance(self):\n        """Perform cache maintenance including migrations and optimizations."""\n        self.integration.perform_integrated_migrations()\n    \n    def get_cache_statistics(self) -> Dict[str, Any]:\n        """Get comprehensive cache statistics."""\n        return self.integration.get_integration_stats()\n\n\ndef create_hierarchical_cache_adapter(config=None) -> Qwen3VLHierarchicalCacheAdapter:\n    """\n    Factory function to create a hierarchical cache adapter.\n    \n    Args:\n        config: Configuration object with memory optimization parameters\n    \n    Returns:\n        Qwen3VLHierarchicalCacheAdapter instance\n    """\n    return Qwen3VLHierarchicalCacheAdapter(config)\n\n\n# Example usage and integration test\nif __name__ == "__main__":\n    import time\n    \n    print("Testing Hierarchical Cache Integration...")\n    \n    # Create mock config\n    class MockConfig:\n        def __init__(self):\n            self.memory_pool_base_capacity = 1024 * 1024 * 512  # 512MB\n            self.memory_pool_dtype = torch.float16\n            self.memory_pool_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n            self.memory_pool_cache_line_size = 64\n            self.memory_pool_l3_cache_size = 6 * 1024 * 1024  # 6MB\n    \n    config = MockConfig()\n    \n    # Create hierarchical cache adapter\n    adapter = create_hierarchical_cache_adapter(config)\n    \n    print(f"\nCreated adapter with:")\n    print(f"  L1 cache: {adapter.cache_config.l1_cache_size / (1024**2):.0f}MB")\n    print(f"  L2 cache: {adapter.cache_config.l2_cache_size / (1024**2):.0f}MB")\n    print(f"  L3 cache: {adapter.cache_config.l3_cache_size / (1024**3):.1f}GB")\n    \n    # Test tensor allocation\n    print("\n1. Testing attention tensor allocation...")\n    attn_tensor = adapter.allocate_attention_weights(2, 8, 512, 64)\n    print(f"Allocated attention tensor: {attn_tensor.shape}, {attn_tensor.dtype}")\n    \n    print("\n2. Testing KV cache tensor allocation...")\n    kv_tensor = adapter.allocate_kv_cache(1, 16, 1024, 128)\n    print(f"Allocated KV cache tensor: {kv_tensor.shape}, {kv_tensor.dtype}")\n    \n    print("\n3. Testing image features tensor allocation...")\n    img_tensor = adapter.allocate_image_features(1, 576, 1152)\n    print(f"Allocated image features tensor: {img_tensor.shape}, {img_tensor.dtype}")\n    \n    # Access tensors multiple times to trigger migration\n    print("\n4. Testing access pattern tracking and migration...")\n    for i in range(5):\n        # Access attention tensor multiple times to make it "hot"\n        hot_tensor = adapter.allocate_attention_weights(2, 8, 512, 64)\n        print(f"  Access {i+1}: Attention tensor shape {hot_tensor.shape}")\n        time.sleep(0.1)  # Small delay to create time intervals\n    \n    # Perform cache maintenance (migrations)\n    print("\n5. Performing cache maintenance...")\n    adapter.perform_cache_maintenance()\n    \n    # Get and display statistics\n    print("\n6. Integration statistics:")\n    stats = adapter.get_cache_statistics()\n    \n    print(f"  Cache hit ratio: {stats['cache_to_pool_hit_ratio']:.2%}")\n    print(f"  Pool hit ratio: {stats['pool_hit_ratio']:.2%}")\n    print(f"  Global cache hit rate: {stats['hierarchical_cache_stats']['global_stats']['global_hit_rate']:.2%}")\n    print(f"  Total migrations: {stats['hierarchical_cache_stats']['global_stats']['migrations']}")\n    \n    print(f"\n  L1 Cache - Hit rate: {stats['hierarchical_cache_stats']['l1_stats']['hit_rate']:.2%}, "\n          f"Utilization: {stats['hierarchical_cache_stats']['l1_stats']['utilization']:.2%}")\n    print(f"  L2 Cache - Hit rate: {stats['hierarchical_cache_stats']['l2_stats']['hit_rate']:.2%}, "\n          f"Utilization: {stats['hierarchical_cache_stats']['l2_stats']['utilization']:.2%}")\n    print(f"  L3 Cache - Hit rate: {stats['hierarchical_cache_stats']['l3_stats']['hit_rate']:.2%}, "\n          f"Utilization: {stats['hierarchical_cache_stats']['l3_stats']['utilization']:.2%}")\n    \n    print("\nHierarchical cache integration test completed successfully!")