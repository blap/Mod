"""\nGeneration utilities for Qwen3-VL.\n\nThis module provides text generation capabilities for the Qwen3-VL model.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\nfrom config.config import Qwen3VLConfig\nfrom models.base_model import Qwen3VLModel\n\n\ndef generate_text(\n    model: Qwen3VLModel,\n    input_ids: torch.LongTensor,\n    max_length: int = 512,\n    temperature: float = 1.0,\n    do_sample: bool = True,\n    pad_token_id: Optional[int] = None,\n    eos_token_id: Optional[int] = None,\n    **kwargs\n):\n    """\n    Generate text using the Qwen3-VL model.\n\n    Args:\n        model: Qwen3VLModel instance\n        input_ids: Input token IDs\n        max_length: Maximum length of generated sequence\n        temperature: Sampling temperature\n        do_sample: Whether to sample or use greedy decoding\n        pad_token_id: Padding token ID\n        eos_token_id: End-of-sequence token ID\n        **kwargs: Additional generation arguments\n\n    Returns:\n        Generated sequences\n    """\n    if pad_token_id is None:\n        pad_token_id = model.config.pad_token_id\n    if eos_token_id is None:\n        eos_token_id = getattr(model.config, 'eos_token_id', model.config.pad_token_id)\n\n    batch_size = input_ids.size(0)\n    cur_len = input_ids.size(1)\n    eos_token_id = eos_token_id if isinstance(eos_token_id, list) else [eos_token_id]\n\n    # Generate tokens one by one\n    sequences = input_ids.clone()\n    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n\n    while cur_len < max_length:\n        # Get model outputs\n        outputs = model(input_ids=sequences)\n        next_token_logits = outputs[:, -1, :] / temperature\n\n        # Apply softmax to get probabilities\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n\n        if do_sample:\n            # Sample from the distribution\n            next_tokens = torch.multinomial(next_token_probs, num_samples=1).squeeze(1)\n        else:\n            # Take the most likely token\n            next_tokens = torch.argmax(next_token_probs, dim=-1)\n\n        # Update sequences\n        next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n        sequences = torch.cat([sequences, next_tokens.unsqueeze(-1)], dim=-1)\n\n        # Update unfinished sequences\n        for eos_id in eos_token_id:\n            unfinished_sequences = unfinished_sequences * (next_tokens != eos_id)\n\n        # Stop if all sequences are finished\n        if unfinished_sequences.max() == 0:\n            break\n\n        cur_len += 1\n\n    return sequences