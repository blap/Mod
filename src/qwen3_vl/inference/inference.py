"""\nBasic inference functionality for Qwen3-VL model\n"""\nimport torch\nfrom typing import Optional, Union, List\nimport time\nfrom PIL import Image\nimport numpy as np\n\nfrom architectures.qwen3_vl import Qwen3VLModel, load_qwen3_vl_model\nfrom config.config import Qwen3VLConfig\nfrom utils.model_utils import verify_model_capacity, check_device_compatibility\n\n\nclass Qwen3VLInference:\n    """\n    Inference class for Qwen3-VL model with basic functionality.\n    """\n    def __init__(self, model: Qwen3VLModel):\n        self.model = model\n        self.device = next(model.model.parameters()).device\n        self.model.eval()  # Set to evaluation mode\n\n    def process_image(self, image_path: str) -> torch.Tensor:\n        """\n        Process an image for model input.\n\n        Args:\n            image_path: Path to the image file\n\n        Returns:\n            Processed image tensor\n        """\n        # Load image using PIL\n        image = Image.open(image_path).convert('RGB')\n\n        # Resize and normalize the image\n        # Using standard preprocessing similar to CLIP/ViT models\n        from torchvision import transforms\n\n        preprocess = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),  # Converts to [0,1] range\n            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n                               std=[0.26862954, 0.26130258, 0.27577711])\n        ])\n\n        image_tensor = preprocess(image)\n        image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n        return image_tensor.to(self.device)\n\n    def process_text(self, text: str) -> torch.Tensor:\n        """\n        Process text for model input.\n\n        Args:\n            text: Input text string\n\n        Returns:\n            Processed token tensor\n        """\n        # Initialize tokenizer if not already done\n        if not hasattr(self, '_tokenizer'):\n            from transformers import AutoTokenizer\n            # Using a standard tokenizer that should work with most models\n            try:\n                # Try to use the model's own tokenizer if available\n                self._tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")\n            except:\n                # Fallback to a generic tokenizer\n                self._tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")\n\n            # Ensure tokenizer has pad token\n            if self._tokenizer.pad_token is None:\n                self._tokenizer.pad_token = self._tokenizer.eos_token\n\n        # Tokenize the text\n        inputs = self._tokenizer(\n            text,\n            return_tensors="pt",\n            padding=True,\n            truncation=True,\n            max_length=512\n        )\n\n        return inputs['input_ids'].to(self.device)\n\n    def generate_response(\n        self,\n        text: str,\n        image_path: Optional[str] = None,\n        max_new_tokens: int = 100,\n        temperature: float = 0.7,\n        do_sample: bool = True,\n        **kwargs\n    ) -> str:\n        """\n        Generate a response from the model based on text and optional image input.\n\n        Args:\n            text: Input text\n            image_path: Optional path to input image\n            max_new_tokens: Maximum number of new tokens to generate\n            temperature: Sampling temperature\n            do_sample: Whether to use sampling or greedy decoding\n            **kwargs: Additional generation arguments\n\n        Returns:\n            Generated response text\n        """\n        # Process text input\n        input_ids = self.process_text(text)\n\n        # Process image input if provided\n        pixel_values = None\n        if image_path:\n            pixel_values = self.process_image(image_path)\n\n        # Create attention mask\n        attention_mask = torch.ones_like(input_ids)\n\n        # Generate response\n        with torch.no_grad():\n            outputs = self.model.generate(\n                input_ids=input_ids,\n                pixel_values=pixel_values,\n                attention_mask=attention_mask,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                do_sample=do_sample,\n                pad_token_id=0,  # Default pad token ID\n                **kwargs\n            )\n\n        # Decode the generated tokens back to text\n        # Extract only the newly generated tokens (excluding input)\n        input_length = input_ids.shape[1]\n        generated_ids = outputs[0][input_length:]\n\n        # Initialize tokenizer if not already done (for decoding)\n        if not hasattr(self, '_tokenizer'):\n            from transformers import AutoTokenizer\n            # Using a standard tokenizer that should work with most models\n            try:\n                # Try to use the model's own tokenizer if available\n                self._tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")\n            except:\n                # Fallback to a generic tokenizer\n                self._tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")\n\n            # Ensure tokenizer has pad token\n            if self._tokenizer.pad_token is None:\n                self._tokenizer.pad_token = self._tokenizer.eos_token\n\n        # Decode the generated tokens to text\n        response = self._tokenizer.decode(generated_ids, skip_special_tokens=True)\n        return response\n\n    def run_multimodal_task(\n        self,\n        text: str,\n        image_path: str,\n        task_type: str = "general"\n    ) -> str:\n        """\n        Run a multimodal task (e.g., VQA, image captioning, etc.).\n        \n        Args:\n            text: Input text/query\n            image_path: Path to input image\n            task_type: Type of multimodal task\n        \n        Returns:\n            Task result\n        """\n        if task_type.lower() == "vqa":\n            # Visual Question Answering\n            prompt = f"Question: {text} Answer concisely based on the image."\n        elif task_type.lower() == "caption":\n            # Image Captioning\n            prompt = f"Describe this image in one sentence: {text}"\n        else:\n            # General multimodal task\n            prompt = text\n        \n        return self.generate_response(prompt, image_path)\n\n    def benchmark_performance(\n        self,\n        text: str,\n        image_path: Optional[str] = None,\n        num_runs: int = 5\n    ) -> dict:\n        """\n        Benchmark model performance.\n        \n        Args:\n            text: Input text for benchmarking\n            image_path: Optional image path for multimodal benchmarking\n            num_runs: Number of runs for averaging\n        \n        Returns:\n            Dictionary with performance metrics\n        """\n        # Process inputs\n        input_ids = self.process_text(text)\n        pixel_values = self.process_image(image_path) if image_path else None\n        attention_mask = torch.ones_like(input_ids)\n        \n        # Warm up\n        for _ in range(2):\n            with torch.no_grad():\n                _ = self.model(\n                    input_ids=input_ids,\n                    pixel_values=pixel_values,\n                    attention_mask=attention_mask\n                )\n        \n        # Benchmark\n        times = []\n        for _ in range(num_runs):\n            start_time = time.time()\n            with torch.no_grad():\n                _ = self.model(\n                    input_ids=input_ids,\n                    pixel_values=pixel_values,\n                    attention_mask=attention_mask\n                )\n            end_time = time.time()\n            times.append(end_time - start_time)\n        \n        avg_time = sum(times) / len(times)\n        std_time = (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5\n        \n        return {\n            'avg_inference_time': avg_time,\n            'std_inference_time': std_time,\n            'num_runs': num_runs,\n            'device': str(self.device),\n            'memory_allocated': torch.cuda.memory_allocated(self.device) if torch.cuda.is_available() else 0\n        }\n\n\ndef create_dummy_model() -> Qwen3VLModel:\n    """\n    Create a dummy Qwen3-VL model for testing purposes.\n\n    Returns:\n        Dummy model instance\n    """\n    # Create a basic configuration\n    config = Qwen3VLConfig()\n\n    # Create the model architecture\nfrom core.modeling_qwen3_vl import Qwen3VLForConditionalGeneration\n    model = Qwen3VLForConditionalGeneration(config)\n\n    # Import the Qwen3VLModel wrapper class\nfrom architectures.qwen3_vl import Qwen3VLModel\n    qwen3_vl_model = Qwen3VLModel(config, model)\n\n    return qwen3_vl_model\n\n\ndef main():\n    """\n    Main function to demonstrate basic inference functionality.\n    """\n    print("Initializing Qwen3-VL model...")\n\n    # Try to load a real model, fallback to dummy if not available\n    try:\nfrom architectures.qwen3_vl import load_qwen3_vl_model\n        model, processor = load_qwen3_vl_model("microsoft/DialoGPT-medium")  # Use a standard model as placeholder\n    except:\n        # Fallback to dummy model if real model loading fails\n        model = create_dummy_model()\n\n    # Verify model capacity\n    capacity_info = verify_model_capacity(model.model)\n    print(f"Model capacity verification: {capacity_info}")\n\n    # Check device compatibility\n    device_info = check_device_compatibility(model.model)\n    print(f"Device compatibility: {device_info}")\n\n    # Create inference instance\n    inference = Qwen3VLInference(model)\n\n    # Run a simple test\n    test_text = "What is in this image?"\n    response = inference.generate_response(test_text, max_new_tokens=20)\n    print(f"Response: {response}")\n\n    # Benchmark performance\n    benchmark_results = inference.benchmark_performance(test_text)\n    print(f"Benchmark results: {benchmark_results}")\n\n    print("Basic inference functionality demonstrated successfully!")\n\n\nif __name__ == "__main__":\n    main()