"""\nStorage Management System for Qwen3-VL Model\nSupports both NVMe SSD and Toshiba Canvio Ready external HDD (2TB, USB 3.0)\n\nThis module implements a comprehensive storage management system that intelligently\nutilizes different storage types based on their characteristics and access patterns.\n"""\n\nimport os\nimport json\nimport time\nimport shutil\nimport threading\nimport psutil\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple, Any, Union\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nimport logging\nimport pickle\nimport tempfile\nimport hashlib\nfrom collections import OrderedDict, defaultdict, deque\nimport statistics\nimport math\nfrom datetime import datetime\n\nfrom qwen3_vl.utils.debug_utils import conditional_debug
from qwen3_vl.utils.general_utils import is_debug_mode\n\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass StorageType(Enum):\n    """Types of storage devices supported"""\n    NVME_SSD = "nvme_ssd"        # High performance NVMe SSD\n    EXTERNAL_HDD = "external_hdd"  # Toshiba Canvio Ready external HDD (2TB, USB 3.0)\n    DEFAULT = "default"          # System default storage\n\n\n@dataclass\nclass StorageDevice:\n    """Represents a storage device with its characteristics"""\n    device_path: str\n    storage_type: StorageType\n    total_space: int  # in bytes\n    available_space: int  # in bytes\n    read_speed: float  # MB/s\n    write_speed: float  # MB/s\n    latency: float  # ms\n    is_available: bool = True\n    mount_point: str = ""\n    device_id: str = ""\n\n\n@dataclass\nclass StorageConfig:\n    """Configuration for storage management"""\n    ssd_path: str = ""  # Will be set to a proper temp directory\n    hdd_path: str = ""  # Will be set to a proper temp directory\n    enable_tiering: bool = True\n    tiering_threshold: int = 100 * 1024 * 1024  # 100MB threshold for tiering\n    cache_size_limit: int = 2 * 1024 * 1024 * 1024  # 2GB cache limit\n    enable_compression: bool = True\n    compression_level: int = 6  # Compression level (1-9)\n    external_storage_timeout: int = 30  # Timeout for external storage operations (seconds)\n    enable_fallback: bool = True\n    fallback_storage_path: str = ""\n\n    def __post_init__(self):\n        """Initialize paths to proper temp directories if not set"""\n        if not self.ssd_path or self.ssd_path == "/tmp/ssd_cache":\n            self.ssd_path = os.path.join(tempfile.gettempdir(), "qwen3vl_ssd_cache")\n\n        if not self.hdd_path or self.hdd_path == "/tmp/hdd_storage":\n            self.hdd_path = os.path.join(tempfile.gettempdir(), "qwen3vl_hdd_storage")\n\n        if not self.fallback_storage_path or self.fallback_storage_path == "/tmp/fallback_storage":\n            self.fallback_storage_path = os.path.join(tempfile.gettempdir(), "qwen3vl_fallback_storage")\n\n        # Create directories if they don't exist\n        for path in [self.ssd_path, self.hdd_path, self.fallback_storage_path]:\n            try:\n                Path(path).mkdir(parents=True, exist_ok=True)\n            except Exception as e:\n                logger.warning(f"Could not create directory {path}: {e}")\n\n\nclass StoragePerformanceMonitor:\n    """Monitors performance of different storage types"""\n    \n    def __init__(self):\n        self.performance_data = defaultdict(list)  # device_path -> [perf_metrics]\n        self.current_io_stats = {}\n        self._lock = threading.Lock()\n        \n    def record_io_operation(self, device_path: str, operation: str, size: int, duration: float):\n        """Record I/O operation performance"""\n        with self._lock:\n            self.performance_data[device_path].append({\n                'operation': operation,\n                'size': size,\n                'duration': duration,\n                'timestamp': time.time(),\n                'throughput': size / duration if duration > 0 else 0\n            })\n            \n            # Keep only last 1000 records per device to avoid memory issues\n            if len(self.performance_data[device_path]) > 1000:\n                self.performance_data[device_path] = self.performance_data[device_path][-1000:]\n    \n    def get_average_throughput(self, device_path: str) -> float:\n        """Get average throughput for a device in MB/s"""\n        with self._lock:\n            if device_path not in self.performance_data or not self.performance_data[device_path]:\n                return 0.0\n            \n            throughputs = [record['throughput'] for record in self.performance_data[device_path]]\n            avg_throughput = sum(throughputs) / len(throughputs) if throughputs else 0.0\n            return avg_throughput / (1024 * 1024)  # Convert to MB/s\n    \n    def get_recent_performance(self, device_path: str, last_n: int = 10) -> Dict[str, float]:\n        """Get recent performance metrics"""\n        with self._lock:\n            if device_path not in self.performance_data:\n                return {}\n            \n            recent_records = self.performance_data[device_path][-last_n:]\n            if not recent_records:\n                return {}\n            \n            sizes = [r['size'] for r in recent_records]\n            durations = [r['duration'] for r in recent_records]\n            throughputs = [r['throughput'] for r in recent_records]\n            \n            return {\n                'avg_size': sum(sizes) / len(sizes),\n                'avg_duration': sum(durations) / len(durations),\n                'avg_throughput_mb_s': (sum(throughputs) / len(throughputs)) / (1024 * 1024),\n                'min_throughput_mb_s': min(throughputs) / (1024 * 1024),\n                'max_throughput_mb_s': max(throughputs) / (1024 * 1024)\n            }\n\n\nclass StorageAccessPatternTracker:\n    """Tracks access patterns to predict future access for storage optimization"""\n    \n    def __init__(self, window_size: int = 1000):\n        self.window_size = window_size\n        self.access_history = deque(maxlen=window_size)\n        self.file_access_intervals = defaultdict(deque)  # Track intervals between accesses\n        self.file_access_counts = defaultdict(int)\n        self.file_last_access = {}\n        self.file_avg_intervals = {}\n        self.file_residency_times = defaultdict(float)  # How long files stay in memory\n        self.file_frequency_scores = defaultdict(float)  # Normalized frequency scores\n        self._lock = threading.Lock()\n    \n    def record_access(self, file_path: str):\n        """Record access to a file"""\n        with self._lock:\n            current_time = time.time()\n            self.access_history.append((file_path, current_time))\n    \n            # Update access count\n            self.file_access_counts[file_path] += 1\n    \n            # Calculate interval if file was accessed before\n            if file_path in self.file_last_access:\n                interval = current_time - self.file_last_access[file_path]\n                self.file_access_intervals[file_path].append(interval)\n    \n                # Update average interval\n                intervals = list(self.file_access_intervals[file_path])\n                if intervals:\n                    self.file_avg_intervals[file_path] = statistics.mean(intervals)\n    \n            self.file_last_access[file_path] = current_time\n    \n            # Update frequency score (normalized by time since creation)\n            time_since_creation = current_time - min(self.file_last_access.values())\n            if time_since_creation > 0:\n                self.file_frequency_scores[file_path] = self.file_access_counts[file_path] / time_since_creation\n    \n    def predict_access(self, file_path: str) -> Tuple[float, Optional[float]]:\n        """\n        Predict probability of file being accessed soon and when.\n        \n        Returns:\n            Tuple of (probability, predicted_access_time)\n        """\n        with self._lock:\n            if file_path not in self.file_avg_intervals:\n                # If no history, use low probability and far prediction\n                return 0.1, time.time() + 300  # Low probability, predict 5 minutes from now\n    \n            avg_interval = self.file_avg_intervals[file_path]\n            last_access = self.file_last_access.get(file_path, 0)\n            time_since_last = time.time() - last_access\n    \n            # If time since last access is close to average interval, high probability\n            if avg_interval > 0:\n                ratio = time_since_last / avg_interval\n                # Sigmoid-like function to convert to probability\n                probability = 1.0 / (1.0 + math.exp(-2 * (ratio - 0.5)))\n                probability = min(probability, 1.0)\n    \n                # Predict next access time\n                predicted_time = last_access + avg_interval\n                return probability, predicted_time\n            else:\n                return 0.5, time.time() + 60  # Default to 1 minute from now\n    \n    def get_hot_files(self, n: int = 10) -> List[str]:\n        """Get top N most frequently accessed files"""\n        with self._lock:\n            sorted_files = sorted(\n                self.file_access_counts.items(),\n                key=lambda x: x[1],\n                reverse=True\n            )\n            return [file_path for file_path, count in sorted_files[:n]]\n    \n    def get_temporal_locality_score(self, file_path: str) -> float:\n        """Get temporal locality score for a file"""\n        with self._lock:\n            if file_path in self.file_avg_intervals:\n                # Higher score for shorter average intervals (more frequent access)\n                avg_interval = self.file_avg_intervals[file_path]\n                return 1.0 / (1.0 + avg_interval)  # Normalize to 0-1 range\n            else:\n                return 0.1  # Low score if no history\n\n\nclass StorageManager:\n    """Main storage management system for Qwen3-VL"""\n    \n    def __init__(self, config: StorageConfig = None):\n        self.config = config or StorageConfig()\n        self.devices: Dict[str, StorageDevice] = {}\n        self.performance_monitor = StoragePerformanceMonitor()\n        self.access_tracker = StorageAccessPatternTracker()\n        self._lock = threading.Lock()\n        self._init_lock = threading.Lock()\n        \n        # Initialize storage paths\n        self._initialize_storage_paths()\n        \n        # Initialize storage devices\n        self._detect_storage_devices()\n        \n        logger.info(f"Storage Manager initialized with devices: {[d.device_path for d in self.devices.values()]}")\n    \n    def _initialize_storage_paths(self):\n        """Initialize storage paths and ensure they exist"""\n        paths_to_create = [\n            self.config.ssd_path,\n            self.config.hdd_path,\n            self.config.fallback_storage_path\n        ]\n        \n        for path in paths_to_create:\n            try:\n                Path(path).mkdir(parents=True, exist_ok=True)\n            except Exception as e:\n                logger.warning(f"Could not create directory {path}: {e}")\n    \n    def _detect_storage_devices(self):\n        """Detect available storage devices and their characteristics"""\n        with self._init_lock:\n            # Get all disk partitions\n            partitions = psutil.disk_partitions()\n\n            for partition in partitions:\n                try:\n                    # Get disk usage\n                    usage = psutil.disk_usage(partition.mountpoint)\n\n                    # Determine storage type based on characteristics\n                    storage_type = self._determine_storage_type(partition)\n\n                    # Estimate performance characteristics\n                    read_speed, write_speed, latency = self._estimate_performance(partition.mountpoint)\n\n                    device = StorageDevice(\n                        device_path=partition.device,\n                        storage_type=storage_type,\n                        total_space=usage.total,\n                        available_space=usage.free,\n                        read_speed=read_speed,\n                        write_speed=write_speed,\n                        latency=latency,\n                        is_available=True,\n                        mount_point=partition.mountpoint,\n                        device_id=partition.device\n                    )\n\n                    self.devices[partition.mountpoint] = device\n                    logger.info(f"Detected storage device: {device}")\n\n                except Exception as e:\n                    logger.warning(f"Could not detect device {partition.device}: {e}")\n\n            # If no devices were detected, create fallback devices based on config paths\n            if not self.devices:\n                logger.warning("No storage devices detected via disk partitions, creating fallback devices based on config paths")\n\n                # Create devices for each of our config paths\n                config_paths = [self.config.ssd_path, self.config.hdd_path, self.config.fallback_storage_path]\n\n                for path in config_paths:\n                    if path and os.path.exists(path):\n                        try:\n                            usage = psutil.disk_usage(path)\n                            # Determine storage type based on path characteristics\n                            if 'ssd' in path.lower() or 'nvme' in path.lower():\n                                storage_type = StorageType.NVME_SSD\n                                read_speed, write_speed, latency = 2000.0, 1500.0, 0.02  # NVMe speeds\n                            elif 'hdd' in path.lower() or 'ext' in path.lower() or 'canvio' in path.lower():\n                                storage_type = StorageType.EXTERNAL_HDD\n                                read_speed, write_speed, latency = 150.0, 120.0, 10.0  # External HDD speeds\n                            else:\n                                storage_type = StorageType.DEFAULT\n                                read_speed, write_speed, latency = 100.0, 80.0, 20.0  # Default speeds\n\n                            device = StorageDevice(\n                                device_path=path,\n                                storage_type=storage_type,\n                                total_space=usage.total,\n                                available_space=usage.free,\n                                read_speed=read_speed,\n                                write_speed=write_speed,\n                                latency=latency,\n                                is_available=True,\n                                mount_point=path,\n                                device_id=f"config_path_{hashlib.md5(path.encode()).hexdigest()[:8]}"\n                            )\n\n                            self.devices[path] = device\n                            logger.info(f"Created fallback storage device: {device}")\n                        except Exception as e:\n                            logger.warning(f"Could not create fallback device for path {path}: {e}")\n\n                # If still no devices, create a minimal fallback\n                if not self.devices:\n                    logger.warning("Creating minimal fallback device using temp directory")\n                    temp_dir = tempfile.gettempdir()\n                    try:\n                        usage = psutil.disk_usage(temp_dir)\n                        device = StorageDevice(\n                            device_path=temp_dir,\n                            storage_type=StorageType.DEFAULT,\n                            total_space=usage.total,\n                            available_space=usage.free,\n                            read_speed=100.0,\n                            write_speed=80.0,\n                            latency=20.0,\n                            is_available=True,\n                            mount_point=temp_dir,\n                            device_id="temp_fallback"\n                        )\n                        self.devices[temp_dir] = device\n                        logger.info(f"Created fallback device using temp directory: {device}")\n                    except Exception as e:\n                        logger.error(f"Could not create fallback device using temp directory: {e}")\n    \n    def _determine_storage_type(self, partition) -> StorageType:\n        """Determine storage type based on partition characteristics"""\n        # On Windows, partition.opts contains options like 'rw', 'fixed', 'removable'\n        if 'removable' in partition.opts or 'cdrom' in partition.opts:\n            # Likely an external drive or removable media\n            return StorageType.EXTERNAL_HDD\n        else:\n            # Check if it's likely an NVMe SSD by looking at device name or mount point\n            # This is a simplified check - in practice, you might want to check more specific indicators\n            device_name = partition.device.lower()\n            mount_point = partition.mountpoint.lower()\n\n            if 'nvme' in device_name or 'nvme' in mount_point or 'ssd' in device_name or 'ssd' in mount_point:\n                return StorageType.NVME_SSD\n            elif 'canvio' in device_name or 'canvio' in mount_point.lower():\n                # Toshiba Canvio Ready specific detection\n                return StorageType.EXTERNAL_HDD\n            else:\n                # For other fixed drives, assume default unless specifically identified\n                return StorageType.DEFAULT\n    \n    def _estimate_performance(self, mount_point: str) -> Tuple[float, float, float]:\n        """Estimate performance characteristics of a storage device"""\n        # Default performance values\n        read_speed = 500.0  # MB/s\n        write_speed = 450.0  # MB/s\n        latency = 0.1  # ms\n        \n        # Try to estimate based on device type\n        if 'nvme' in mount_point.lower():\n            read_speed = 3500.0  # NVMe can reach 3500+ MB/s\n            write_speed = 3000.0\n            latency = 0.01\n        elif any(removable_word in mount_point.lower() for removable_word in ['usb', 'removable']):\n            # Likely external HDD\n            read_speed = 150.0  # Typical USB 3.0 HDD speed\n            write_speed = 120.0\n            latency = 10.0  # Higher latency for HDD\n        \n        return read_speed, write_speed, latency\n    \n    def get_optimal_storage_for_file(self, file_size: int, access_pattern: str = "unknown") -> Optional[StorageDevice]:\n        """Determine optimal storage device for a file based on size and access pattern"""\n        with self._lock:\n            if not self.devices:\n                return None\n            \n            # For small, frequently accessed files, prefer SSD\n            if file_size < self.config.tiering_threshold and access_pattern in ["frequent", "hot", "unknown"]:\n                for device in self.devices.values():\n                    if device.storage_type in [StorageType.NVME_SSD, StorageType.DEFAULT] and device.is_available:\n                        return device\n            \n            # For large files or archival data, prefer HDD\n            if file_size >= self.config.tiering_threshold or access_pattern in ["rare", "cold"]:\n                for device in self.devices.values():\n                    if device.storage_type == StorageType.EXTERNAL_HDD and device.is_available:\n                        return device\n            \n            # Fallback: return first available device\n            for device in self.devices.values():\n                if device.is_available:\n                    return device\n            \n            return None\n    \n    def write_file(self, file_path: str, data: Any, storage_preference: StorageType = None) -> bool:\n        """Write a file to the most appropriate storage location"""\n        # Calculate file size\n        if isinstance(data, (bytes, bytearray)):\n            file_size = len(data)\n        elif isinstance(data, str):\n            file_size = len(data.encode('utf-8'))\n        else:\n            # For other types, serialize to estimate size\n            try:\n                serialized_data = pickle.dumps(data)\n                file_size = len(serialized_data)\n            except:\n                # If serialization fails, use a default size\n                file_size = 1024 * 1024  # 1MB default\n\n        # Determine optimal storage device\n        if storage_preference:\n            # Find device with specific preference\n            target_device = None\n            for device in self.devices.values():\n                if device.storage_type == storage_preference and device.is_available:\n                    target_device = device\n                    break\n        else:\n            target_device = self.get_optimal_storage_for_file(file_size)\n\n        if not target_device:\n            # Fallback: use the config paths if no devices are available\n            if storage_preference == StorageType.NVME_SSD or storage_preference == StorageType.DEFAULT:\n                # Use SSD path\n                full_path = os.path.join(self.config.ssd_path, file_path.lstrip('/'))\n            elif storage_preference == StorageType.EXTERNAL_HDD:\n                # Use HDD path\n                full_path = os.path.join(self.config.hdd_path, file_path.lstrip('/'))\n            else:\n                # Default to SSD path\n                full_path = os.path.join(self.config.ssd_path, file_path.lstrip('/'))\n\n            logger.warning(f"No available storage device found, using config path: {full_path}")\n        else:\n            # Create full file path on target device\n            full_path = os.path.join(target_device.mount_point, file_path.lstrip('/'))\n\n        try:\n            # Ensure directory exists\n            Path(full_path).parent.mkdir(parents=True, exist_ok=True)\n\n            # Start timing for performance monitoring\n            start_time = time.time()\n\n            # Write the file\n            if isinstance(data, (bytes, bytearray, str)):\n                with open(full_path, 'wb' if isinstance(data, (bytes, bytearray)) else 'w') as f:\n                    f.write(data)\n            else:\n                # For other data types, use pickle\n                with open(full_path, 'wb') as f:\n                    pickle.dump(data, f)\n\n            # Record performance if we have a target device\n            if target_device:\n                duration = time.time() - start_time\n                self.performance_monitor.record_io_operation(\n                    target_device.mount_point,\n                    'write',\n                    file_size,\n                    duration\n                )\n\n                # Record access\n                self.access_tracker.record_access(full_path)\n\n            conditional_debug(logger, f"File written to {full_path}")\n            return True\n\n        except Exception as e:\n            logger.error(f"Error writing file {full_path}: {e}")\n            return False\n    \n    def read_file(self, file_path: str) -> Optional[Any]:\n        """Read a file from storage, handling potential external storage disconnection"""\n        # First, try to find the file in any of our known devices\n        actual_file_path = None\n        device = None\n\n        for dev in self.devices.values():\n            potential_path = os.path.join(dev.mount_point, file_path.lstrip('/'))\n            if os.path.exists(potential_path):\n                actual_file_path = potential_path\n                device = dev\n                break\n\n        # If not found in devices, try config paths as fallback\n        if not actual_file_path:\n            # Check in SSD path\n            potential_path = os.path.join(self.config.ssd_path, file_path.lstrip('/'))\n            if os.path.exists(potential_path):\n                actual_file_path = potential_path\n            else:\n                # Check in HDD path\n                potential_path = os.path.join(self.config.hdd_path, file_path.lstrip('/'))\n                if os.path.exists(potential_path):\n                    actual_file_path = potential_path\n                else:\n                    # Check in fallback path\n                    potential_path = os.path.join(self.config.fallback_storage_path, file_path.lstrip('/'))\n                    if os.path.exists(potential_path):\n                        actual_file_path = potential_path\n\n        if not actual_file_path:\n            logger.error(f"File not found: {file_path}")\n            return None\n\n        try:\n            # Start timing for performance monitoring\n            start_time = time.time()\n\n            # Read the file\n            if actual_file_path.endswith(('.pkl', '.pickle')):\n                with open(actual_file_path, 'rb') as f:\n                    data = pickle.load(f)\n            else:\n                with open(actual_file_path, 'rb') as f:\n                    data = f.read()\n\n            # Record performance if we have a device\n            if device:\n                duration = time.time() - start_time\n                file_size = len(data) if isinstance(data, (bytes, bytearray)) else os.path.getsize(actual_file_path)\n                self.performance_monitor.record_io_operation(\n                    device.mount_point,\n                    'read',\n                    file_size,\n                    duration\n                )\n\n                # Record access\n                self.access_tracker.record_access(actual_file_path)\n\n            conditional_debug(logger, f"File read from {actual_file_path}")\n            return data\n\n        except Exception as e:\n            logger.error(f"Error reading file {actual_file_path}: {e}")\n\n            # Try fallback if enabled\n            if self.config.enable_fallback:\n                fallback_path = os.path.join(self.config.fallback_storage_path, os.path.basename(actual_file_path))\n                if os.path.exists(fallback_path):\n                    logger.info(f"Found file in fallback storage: {fallback_path}")\n                    try:\n                        with open(fallback_path, 'rb') as f:\n                            return f.read()\n                    except Exception as fallback_error:\n                        logger.error(f"Error reading from fallback: {fallback_error}")\n\n            return None\n    \n    def delete_file(self, file_path: str) -> bool:\n        """Delete a file from storage"""\n        actual_file_path = None\n        \n        for dev in self.devices.values():\n            potential_path = os.path.join(dev.mount_point, file_path.lstrip('/'))\n            if os.path.exists(potential_path):\n                actual_file_path = potential_path\n                break\n        \n        if not actual_file_path:\n            logger.error(f"File not found for deletion: {file_path}")\n            return False\n        \n        try:\n            os.remove(actual_file_path)\n            conditional_debug(logger, f"File deleted: {actual_file_path}")\n            return True\n        except Exception as e:\n            logger.error(f"Error deleting file {actual_file_path}: {e}")\n            return False\n    \n    def move_file(self, source_path: str, dest_path: str, storage_preference: StorageType = None) -> bool:\n        """Move a file between storage locations"""\n        # Read the file\n        data = self.read_file(source_path)\n        if data is None:\n            return False\n        \n        # Determine destination storage\n        if storage_preference:\n            dest_device = None\n            for device in self.devices.values():\n                if device.storage_type == storage_preference and device.is_available:\n                    dest_device = device\n                    break\n        else:\n            # Determine based on size and access pattern\n            if isinstance(data, (bytes, bytearray)):\n                file_size = len(data)\n            else:\n                try:\n                    serialized_data = pickle.dumps(data)\n                    file_size = len(serialized_data)\n                except:\n                    file_size = 1024 * 1024  # 1MB default\n            \n            dest_device = self.get_optimal_storage_for_file(file_size)\n        \n        if not dest_device:\n            logger.error("No available storage device for destination")\n            return False\n        \n        # Write to destination\n        dest_full_path = os.path.join(dest_device.mount_point, dest_path.lstrip('/'))\n        if self.write_file(dest_full_path, data):\n            # Delete source file if write was successful\n            self.delete_file(source_path)\n            return True\n        else:\n            return False\n    \n    def get_storage_stats(self) -> Dict[str, Any]:\n        """Get comprehensive storage statistics"""\n        stats = {\n            'devices': {},\n            'total_space': 0,\n            'available_space': 0,\n            'performance': {}\n        }\n        \n        for mount_point, device in self.devices.items():\n            device_stats = {\n                'device_path': device.device_path,\n                'storage_type': device.storage_type.value,\n                'total_space_gb': device.total_space / (1024**3),\n                'available_space_gb': device.available_space / (1024**3),\n                'used_space_gb': (device.total_space - device.available_space) / (1024**3),\n                'utilization_percent': (device.total_space - device.available_space) / device.total_space * 100,\n                'read_speed_mb_s': device.read_speed,\n                'write_speed_mb_s': device.write_speed,\n                'latency_ms': device.latency,\n                'is_available': device.is_available\n            }\n            \n            stats['devices'][mount_point] = device_stats\n            stats['total_space'] += device.total_space\n            stats['available_space'] += device.available_space\n            \n            # Add performance metrics\n            perf_metrics = self.performance_monitor.get_recent_performance(mount_point)\n            stats['performance'][mount_point] = perf_metrics\n        \n        stats['total_space_gb'] = stats['total_space'] / (1024**3)\n        stats['available_space_gb'] = stats['available_space'] / (1024**3)\n        stats['used_space_gb'] = (stats['total_space'] - stats['available_space']) / (1024**3)\n        stats['overall_utilization_percent'] = (stats['total_space'] - stats['available_space']) / stats['total_space'] * 100 if stats['total_space'] > 0 else 0\n        \n        return stats\n    \n    def get_hot_files(self, n: int = 10) -> List[str]:\n        """Get top N most frequently accessed files"""\n        return self.access_tracker.get_hot_files(n)\n    \n    def predict_file_access(self, file_path: str) -> Tuple[float, Optional[float]]:\n        """Predict probability and time of next access for a file"""\n        return self.access_tracker.predict_access(file_path)\n    \n    def refresh_device_status(self):\n        """Refresh the status of all storage devices"""\n        with self._init_lock:\n            # Get current partitions\n            current_partitions = set(p.mountpoint for p in psutil.disk_partitions())\n            detected_devices = {}\n            \n            for partition in psutil.disk_partitions():\n                try:\n                    # Get disk usage\n                    usage = psutil.disk_usage(partition.mountpoint)\n                    \n                    # Check if this device was previously known\n                    if partition.mountpoint in self.devices:\n                        # Update existing device info\n                        old_device = self.devices[partition.mountpoint]\n                        old_device.total_space = usage.total\n                        old_device.available_space = usage.free\n                        old_device.is_available = True\n                        detected_devices[partition.mountpoint] = old_device\n                    else:\n                        # New device detected\n                        storage_type = self._determine_storage_type(partition)\n                        read_speed, write_speed, latency = self._estimate_performance(partition.mountpoint)\n                        \n                        new_device = StorageDevice(\n                            device_path=partition.device,\n                            storage_type=storage_type,\n                            total_space=usage.total,\n                            available_space=usage.free,\n                            read_speed=read_speed,\n                            write_speed=write_speed,\n                            latency=latency,\n                            is_available=True,\n                            mount_point=partition.mountpoint,\n                            device_id=partition.device\n                        )\n                        \n                        detected_devices[partition.mountpoint] = new_device\n                        logger.info(f"New storage device detected: {new_device}")\n                \n                except Exception as e:\n                    logger.warning(f"Could not update device {partition.mountpoint}: {e}")\n            \n            # Mark any previously known devices that are no longer present as unavailable\n            for mount_point, device in self.devices.items():\n                if mount_point not in detected_devices:\n                    device.is_available = False\n                    detected_devices[mount_point] = device\n                    logger.warning(f"Storage device no longer available: {device}")\n            \n            self.devices = detected_devices\n\n\nclass Qwen3VLStorageManager(StorageManager):\n    """Qwen3-VL specific storage manager with model-specific optimizations"""\n    \n    def __init__(self, config: StorageConfig = None):\n        super().__init__(config)\n        \n        # Model-specific storage paths\n        self.model_weights_path = "model_weights"\n        self.tensor_cache_path = "tensor_cache"\n        self.compressed_tensors_path = "compressed_tensors"\n        self.archived_tensors_path = "archived_tensors"\n        \n        # Initialize model-specific directories\n        self._init_model_directories()\n        \n        # Model-specific access patterns\n        self.weight_access_frequency = defaultdict(int)\n        self.tensor_access_frequency = defaultdict(int)\n        self._model_lock = threading.Lock()\n    \n    def _init_model_directories(self):\n        """Initialize model-specific storage directories"""\n        model_dirs = [\n            os.path.join(self.config.ssd_path, self.model_weights_path),\n            os.path.join(self.config.ssd_path, self.tensor_cache_path),\n            os.path.join(self.config.hdd_path, self.compressed_tensors_path),\n            os.path.join(self.config.hdd_path, self.archived_tensors_path)\n        ]\n        \n        for directory in model_dirs:\n            try:\n                Path(directory).mkdir(parents=True, exist_ok=True)\n            except Exception as e:\n                logger.warning(f"Could not create model directory {directory}: {e}")\n    \n    def store_model_weights(self, weights: Dict[str, Any], model_name: str = "qwen3_vl") -> bool:\n        """Store model weights on appropriate storage based on access patterns"""\n        with self._model_lock:\n            # Model weights are accessed frequently during inference, so store on SSD\n            weights_path = os.path.join(self.model_weights_path, f"{model_name}_weights.pkl")\n            \n            # Update access frequency\n            self.weight_access_frequency[model_name] += 1\n            \n            return self.write_file(\n                weights_path,\n                weights,\n                storage_preference=StorageType.NVME_SSD\n            )\n    \n    def load_model_weights(self, model_name: str = "qwen3_vl") -> Optional[Dict[str, Any]]:\n        """Load model weights from storage"""\n        weights_path = os.path.join(self.model_weights_path, f"{model_name}_weights.pkl")\n        return self.read_file(weights_path)\n    \n    def store_tensor(self, tensor: Any, tensor_name: str, compression: bool = True) -> bool:\n        """Store tensor with intelligent placement based on size and access pattern"""\n        # Determine tensor size\n        try:\n            serialized_tensor = pickle.dumps(tensor)\n            tensor_size = len(serialized_tensor)\n        except:\n            tensor_size = 10 * 1024 * 1024  # 10MB default if serialization fails\n\n        with self._model_lock:\n            # Update access frequency\n            self.tensor_access_frequency[tensor_name] += 1\n\n            # Determine storage location based on size and access frequency\n            access_freq = self.tensor_access_frequency[tensor_name]\n\n            if tensor_size < 100 * 1024 * 1024 and access_freq > 5:  # <100MB and accessed >5 times\n                # Hot tensor - store on SSD\n                tensor_path = os.path.join(self.tensor_cache_path, f"{tensor_name}.pkl")\n                storage_pref = StorageType.NVME_SSD\n                # For hot tensors, don't compress as they need fast access\n                return self.write_file(tensor_path, tensor, storage_pref)\n            elif compression and self.config.enable_compression:\n                # Compress and store on HDD\n                tensor_path = os.path.join(self.compressed_tensors_path, f"{tensor_name}_compressed.pkl")\n                storage_pref = StorageType.EXTERNAL_HDD\n                # Apply compression if enabled\n                if self.config.enable_compression:\n                    import gzip\n                    try:\n                        serialized_tensor = pickle.dumps(tensor)\n                        compressed_data = gzip.compress(serialized_tensor, compresslevel=self.config.compression_level)\n                        return self.write_file(tensor_path, compressed_data, storage_pref)\n                    except Exception as e:\n                        logger.warning(f"Compression failed for {tensor_name}, storing uncompressed: {e}")\n                        # Fall back to uncompressed storage on HDD\n                        tensor_path = os.path.join(self.archived_tensors_path, f"{tensor_name}.pkl")\n                        storage_pref = StorageType.EXTERNAL_HDD\n                        return self.write_file(tensor_path, tensor, storage_pref)\n            else:\n                # Large tensor - store on HDD\n                tensor_path = os.path.join(self.archived_tensors_path, f"{tensor_name}.pkl")\n                storage_pref = StorageType.EXTERNAL_HDD\n                return self.write_file(tensor_path, tensor, storage_pref)\n    \n    def load_tensor(self, tensor_name: str, compressed: bool = False) -> Optional[Any]:\n        """Load tensor from storage with fallback mechanisms"""\n        # Try different possible locations\n        possible_paths = []\n\n        # Check in compressed tensors path first if looking for compressed data\n        if compressed:\n            possible_paths.append(os.path.join(self.compressed_tensors_path, f"{tensor_name}_compressed.pkl"))\n\n        # Check other locations\n        possible_paths.extend([\n            os.path.join(self.tensor_cache_path, f"{tensor_name}.pkl"),\n            os.path.join(self.archived_tensors_path, f"{tensor_name}.pkl")\n        ])\n\n        for path in possible_paths:\n            data = self.read_file(path)\n            if data is not None:\n                # Check if the file is compressed based on its path or content\n                is_compressed_file = '_compressed' in path or compressed\n\n                if is_compressed_file:\n                    try:\n                        # If the data looks like compressed data (starts with gzip header), decompress it\n                        if isinstance(data, bytes) and len(data) > 2 and data[:2] == b'\x1f\x8b':\n                            import gzip\n                            decompressed_data = gzip.decompress(data)\n                            return pickle.loads(decompressed_data)\n                        else:\n                            # Data might not be compressed, try to unpickle directly\n                            return pickle.loads(data)\n                    except Exception as e:\n                        logger.error(f"Error decompressing or unpickling tensor {tensor_name}: {e}")\n                        return None\n                else:\n                    # For non-compressed files, try to unpickle\n                    try:\n                        return pickle.loads(data)\n                    except:\n                        # If unpickling fails, return raw data\n                        return data\n\n        logger.error(f"Tensor {tensor_name} not found in any storage location")\n        return None\n    \n    def store_model_artifact(self, artifact: Any, artifact_name: str, artifact_type: str) -> bool:\n        """Store model artifacts (cache, checkpoints, etc.) with intelligent placement"""\n        with self._model_lock:\n            # Determine optimal storage based on artifact type\n            if artifact_type in ["cache", "temporary", "frequent_access"]:\n                # Store on SSD for quick access\n                artifact_path = os.path.join(self.tensor_cache_path, f"{artifact_name}.pkl")\n                storage_pref = StorageType.NVME_SSD\n            elif artifact_type in ["checkpoint", "backup", "archive"]:\n                # Store on HDD for large storage\n                artifact_path = os.path.join(self.archived_tensors_path, f"{artifact_name}.pkl")\n                storage_pref = StorageType.EXTERNAL_HDD\n            else:\n                # Default to intelligent placement\n                try:\n                    artifact_size = len(pickle.dumps(artifact))\n                except:\n                    artifact_size = 10 * 1024 * 1024  # 10MB default\n                \n                storage_pref = None  # Let the system decide based on size\n            \n            return self.write_file(artifact_path, artifact, storage_pref)\n    \n    def migrate_hot_tensors(self):\n        """Migrate frequently accessed tensors to faster storage"""\n        with self._model_lock:\n            # Get hot files\n            hot_files = self.get_hot_files(20)  # Get top 20 hot files\n            \n            migrated_count = 0\n            for file_path in hot_files:\n                # Check if file is currently on HDD but should be on SSD\n                if StorageType.EXTERNAL_HDD.value in file_path:\n                    # Calculate access frequency\n                    file_name = os.path.basename(file_path)\n                    if file_name.startswith('tensor_') and file_name.endswith('.pkl'):\n                        tensor_name = file_name.replace('.pkl', '')\n                        if tensor_name in self.tensor_access_frequency:\n                            access_freq = self.tensor_access_frequency[tensor_name]\n                            if access_freq > 10:  # If accessed more than 10 times\n                                # Move to SSD cache\n                                new_path = os.path.join(self.tensor_cache_path, file_name)\n                                if self.move_file(file_path, new_path, StorageType.NVME_SSD):\n                                    migrated_count += 1\n                                    logger.info(f"Migrated hot tensor {tensor_name} to SSD")\n            \n            logger.info(f"Migrated {migrated_count} hot tensors to faster storage")\n            return migrated_count\n    \n    def compress_cold_tensors(self):\n        """Compress infrequently accessed tensors to save space"""\n        with self._model_lock:\n            # This would implement compression logic for cold tensors\n            # For now, it's a placeholder\n            logger.info("Compression of cold tensors is a placeholder implementation")\n            return 0\n\n\ndef create_qwen3vl_storage_manager(config: StorageConfig = None) -> Qwen3VLStorageManager:\n    """Factory function to create a Qwen3-VL storage manager"""\n    return Qwen3VLStorageManager(config)\n\n\ndef integrate_with_qwen3vl_model(storage_manager: Qwen3VLStorageManager):\n    """Integration functions for Qwen3-VL model components"""\n    \n    def save_model_weights(model_state_dict, model_name: str = "qwen3_vl"):\n        """Save model weights using the storage manager"""\n        return storage_manager.store_model_weights(model_state_dict, model_name)\n    \n    def load_model_weights(model_name: str = "qwen3_vl"):\n        """Load model weights using the storage manager"""\n        return storage_manager.load_model_weights(model_name)\n    \n    def save_tensor(tensor, tensor_name: str, compression: bool = True):\n        """Save tensor using the storage manager"""\n        return storage_manager.store_tensor(tensor, tensor_name, compression)\n    \n    def load_tensor(tensor_name: str, compressed: bool = False):\n        """Load tensor using the storage manager"""\n        return storage_manager.load_tensor(tensor_name, compressed)\n    \n    def save_model_artifact(artifact, artifact_name: str, artifact_type: str):\n        """Save model artifact using the storage manager"""\n        return storage_manager.store_model_artifact(artifact, artifact_name, artifact_type)\n    \n    def run_storage_optimizations():\n        """Run storage optimization routines"""\n        storage_manager.migrate_hot_tensors()\n        storage_manager.compress_cold_tensors()\n    \n    return {\n        'save_model_weights': save_model_weights,\n        'load_model_weights': load_model_weights,\n        'save_tensor': save_tensor,\n        'load_tensor': load_tensor,\n        'save_model_artifact': save_model_artifact,\n        'run_storage_optimizations': run_storage_optimizations\n    }\n\n\n# Example usage and testing\nif __name__ == "__main__":\n    print("Qwen3-VL Storage Management System")\n    print("=" * 50)\n    \n    # Create storage configuration\n    config = StorageConfig(\n        ssd_path="./test_ssd_cache",\n        hdd_path="./test_hdd_storage",\n        enable_tiering=True,\n        tiering_threshold=50 * 1024 * 1024,  # 50MB threshold\n        cache_size_limit=1 * 1024 * 1024 * 1024,  # 1GB cache\n        enable_compression=True,\n        compression_level=6,\n        external_storage_timeout=30,\n        enable_fallback=True,\n        fallback_storage_path="./test_fallback"\n    )\n    \n    # Create storage manager\n    storage_manager = create_qwen3vl_storage_manager(config)\n    \n    # Display storage statistics\n    print("\n1. Storage Statistics:")\n    stats = storage_manager.get_storage_stats()\n    for device_path, device_stats in stats['devices'].items():\n        print(f"   Device: {device_path}")\n        print(f"     Type: {device_stats['storage_type']}")\n        print(f"     Total: {device_stats['total_space_gb']:.2f}GB")\n        print(f"     Available: {device_stats['available_space_gb']:.2f}GB")\n        print(f"     Utilization: {device_stats['utilization_percent']:.1f}%")\n        print(f"     Read Speed: {device_stats['read_speed_mb_s']:.1f}MB/s")\n        print(f"     Write Speed: {device_stats['write_speed_mb_s']:.1f}MB/s")\n        print(f"     Latency: {device_stats['latency_ms']:.2f}ms")\n        print(f"     Available: {device_stats['is_available']}")\n    \n    print(f"\n   Overall Utilization: {stats['overall_utilization_percent']:.1f}%")\n    \n    # Test file operations\n    print(f"\n2. Testing file operations...")\n    \n    # Write a test file to SSD\n    test_data_ssd = b"This is test data for SSD storage" * 10000  # ~300KB\n    success_ssd = storage_manager.write_file("test_ssd_file.txt", test_data_ssd, StorageType.NVME_SSD)\n    print(f"   SSD write success: {success_ssd}")\n    \n    # Write a test file to HDD\n    test_data_hdd = b"This is test data for HDD storage" * 50000  # ~1.5MB\n    success_hdd = storage_manager.write_file("test_hdd_file.txt", test_data_hdd, StorageType.EXTERNAL_HDD)\n    print(f"   HDD write success: {success_hdd}")\n    \n    # Write a test file without preference (system decides)\n    test_data_auto = b"This is auto-placed test data" * 20000  # ~600KB\n    success_auto = storage_manager.write_file("test_auto_file.txt", test_data_auto)\n    print(f"   Auto placement success: {success_auto}")\n    \n    # Read the files back\n    read_data_ssd = storage_manager.read_file("test_ssd_file.txt")\n    print(f"   SSD read success: {read_data_ssd is not None}")\n    \n    read_data_hdd = storage_manager.read_file("test_hdd_file.txt")\n    print(f"   HDD read success: {read_data_hdd is not None}")\n    \n    read_data_auto = storage_manager.read_file("test_auto_file.txt")\n    print(f"   Auto placement read success: {read_data_auto is not None}")\n    \n    # Test model-specific operations\n    print(f"\n3. Testing model-specific operations...")\n    \n    # Simulate model weights\n    mock_weights = {\n        'layer_1': [1.0, 2.0, 3.0] * 1000,\n        'layer_2': [4.0, 5.0, 6.0] * 1000,\n        'layer_3': [7.0, 8.0, 9.0] * 1000\n    }\n    \n    weights_success = storage_manager.store_model_weights(mock_weights, "test_model")\n    print(f"   Model weights storage success: {weights_success}")\n    \n    loaded_weights = storage_manager.load_model_weights("test_model")\n    print(f"   Model weights load success: {loaded_weights is not None}")\n    \n    # Test tensor operations\n    mock_tensor = [[1.0, 2.0, 3.0]] * 5000  # Simulate a tensor\n    tensor_success = storage_manager.store_tensor(mock_tensor, "test_tensor", compression=True)\n    print(f"   Tensor storage success: {tensor_success}")\n    \n    loaded_tensor = storage_manager.load_tensor("test_tensor", compressed=True)\n    print(f"   Tensor load success: {loaded_tensor is not None}")\n    \n    # Test access pattern tracking\n    print(f"\n4. Testing access pattern tracking...")\n    \n    # Access the same file multiple times to create a hot pattern\n    for i in range(5):\n        _ = storage_manager.read_file("test_ssd_file.txt")\n        time.sleep(0.01)  # Small delay\n    \n    # Check hot files\n    hot_files = storage_manager.get_hot_files(5)\n    print(f"   Hot files: {hot_files}")\n    \n    # Predict access for a file\n    prob, next_access = storage_manager.predict_file_access("test_ssd_file.txt")\n    print(f"   Predicted access for test file: {prob:.2f} probability at {datetime.fromtimestamp(next_access) if next_access else 'unknown'}")\n    \n    # Run storage optimizations\n    print(f"\n5. Running storage optimizations...")\n    migrated_count = storage_manager.migrate_hot_tensors()\n    print(f"   Migrated {migrated_count} hot tensors to faster storage")\n    \n    compressed_count = storage_manager.compress_cold_tensors()\n    print(f"   Compressed {compressed_count} cold tensors")\n    \n    # Test device refresh\n    print(f"\n6. Testing device refresh...")\n    storage_manager.refresh_device_status()\n    print(f"   Device status refreshed")\n    \n    # Get final statistics\n    print(f"\n7. Final storage statistics:")\n    final_stats = storage_manager.get_storage_stats()\n    print(f"   Total devices: {len(final_stats['devices'])}")\n    print(f"   Total space: {final_stats['total_space_gb']:.2f}GB")\n    print(f"   Available space: {final_stats['available_space_gb']:.2f}GB")\n    \n    print(f"\nQwen3-VL Storage Management System initialized successfully!")\n    print(f"This system intelligently manages storage across different device types based on performance characteristics.")