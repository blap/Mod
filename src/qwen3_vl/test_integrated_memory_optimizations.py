"""\nTest file for integrated memory optimizations in Qwen3-VL model\n\nThis test validates that all memory optimization techniques (pooling, caching, compression,\nswapping, tiering, and garbage collection) are properly integrated with the Qwen3-VL model.\n"""\nimport torch\nimport pytest\nfrom typing import Dict, Any\n\n# Import with proper relative imports\nimport sys\nimport os\n# Add the root directory to path\nroot_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nif root_dir not in sys.path:\n    sys.path.insert(0, root_dir)\n\nfrom ..config import Qwen3VLConfig\nfrom ..models.modeling_qwen3_vl_integrated import Qwen3VLForConditionalGeneration\nfrom ..optimization.integrated_memory_manager import IntegratedMemoryManager, create_optimized_memory_manager\n\n\ndef create_test_config() -> Qwen3VLConfig:\n    """Create a test configuration for Qwen3-VL"""\n    config = Qwen3VLConfig()\n    config.hidden_size = 512\n    config.num_attention_heads = 8\n    config.num_hidden_layers = 4\n    config.vocab_size = 1000\n    config.max_position_embeddings = 512\n    config.vision_hidden_size = 512\n    config.vision_num_attention_heads = 8\n    config.vision_num_hidden_layers = 4\n    config.vision_image_size = 224\n    config.vision_patch_size = 14\n    config.vision_num_channels = 3\n    return config\n\n\ndef test_memory_manager_creation():\n    """Test that the integrated memory manager can be created properly"""\n    hardware_config = {\n        'cpu_model': 'Intel i5-10210U',\n        'gpu_model': 'NVIDIA SM61',\n        'memory_size': 8 * 1024 * 1024 * 1024,  # 8GB\n        'storage_type': 'nvme'\n    }\n    \n    memory_manager = create_optimized_memory_manager(hardware_config)\n    assert memory_manager is not None\n    assert memory_manager.memory_optimizer is not None\n    assert memory_manager.compression_manager is not None\n    assert memory_manager.swapping_system is not None\n    assert memory_manager.tiering_system is not None\n    assert memory_manager.lifecycle_manager is not None\n    \n    # Test that all components were properly initialized\n    stats = memory_manager.get_stats()\n    assert 'total_tensors_managed' in stats\n    assert 'compression_ratio' in stats\n    \n    print("✓ Memory manager creation test passed")\n\n\ndef test_model_with_memory_optimizations():\n    """Test that the Qwen3-VL model can be created with memory optimizations"""\n    config = create_test_config()\n    \n    # Create memory manager\n    hardware_config = {\n        'cpu_model': 'Intel i5-10210U',\n        'gpu_model': 'NVIDIA SM61',\n        'memory_size': 8 * 1024 * 1024 * 1024,  # 8GB\n        'storage_type': 'nvme'\n    }\n    memory_manager = create_optimized_memory_manager(hardware_config)\n    \n    # Create model with memory manager\n    model = Qwen3VLForConditionalGeneration(config, memory_manager=memory_manager)\n    \n    # Check that model was created successfully\n    assert model is not None\n    assert model.memory_manager is memory_manager\n    \n    # Check that all components have the memory manager\n    assert model.vision_tower.memory_manager is memory_manager\n    assert model.multi_modal_projector.memory_manager is memory_manager\n    assert model.language_model.memory_manager is memory_manager\n    \n    # Check that all layers have the memory manager\n    for layer in model.language_model.layers:\n        assert layer.memory_manager is memory_manager\n        assert layer.self_attn.memory_manager is memory_manager\n        assert layer.mlp.memory_manager is memory_manager\n    \n    for layer in model.vision_tower.layers:\n        assert layer.memory_manager is memory_manager\n        assert layer.attn.memory_manager is memory_manager\n        assert layer.mlp.memory_manager is memory_manager\n    \n    print("✓ Model with memory optimizations test passed")\n\n\ndef test_tensor_allocation_with_memory_manager():\n    """Test tensor allocation using the memory manager"""\n    hardware_config = {\n        'cpu_model': 'Intel i5-10210U',\n        'gpu_model': 'NVIDIA SM61',\n        'memory_size': 8 * 1024 * 1024 * 1024,  # 8GB\n        'storage_type': 'nvme'\n    }\n    memory_manager = create_optimized_memory_manager(hardware_config)\n    \n    # Test tensor allocation\n    tensor = memory_manager.allocate_tensor((10, 20, 512), torch.float32, "general")\n    assert tensor.shape == (10, 20, 512)\n    assert tensor.dtype == torch.float32\n    \n    # Test that tensor has been registered with lifecycle manager\n    if memory_manager.lifecycle_manager:\n        # Access the tensor to update its state\n        accessed_tensor = memory_manager.access_tensor(tensor, "test_context")\n        assert accessed_tensor is tensor\n    \n    # Check stats were updated\n    stats = memory_manager.get_stats()\n    assert stats['total_tensors_managed'] >= 1\n    \n    print("✓ Tensor allocation with memory manager test passed")\n\n\ndef test_model_forward_pass_with_memory_optimizations():\n    """Test that the model can perform a forward pass with memory optimizations"""\n    config = create_test_config()\n    \n    # Create memory manager\n    hardware_config = {\n        'cpu_model': 'Intel i5-10210U',\n        'gpu_model': 'NVIDIA SM61',\n        'memory_size': 8 * 1024 * 1024 * 1024,  # 8GB\n        'storage_type': 'nvme'\n    }\n    memory_manager = create_optimized_memory_manager(hardware_config)\n    \n    # Create model\n    model = Qwen3VLForConditionalGeneration(config, memory_manager=memory_manager)\n    model.eval()  # Set to evaluation mode\n    \n    # Create test inputs\n    batch_size = 2\n    seq_len = 10\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n    \n    # Test text-only forward pass\n    with torch.no_grad():\n        output = model(input_ids=input_ids)\n        assert output.shape[0] == batch_size\n        assert output.shape[1] == seq_len\n        assert output.shape[2] == config.hidden_size\n    \n    # Create test image inputs\n    pixel_values = torch.randn(batch_size, 3, 224, 224)  # RGB image\n    \n    # Test vision-language forward pass\n    with torch.no_grad():\n        output = model(input_ids=input_ids, pixel_values=pixel_values)\n        assert output.shape[0] == batch_size\n        assert output.shape[1] == seq_len  # Might be different based on image features\n        assert output.shape[2] == config.hidden_size\n    \n    # Check that memory optimizations were performed\n    stats = memory_manager.get_stats()\n    # Stats should have been updated during the forward pass\n    print(f"Memory optimization stats: {stats}")\n    \n    print("✓ Model forward pass with memory optimizations test passed")\n\n\ndef test_memory_optimization_stats():\n    """Test that memory optimization statistics are properly tracked"""\n    hardware_config = {\n        'cpu_model': 'Intel i5-10210U',\n        'gpu_model': 'NVIDIA SM61',\n        'memory_size': 8 * 1024 * 1024 * 1024,  # 8GB\n        'storage_type': 'nvme'\n    }\n    memory_manager = create_optimized_memory_manager(hardware_config)\n    \n    # Perform some operations to generate stats\n    tensor1 = memory_manager.allocate_tensor((5, 10, 512), torch.float32, "kv_cache")\n    tensor2 = memory_manager.allocate_tensor((5, 10, 512), torch.float32, "image_features")\n    \n    # Access tensors\n    memory_manager.access_tensor(tensor1, "context1")\n    memory_manager.access_tensor(tensor2, "context2")\n    \n    # Perform memory optimizations\n    memory_manager.perform_memory_optimizations()\n    \n    # Get stats\n    stats = memory_manager.get_stats()\n    \n    # Check that basic stats are present\n    assert 'total_tensors_managed' in stats\n    assert 'total_memory_saved_gb' in stats\n    assert 'compression_ratio' in stats\n    \n    # Check component-specific stats are present\n    assert 'compression_stats' in stats\n    assert 'swapping_stats' in stats\n    assert 'tiering_stats' in stats\n    assert 'gc_stats' in stats\n    \n    print("✓ Memory optimization stats test passed")\n\n\ndef test_model_generation_with_memory_optimizations():\n    """Test that the model can generate text with memory optimizations"""\n    config = create_test_config()\n    \n    # Create memory manager\n    hardware_config = {\n        'cpu_model': 'Intel i5-10210U',\n        'gpu_model': 'NVIDIA SM61',\n        'memory_size': 8 * 1024 * 1024 * 1024,  # 8GB\n        'storage_type': 'nvme'\n    }\n    memory_manager = create_optimized_memory_manager(hardware_config)\n    \n    # Create model\n    model = Qwen3VLForConditionalGeneration(config, memory_manager=memory_manager)\n    model.eval()\n    \n    # Create test inputs\n    batch_size = 1\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, 5))\n    \n    # Generate text\n    with torch.no_grad():\n        generated_ids = model.generate(\n            input_ids=input_ids,\n            max_new_tokens=10,\n            do_sample=False\n        )\n        \n        # Check that generation worked\n        assert generated_ids.shape[0] == batch_size\n        assert generated_ids.shape[1] >= input_ids.shape[1]  # At least as many tokens as input + generated\n    \n    # Check that memory optimizations were performed during generation\n    stats = memory_manager.get_stats()\n    print(f"Generation stats: {stats}")\n    \n    print("✓ Model generation with memory optimizations test passed")\n\n\ndef test_cleanup_functionality():\n    """Test that cleanup works properly"""\n    hardware_config = {\n        'cpu_model': 'Intel i5-10210U',\n        'gpu_model': 'NVIDIA SM61',\n        'memory_size': 8 * 1024 * 1024 * 1024,  # 8GB\n        'storage_type': 'nvme'\n    }\n    memory_manager = create_optimized_memory_manager(hardware_config)\n    \n    # Perform some operations\n    tensor = memory_manager.allocate_tensor((5, 10, 512), torch.float32, "general")\n    \n    # Clean up\n    memory_manager.cleanup()\n    \n    # Verify cleanup was performed (this is more of a smoke test since cleanup is mostly about releasing resources)\n    print("✓ Cleanup functionality test passed")\n\n\ndef run_all_tests():\n    """Run all tests"""\n    print("Running integrated memory optimizations tests for Qwen3-VL...")\n    print("=" * 60)\n    \n    test_memory_manager_creation()\n    test_model_with_memory_optimizations()\n    test_tensor_allocation_with_memory_manager()\n    test_model_forward_pass_with_memory_optimizations()\n    test_memory_optimization_stats()\n    test_model_generation_with_memory_optimizations()\n    test_cleanup_functionality()\n    \n    print("=" * 60)\n    print("All tests passed! ✓")\n\n\nif __name__ == "__main__":\n    run_all_tests()