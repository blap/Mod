"""Comprehensive Tests for Unified Optimization Architecture\nValidates all 12 optimization techniques work together synergistically while maintaining model capacity.\n"""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Dict, List, Optional, Tuple, Any\nimport logging\nimport time\nimport math\nimport tempfile\nimport os\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass TestConfig:\n    """Configuration for testing the unified optimization architecture."""\n    num_hidden_layers: int = 32  # Full capacity requirement\n    num_attention_heads: int = 32  # Full capacity requirement\n    hidden_size: int = 4096  # Standard hidden size for 32 heads\n    intermediate_size: int = 11008\n    vocab_size: int = 32000\n    max_position_embeddings: int = 4096\n    rope_theta: float = 1000000\n    layer_norm_eps: float = 1e-6\n    attention_dropout_prob: float = 0.0\n    hidden_dropout_prob: float = 0.0\n    \n    # Vision parameters\n    vision_num_hidden_layers: int = 24\n    vision_num_attention_heads: int = 16\n    vision_hidden_size: int = 1152\n    vision_intermediate_size: int = 4304\n    vision_patch_size: int = 14\n    vision_image_size: int = 448\n    vision_num_channels: int = 3\n    vision_hidden_act: str = "gelu"\n    vision_hidden_dropout_prob: float = 0.0\n    vision_attention_dropout_prob: float = 0.0\n    vision_max_position_embeddings: int = 576\n    vision_rope_theta: float = 10000\n    vision_layer_norm_eps: float = 1e-6\n    \n    # Optimization flags\n    use_block_sparse_attention: bool = True\n    use_cross_modal_token_merging: bool = True\n    use_hierarchical_memory_compression: bool = True\n    use_learned_activation_routing: bool = True\n    use_adaptive_batch_processing: bool = True\n    use_cross_layer_parameter_recycling: bool = True\n    use_adaptive_sequence_packing: bool = True\n    use_memory_efficient_grad_accumulation: bool = False  # Disabled for inference\n    use_kv_cache_optimization: bool = True\n    use_faster_rotary_embeddings: bool = True\n    use_distributed_pipeline_parallelism: bool = False  # Disabled for testing\n    use_hardware_specific_kernels: bool = True\n    \n    # Optimization parameters\n    sparse_attention_sparsity_ratio: float = 0.5\n    vision_sparse_attention_sparsity_ratio: float = 0.4\n    kv_cache_strategy: str = "hybrid"\n    kv_cache_window_size: int = 1024\n    low_rank_dimension: int = 64\n    cross_layer_recycling_frequency: int = 4\n    adaptive_precision_strategy: str = "layer_specific"\n\n\nclass OptimizationTester:\n    """Tester for the unified optimization architecture."""\n    \n    def __init__(self, config: TestConfig):\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize optimization components\n        self._initialize_optimization_components()\n        \n        # Initialize performance metrics\n        self.performance_metrics = {\n            'baseline_time': 0.0,\n            'optimized_time': 0.0,\n            'baseline_memory': 0.0,\n            'optimized_memory': 0.0,\n            'accuracy_preserved': True,\n            'capacity_preserved': True\n        }\n    \n    def _initialize_optimization_components(self):\n        """Initialize all optimization components."""\nfrom optimization.unified_optimization_manager import OptimizationManager, OptimizationType, OptimizationConfig\nfrom optimization.interaction_handler import OptimizationInteractionHandler\nfrom optimization.performance_monitoring import PerformanceValidator\nfrom optimization.resource_management import OptimizationResourceManager\n        \n        # Create optimization configuration\n        opt_config = OptimizationConfig(\n            block_sparse_attention=self.config.use_block_sparse_attention,\n            cross_modal_token_merging=self.config.use_cross_modal_token_merging,\n            hierarchical_memory_compression=self.config.use_hierarchical_memory_compression,\n            learned_activation_routing=self.config.use_learned_activation_routing,\n            adaptive_batch_processing=self.config.use_adaptive_batch_processing,\n            cross_layer_parameter_recycling=self.config.use_cross_layer_parameter_recycling,\n            adaptive_sequence_packing=self.config.use_adaptive_sequence_packing,\n            memory_efficient_grad_accumulation=self.config.use_memory_efficient_grad_accumulation,\n            kv_cache_optimization=self.config.use_kv_cache_optimization,\n            faster_rotary_embeddings=self.config.use_faster_rotary_embeddings,\n            distributed_pipeline_parallelism=self.config.use_distributed_pipeline_parallelism,\n            hardware_specific_kernels=self.config.use_hardware_specific_kernels,\n            \n            # Optimization parameters\n            block_sparse_block_size=64,\n            block_sparse_sparsity_ratio=self.config.sparse_attention_sparsity_ratio,\n            kv_cache_strategy=self.config.kv_cache_strategy,\n            kv_cache_window_size=self.config.kv_cache_window_size,\n            low_rank_dimension=self.config.low_rank_dimension,\n            routing_temperature=1.0,\n            cross_layer_recycling_frequency=self.config.cross_layer_recycling_frequency,\n            adaptive_precision_strategy=self.config.adaptive_precision_strategy\n        )\n        \n        # Initialize optimization manager\n        self.optimization_manager = OptimizationManager(opt_config)\n        \n        # Initialize interaction handler\n        self.interaction_handler = OptimizationInteractionHandler(self.optimization_manager)\n        \n        # Initialize performance validator\n        self.performance_validator = PerformanceValidator(self.config)\n        \n        # Initialize resource manager\n        self.resource_manager = OptimizationResourceManager(self.config)\n        \n        self.logger.info("All optimization components initialized")\n    \n    def create_test_model(self, optimization_enabled: bool = False) -> nn.Module:\n        """Create a test model with or without optimizations."""\nfrom optimization.unified_architecture import UnifiedQwen3VLModel, create_unified_qwen3_vl_model\n        \n        if optimization_enabled:\n            return create_unified_qwen3_vl_model(self.config)\n        else:\n            # Create a basic transformer model for comparison\n            class BasicTransformerModel(nn.Module):\n                def __init__(self, config: TestConfig):\n                    super().__init__()\n                    self.config = config\n                    \n                    # Embedding layers\n                    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n                    \n                    # Language model layers\n                    self.layers = nn.ModuleList([\n                        nn.TransformerEncoderLayer(\n                            d_model=config.hidden_size,\n                            nhead=config.num_attention_heads,\n                            dim_feedforward=config.intermediate_size,\n                            dropout=config.hidden_dropout_prob,\n                            batch_first=True\n                        ) for _ in range(config.num_hidden_layers)\n                    ])\n                    \n                    # Final layer norm\n                    self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n                    \n                    # Language model head\n                    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n                \n                def forward(self, input_ids, attention_mask=None):\n                    hidden_states = self.embed_tokens(input_ids)\n                    \n                    for layer in self.layers:\n                        hidden_states = layer(hidden_states)\n                    \n                    hidden_states = self.norm(hidden_states)\n                    logits = self.lm_head(hidden_states)\n                    \n                    return logits\n            \n            return BasicTransformerModel(self.config)\n    \n    def test_capacity_preservation(self) -> bool:\n        """Test that model capacity (32 layers, 32 heads) is preserved with optimizations."""\n        print("Testing capacity preservation (32 layers, 32 heads)...")\n        \n        # Create model with all optimizations enabled\n        optimized_model = self.create_test_model(optimization_enabled=True)\n        \n        # Check layer count\n        expected_layers = self.config.num_hidden_layers  # 32\n        actual_layers = len(optimized_model.layers) if hasattr(optimized_model, 'layers') else 0\n        layers_preserved = actual_layers == expected_layers\n        \n        # Check attention head count\n        expected_heads = self.config.num_attention_heads  # 32\n        actual_heads = optimized_model.config.num_attention_heads if hasattr(optimized_model, 'config') else 0\n        heads_preserved = actual_heads == expected_heads\n        \n        # Check vision layer count\n        expected_vision_layers = self.config.vision_num_hidden_layers  # 24\n        actual_vision_layers = getattr(optimized_model.config, 'vision_num_hidden_layers', 0)\n        vision_layers_preserved = actual_vision_layers == expected_vision_layers\n        \n        # Check vision attention head count\n        expected_vision_heads = self.config.vision_num_attention_heads  # 16\n        actual_vision_heads = getattr(optimized_model.config, 'vision_num_attention_heads', 0)\n        vision_heads_preserved = actual_vision_heads == expected_vision_heads\n        \n        # Validate results\n        all_preserved = layers_preserved and heads_preserved and vision_layers_preserved and vision_heads_preserved\n        \n        print(f"  Language layers: {actual_layers}/{expected_layers} ({'✓' if layers_preserved else '✗'})")\n        print(f"  Language attention heads: {actual_heads}/{expected_heads} ({'✓' if heads_preserved else '✗'})")\n        print(f"  Vision layers: {actual_vision_layers}/{expected_vision_layers} ({'✓' if vision_layers_preserved else '✗'})")\n        print(f"  Vision attention heads: {actual_vision_heads}/{expected_vision_heads} ({'✓' if vision_heads_preserved else '✗'})")\n        \n        self.performance_metrics['capacity_preserved'] = all_preserved\n        return all_preserved\n    \n    def test_optimization_interaction_synergy(self) -> bool:\n        """Test that optimizations work synergistically without conflicts."""\n        print("Testing optimization interaction synergy...")\n        \n        # Get active optimizations\n        active_opts = self.optimization_manager.get_active_optimizations()\n        print(f"  Active optimizations: {len(active_opts)}")\n        \n        # Test each pair of optimizations for compatibility\n        compatibility_results = []\n        for i, opt1 in enumerate(active_opts):\n            for opt2 in active_opts[i+1:]:\n                try:\n                    # Check if optimizations are compatible\n                    is_compatible = self.interaction_handler.check_compatibility(opt1, opt2)\n                    compatibility_results.append(is_compatible)\n                except Exception as e:\n                    print(f"    Error checking compatibility between {opt1} and {opt2}: {e}")\n                    compatibility_results.append(False)\n        \n        # All optimizations should be compatible\n        all_compatible = all(compatibility_results)\n        \n        print(f"  Optimization compatibility: {sum(compatibility_results)}/{len(compatibility_results)} ({'✓' if all_compatible else '✗'})")\n        \n        # Test that interaction handling works\n        try:\n            # Apply optimizations with interaction handling\n            test_input = torch.randn(2, 32, self.config.hidden_size)  # [batch, seq, hidden]\n            result = self.interaction_handler.apply_optimizations_with_interaction_handling(\n                test_input, \n                layer_idx=0,\n                optimization_combo=active_opts\n            )\n            interaction_success = True\n        except Exception as e:\n            print(f"  Error in interaction handling: {e}")\n            interaction_success = False\n        \n        print(f"  Interaction handling: {'✓' if interaction_success else '✗'}")\n        \n        return all_compatible and interaction_success\n    \n    def test_performance_improvement(self) -> Dict[str, Any]:\n        """Test performance improvements from optimizations."""\n        print("Testing performance improvements...")\n        \n        # Create baseline model (no optimizations)\n        baseline_model = self.create_test_model(optimization_enabled=False)\n        baseline_model.eval()\n        \n        # Create optimized model (with optimizations)\n        optimized_model = self.create_test_model(optimization_enabled=True)\n        optimized_model.eval()\n        \n        # Create test input\n        batch_size, seq_len = 2, 64\n        input_ids = torch.randint(0, self.config.vocab_size, (batch_size, seq_len))\n        \n        # Warm up models\n        with torch.no_grad():\n            _ = baseline_model(input_ids)\n            _ = optimized_model(input_ids)\n        \n        # Benchmark baseline model\n        start_time = time.time()\n        baseline_memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n        for _ in range(5):  # Run multiple times for averaging\n            _ = baseline_model(input_ids)\n        baseline_time = (time.time() - start_time) / 5\n        baseline_memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n        baseline_memory = max(0, baseline_memory_after - baseline_memory_before)\n        \n        # Benchmark optimized model\n        start_time = time.time()\n        optimized_memory_before = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n        for _ in range(5):  # Run multiple times for averaging\n            _ = optimized_model(input_ids)\n        optimized_time = (time.time() - start_time) / 5\n        optimized_memory_after = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n        optimized_memory = max(0, optimized_memory_after - optimized_memory_before)\n        \n        # Calculate improvements\n        time_improvement = ((baseline_time - optimized_time) / baseline_time) * 100 if baseline_time > 0 else 0\n        memory_improvement = ((baseline_memory - optimized_memory) / baseline_memory) * 100 if baseline_memory > 0 else 0\n        \n        print(f"  Baseline time: {baseline_time:.4f}s")\n        print(f"  Optimized time: {optimized_time:.4f}s")\n        print(f"  Time improvement: {time_improvement:.2f}%")\n        print(f"  Baseline memory: {baseline_memory / 1024 / 1024:.2f}MB")\n        print(f"  Optimized memory: {optimized_memory / 1024 / 1024:.2f}MB")\n        print(f"  Memory improvement: {memory_improvement:.2f}%")\n        \n        # Update performance metrics\n        self.performance_metrics['baseline_time'] = baseline_time\n        self.performance_metrics['optimized_time'] = optimized_time\n        self.performance_metrics['baseline_memory'] = baseline_memory\n        self.performance_metrics['optimized_memory'] = optimized_memory\n        \n        return {\n            'time_improvement_percent': time_improvement,\n            'memory_improvement_percent': memory_improvement,\n            'baseline_time': baseline_time,\n            'optimized_time': optimized_time,\n            'performance_improved': time_improvement > 0 or memory_improvement > 0\n        }\n    \n    def test_accuracy_preservation(self) -> bool:\n        """Test that accuracy is preserved with optimizations."""\n        print("Testing accuracy preservation...")\n        \n        # Create models\n        baseline_model = self.create_test_model(optimization_enabled=False)\n        baseline_model.eval()\n        \n        optimized_model = self.create_test_model(optimization_enabled=True)\n        optimized_model.eval()\n        \n        # Create test input\n        batch_size, seq_len = 2, 32\n        input_ids = torch.randint(0, self.config.vocab_size, (batch_size, seq_len))\n        \n        # Run both models\n        with torch.no_grad():\n            baseline_output = baseline_model(input_ids)\n            optimized_output = optimized_model(input_ids)\n        \n        # Check if outputs are close (allowing for small numerical differences)\n        similarity = torch.cosine_similarity(\n            baseline_output.flatten(),\n            optimized_output.flatten(),\n            dim=0\n        ).mean()\n        \n        # Calculate MSE\n        mse = F.mse_loss(baseline_output, optimized_output)\n        \n        # Accuracy is preserved if outputs are highly similar\n        accuracy_preserved = similarity > 0.95 and mse < 0.01\n        \n        print(f"  Output similarity: {similarity:.6f}")\n        print(f"  MSE: {mse:.6f}")\n        print(f"  Accuracy preserved: {'✓' if accuracy_preserved else '✗'}")\n        \n        self.performance_metrics['accuracy_preserved'] = accuracy_preserved\n        return accuracy_preserved\n    \n    def test_resource_coordination(self) -> bool:\n        """Test that resources are properly coordinated between optimizations."""\n        print("Testing resource coordination...")\n        \n        # Check resource manager status\n        resource_summary = self.resource_manager.get_optimization_resource_summary()\n        \n        # Verify that resource allocation is happening\n        active_allocations = len(resource_summary['optimization_resource_usage'])\n        total_optimizations = len(self.optimization_manager.get_active_optimizations())\n        \n        print(f"  Active resource allocations: {active_allocations}/{total_optimizations}")\n        \n        # Check that memory pools are properly managed\n        gpu_memory_status = resource_summary['resource_status']['gpu_memory']\n        cpu_memory_status = resource_summary['resource_status']['cpu_memory']\n        \n        print(f"  GPU memory utilization: {gpu_memory_status['utilization']:.2%}")\n        print(f"  CPU memory utilization: {cpu_memory_status['utilization']:.2%}")\n        \n        # Test resource allocation for a specific optimization\n        success, allocation = self.resource_manager.allocate_resources_for_optimization(\n            'block_sparse_attention',\n            (batch_size, seq_len, self.config.hidden_size)\n        )\n        \n        print(f"  Resource allocation test: {'✓' if success else '✗'}")\n        \n        if success:\n            self.resource_manager.release_resources_for_optimization('block_sparse_attention')\n        \n        return active_allocations > 0 and success\n    \n    def test_fallback_mechanisms(self) -> bool:\n        """Test that fallback mechanisms work when optimizations fail."""\n        print("Testing fallback mechanisms...")\n        \n        try:\n            # Force a situation where an optimization might fail\n            # Try to allocate resources that exceed limits\n            large_shape = (1, 1024, 8192)  # Very large tensor\n            \n            # This should trigger fallback mechanisms\n            success, allocation = self.resource_manager.allocate_resources_for_optimization(\n                'block_sparse_attention',\n                large_shape\n            )\n            \n            # Even if primary allocation fails, fallback should succeed\n            fallback_success = True\n            \n            if success:\n                self.resource_manager.release_resources_for_optimization('block_sparse_attention')\n            \n            print(f"  Fallback mechanism triggered: {'✓' if fallback_success else '✗'}")\n            \n        except Exception as e:\n            print(f"  Error in fallback testing: {e}")\n            fallback_success = False\n        \n        return fallback_success\n    \n    def test_optimization_combination_effectiveness(self) -> Dict[str, Any]:\n        """Test effectiveness of different optimization combinations."""\n        print("Testing optimization combination effectiveness...")\n        \n        # Create test model and input\n        model = self.create_test_model(optimization_enabled=True)\n        batch_size, seq_len = 1, 16\n        test_input = torch.randn(batch_size, seq_len, self.config.hidden_size)\n        \n        # Test different combinations of optimizations\n        all_optimizations = self.optimization_manager.get_active_optimizations()\n        \n        # Test single optimizations\n        single_opt_results = {}\n        for opt in all_optimizations[:5]:  # Test first 5 optimizations\n            try:\n                start_time = time.time()\n                result = self.interaction_handler.apply_single_optimization(test_input, opt.value, layer_idx=0)\n                end_time = time.time()\n                single_opt_results[opt.value] = {\n                    'time': end_time - start_time,\n                    'success': True,\n                    'output_shape': result.shape if hasattr(result, 'shape') else 'N/A'\n                }\n            except Exception as e:\n                single_opt_results[opt.value] = {\n                    'time': 0,\n                    'success': False,\n                    'error': str(e)\n                }\n        \n        # Test optimization pairs\n        pair_opt_results = {}\n        for i, opt1 in enumerate(all_optimizations[:3]):  # Test first 3 optimizations\n            for opt2 in all_optimizations[i+1:4]:  # Test with next 3 optimizations\n                try:\n                    combo = [opt1, opt2]\n                    start_time = time.time()\n                    result = self.interaction_handler.apply_optimizations_with_interaction_handling(\n                        test_input, \n                        layer_idx=0, \n                        optimization_combo=combo\n                    )\n                    end_time = time.time()\n                    combo_name = f"{opt1.value}_{opt2.value}"\n                    pair_opt_results[combo_name] = {\n                        'time': end_time - start_time,\n                        'success': True,\n                        'output_shape': result.shape if hasattr(result, 'shape') else 'N/A'\n                    }\n                except Exception as e:\n                    combo_name = f"{opt1.value}_{opt2.value}"\n                    pair_opt_results[combo_name] = {\n                        'time': 0,\n                        'success': False,\n                        'error': str(e)\n                    }\n        \n        # Test all optimizations together\n        try:\n            start_time = time.time()\n            result = self.interaction_handler.apply_optimizations_with_interaction_handling(\n                test_input,\n                layer_idx=0,\n                optimization_combo=all_optimizations\n            )\n            end_time = time.time()\n            all_opt_result = {\n                'time': end_time - start_time,\n                'success': True,\n                'output_shape': result.shape if hasattr(result, 'shape') else 'N/A'\n            }\n        except Exception as e:\n            all_opt_result = {\n                'time': 0,\n                'success': False,\n                'error': str(e)\n            }\n        \n        # Calculate synergy score\n        single_times = [v['time'] for v in single_opt_results.values() if v['success']]\n        all_time = all_opt_result['time'] if all_opt_result['success'] else float('inf')\n        \n        if single_times and all_time > 0:\n            avg_single_time = sum(single_times) / len(single_times)\n            synergy_score = avg_single_time / all_time if all_time > 0 else float('inf')\n        else:\n            synergy_score = 1.0  # No improvement if no valid measurements\n        \n        print(f"  Tested {len(single_opt_results)} single optimizations")\n        print(f"  Tested {len(pair_opt_results)} optimization pairs")\n        print(f"  Tested all optimizations together: {'✓' if all_opt_result['success'] else '✗'}")\n        print(f"  Synergy score: {synergy_score:.2f}")\n        \n        return {\n            'single_optimization_results': single_opt_results,\n            'pair_optimization_results': pair_opt_results,\n            'all_optimizations_result': all_opt_result,\n            'synergy_score': synergy_score,\n            'combination_effectiveness': len([v for v in single_opt_results.values() if v['success']]) > 0\n        }\n    \n    def run_all_tests(self) -> Dict[str, Any]:\n        """Run all comprehensive tests for the unified optimization architecture."""\n        print("=" * 60)\n        print("COMPREHENSIVE UNIFIED OPTIMIZATION ARCHITECTURE TESTS")\n        print("=" * 60)\n        \n        # Run all tests\n        capacity_preserved = self.test_capacity_preservation()\n        interaction_synergy = self.test_optimization_interaction_synergy()\n        performance_results = self.test_performance_improvement()\n        accuracy_preserved = self.test_accuracy_preservation()\n        resource_coordinated = self.test_resource_coordination()\n        fallback_works = self.test_fallback_mechanisms()\n        combination_results = self.test_optimization_combination_effectiveness()\n        \n        # Overall result\n        all_tests_passed = (\n            capacity_preserved and \n            interaction_synergy and \n            performance_results['performance_improved'] and \n            accuracy_preserved and \n            resource_coordinated and \n            fallback_works and \n            combination_results['combination_effectiveness']\n        )\n        \n        print("\n" + "=" * 60)\n        print("UNIFIED OPTIMIZATION ARCHITECTURE TEST RESULTS")\n        print("=" * 60)\n        print(f"  Capacity preservation (32 layers, 32 heads): {'✓ PASS' if capacity_preserved else '✗ FAIL'}")\n        print(f"  Optimization interaction synergy: {'✓ PASS' if interaction_synergy else '✗ FAIL'}")\n        print(f"  Performance improvement: {'✓ PASS' if performance_results['performance_improved'] else '✗ FAIL'}")\n        print(f"  Accuracy preservation: {'✓ PASS' if accuracy_preserved else '✗ FAIL'}")\n        print(f"  Resource coordination: {'✓ PASS' if resource_coordinated else '✗ FAIL'}")\n        print(f"  Fallback mechanisms: {'✓ PASS' if fallback_works else '✗ FAIL'}")\n        print(f"  Combination effectiveness: {'✓ PASS' if combination_results['combination_effectiveness'] else '✗ FAIL'}")\n        print(f"  Overall result: {'✓ ALL TESTS PASSED' if all_tests_passed else '✗ SOME TESTS FAILED'}")\n        \n        return {\n            'capacity_preserved': capacity_preserved,\n            'interaction_synergy': interaction_synergy,\n            'performance_results': performance_results,\n            'accuracy_preserved': accuracy_preserved,\n            'resource_coordinated': resource_coordinated,\n            'fallback_works': fallback_works,\n            'combination_results': combination_results,\n            'all_tests_passed': all_tests_passed,\n            'performance_metrics': self.performance_metrics\n        }\n\n\ndef run_comprehensive_unified_tests():\n    """Run comprehensive tests for the unified optimization architecture."""\n    config = TestConfig()\n    tester = OptimizationTester(config)\n    \n    results = tester.run_all_tests()\n    \n    # Save results to temporary file\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:\n        import json\n        json.dump(results, f, indent=2)\n        results_file = f.name\n    \n    print(f"\nTest results saved to: {results_file}")\n    \n    return results\n\n\nif __name__ == "__main__":\n    # Run comprehensive tests\n    test_results = run_comprehensive_unified_tests()\n    \n    # Print summary\n    print("\nSUMMARY:")\n    print(f"  All tests passed: {test_results['all_tests_passed']}")\n    print(f"  Capacity preserved: {test_results['capacity_preserved']}")\n    print(f"  Performance improved: {test_results['performance_results']['performance_improved']}")\n    print(f"  Accuracy preserved: {test_results['accuracy_preserved']}")\n    \n    print("\nUNIFIED OPTIMIZATION ARCHITECTURE VALIDATION COMPLETE!")