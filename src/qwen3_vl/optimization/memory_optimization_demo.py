"""\nQwen3-VL Memory Optimization System - Demonstration Script\n\nThis script demonstrates how to integrate all memory optimization systems\ninto a Qwen3-VL model and shows how to activate the optimizations.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Tuple, Optional\nimport time\nimport logging\n\nfrom optimization.memory_optimization_integrator import MemoryOptimizationIntegrator, create_memory_optimizer, allocate_model_tensor, access_model_tensor, deallocate_model_tensor, OptimizationConfig, OptimizationLevel, TensorType\n    MemoryOptimizationIntegrator,\n    create_memory_optimizer,\n    allocate_model_tensor,\n    access_model_tensor,\n    deallocate_model_tensor,\n    OptimizationConfig,\n    OptimizationLevel,\n    TensorType\n)\nfrom optimization.memory_optimized_model import create_optimized_qwen3_vl_model\nfrom config import Qwen3VLConfig\n\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef create_optimized_model_demo():\n    """\n    Demonstrate creating an optimized Qwen3-VL model with all memory optimizations.\n    """\n    print("Qwen3-VL Memory Optimization System - Demonstration")\n    print("=" * 60)\n\n    print("\n1. Creating optimized Qwen3-VL model with memory optimizations...")\n\n    # Create base configuration\n    config = Qwen3VLConfig()\n    config.hidden_size = 512  # Smaller for demo\n    config.num_attention_heads = 8\n    config.num_hidden_layers = 4\n    config.vocab_size = 1000\n\n    # Create optimized model with balanced optimization level\n    model, memory_optimizer = create_optimized_qwen3_vl_model(\n        config,\n        optimization_level=OptimizationLevel.BALANCED\n    )\n\n    print(f"   OK Created model with {config.num_hidden_layers} layers and {config.num_attention_heads} attention heads")\n    print(f"   OK Applied {memory_optimizer.config.optimization_level.value} optimization level")\n    print(f"   OK Enabled systems: "\n          f"Pooling={memory_optimizer.config.enable_memory_pooling}, "\n          f"Caching={memory_optimizer.config.enable_hierarchical_caching}, "\n          f"Compression={memory_optimizer.config.enable_compression}, "\n          f"Swapping={memory_optimizer.config.enable_swapping}, "\n          f"Tiering={memory_optimizer.config.enable_tiering}")\n\n    return model, memory_optimizer, config\n\n\ndef demonstrate_tensor_management_demo(model: nn.Module, memory_optimizer: MemoryOptimizationIntegrator):\n    """\n    Demonstrate tensor management with memory optimizations.\n    """\n    print("\n2. Demonstrating tensor management with memory optimizations...")\n\n    # Example 1: Allocate KV cache tensor (frequently accessed)\n    print("\n   a) Allocating KV cache tensor...")\n    kv_tensor, kv_tensor_id = allocate_model_tensor(\n        memory_optimizer,\n        shape=(4, 128, 512),  # batch, seq, hidden_dim\n        dtype=torch.float16,\n        tensor_type="kv_cache"\n    )\n    print(f"      OK Allocated KV cache tensor with ID: {kv_tensor_id[:15]}...")\n\n    # Access the tensor multiple times (simulating attention mechanism)\n    for i in range(3):\n        retrieved_tensor = access_model_tensor(memory_optimizer, kv_tensor_id)\n        print(f"      OK Access {i+1}: Tensor retrieved successfully")\n\n    # Example 2: Allocate image features tensor\n    print("\n   b) Allocating image features tensor...")\n    img_tensor, img_tensor_id = allocate_model_tensor(\n        memory_optimizer,\n        shape=(1, 576, 512),  # batch, patches, hidden_dim\n        dtype=torch.float16,\n        tensor_type="image_features"\n    )\n    print(f"      OK Allocated image features tensor with ID: {img_tensor_id[:15]}...")\n\n    # Access the tensor\n    retrieved_img_tensor = access_model_tensor(memory_optimizer, img_tensor_id)\n    print(f"      OK Image features tensor retrieved successfully")\n\n    # Example 3: Allocate temporary tensor\n    print("\n   c) Allocating temporary tensor...")\n    temp_tensor, temp_tensor_id = allocate_model_tensor(\n        memory_optimizer,\n        shape=(10, 10),\n        dtype=torch.float16,\n        tensor_type="temporary"\n    )\n    print(f"      OK Allocated temporary tensor with ID: {temp_tensor_id[:15]}...")\n\n    # Access the tensor\n    retrieved_temp_tensor = access_model_tensor(memory_optimizer, temp_tensor_id)\n    print(f"      OK Temporary tensor retrieved successfully")\n\n    # Clean up tensors\n    print("\n   d) Cleaning up tensors...")\n    deallocate_model_tensor(memory_optimizer, kv_tensor_id)\n    deallocate_model_tensor(memory_optimizer, img_tensor_id)\n    deallocate_model_tensor(memory_optimizer, temp_tensor_id)\n    print("      OK All tensors cleaned up")\n\n\ndef demonstrate_configuration_options_demo():\n    """\n    Demonstrate different configuration options for memory optimizations.\n    """\n    print("\n3. Demonstrating configuration options...")\n\n    # Option 1: Minimal optimizations (for systems with plenty of memory)\n    print("\n   a) Minimal optimizations configuration:")\n    minimal_config = OptimizationConfig(\n        optimization_level=OptimizationLevel.MINIMAL,\n        enable_memory_pooling=False,\n        enable_hierarchical_caching=False,\n        enable_compression=False,\n        enable_swapping=False,\n        enable_tiering=False,\n        enable_ml_prediction=False\n    )\n    minimal_optimizer = MemoryOptimizationIntegrator(minimal_config)\n    print(f"      OK Minimal config: All optimizations disabled")\n\n    # Option 2: Aggressive optimizations (for memory-constrained systems)\n    print("\n   b) Aggressive optimizations configuration:")\n    aggressive_config = OptimizationConfig(\n        optimization_level=OptimizationLevel.AGGRESSIVE,\n        enable_memory_pooling=True,\n        enable_hierarchical_caching=True,\n        enable_compression=True,\n        enable_swapping=True,\n        enable_tiering=True,\n        enable_ml_prediction=True,\n        compression_threshold=0.05,  # Lower threshold for more compression\n        swap_threshold=0.6,  # Start swapping earlier\n        gc_frequency=50  # More frequent garbage collection\n    )\n    aggressive_optimizer = MemoryOptimizationIntegrator(aggressive_config)\n    print(f"      OK Aggressive config: All optimizations enabled with aggressive settings")\n\n    # Option 3: Custom configuration (fine-tuned for specific needs)\n    print("\n   c) Custom configuration:")\n    custom_config = OptimizationConfig(\n        optimization_level=OptimizationLevel.BALANCED,\n        enable_memory_pooling=True,\n        enable_hierarchical_caching=True,\n        enable_compression=True,\n        enable_swapping=False,  # Disable swapping for this example\n        enable_tiering=True,\n        enable_ml_prediction=True,\n        cache_l1_size=64 * 1024 * 1024,   # 64MB L1 cache\n        cache_l2_size=256 * 1024 * 1024,  # 256MB L2 cache\n        cache_l3_size=512 * 1024 * 1024,  # 512MB L3 cache\n        gpu_hbm_size=1 * 1024 * 1024 * 1024,  # 1GB GPU HBM\n        cpu_ram_size=2 * 1024 * 1024 * 1024,  # 2GB CPU RAM\n        nvme_ssd_size=10 * 1024 * 1024 * 1024  # 10GB NVMe SSD\n    )\n    custom_optimizer = MemoryOptimizationIntegrator(custom_config)\n    print(f"      OK Custom config: Tiered cache with specific sizes")\n\n    # Clean up\n    minimal_optimizer.cleanup()\n    aggressive_optimizer.cleanup()\n    custom_optimizer.cleanup()\n\n    print("      OK All configuration examples demonstrated")\n\n\ndef demonstrate_model_inference_demo():\n    """\n    Demonstrate model inference with memory optimizations.\n    """\n    print("\n4. Demonstrating model inference with memory optimizations...")\n\n    # Create model with optimizations\n    config = Qwen3VLConfig()\n    config.hidden_size = 256\n    config.num_attention_heads = 4\n    config.num_hidden_layers = 2\n    config.vocab_size = 1000\n\n    model, memory_optimizer = create_optimized_qwen3_vl_model(\n        config,\n        optimization_level=OptimizationLevel.BALANCED\n    )\n\n    # Prepare sample inputs\n    batch_size, seq_len = 2, 10\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n\n    print(f"   Input shape: {input_ids.shape}")\n\n    # Run inference\n    print("   Running inference...")\n    start_time = time.time()\n\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids)\n\n    inference_time = time.time() - start_time\n\n    print(f"   OK Inference completed in {inference_time:.4f}s")\n    print(f"   Output shape: {outputs.shape}")\n\n    # Show memory optimization statistics\n    stats = memory_optimizer.get_optimization_stats()\n    print(f"   OK Memory stats: {stats.allocations} allocations, "\n          f"{stats.deallocations} deallocations, "\n          f"{stats.compressions} compressions")\n\n    # Clean up\n    memory_optimizer.cleanup()\n\n\ndef demonstrate_advanced_features_demo():\n    """\n    Demonstrate advanced features of the memory optimization system.\n    """\n    print("\n5. Demonstrating advanced features...")\n\n    # Create optimizer with all features enabled\n    config = OptimizationConfig(\n        optimization_level=OptimizationLevel.MAXIMUM,\n        enable_memory_pooling=True,\n        enable_hierarchical_caching=True,\n        enable_compression=True,\n        enable_swapping=True,\n        enable_tiering=True,\n        enable_ml_prediction=True,\n        enable_advanced_gc=True\n    )\n    memory_optimizer = create_memory_optimizer(config)\n\n    # Feature 1: ML-based access prediction\n    print("\n   a) ML-based access prediction:")\n    # Allocate tensors and access them to build access patterns\n    for i in range(5):\n        tensor, tensor_id = allocate_model_tensor(\n            memory_optimizer,\n            shape=(50, 50),\n            dtype=torch.float16,\n            tensor_type="general"\n        )\n\n        # Access the tensor multiple times to establish a pattern\n        for _ in range(3):\n            access_model_tensor(memory_optimizer, tensor_id)\n\n        # Deallocate after establishing pattern\n        deallocate_model_tensor(memory_optimizer, tensor_id)\n\n    print("      OK Access patterns established for ML prediction")\n\n    # Feature 2: Tiering with automatic migration\n    print("\n   b) Tiering with automatic migration:")\n    large_tensor, large_tensor_id = allocate_model_tensor(\n        memory_optimizer,\n        shape=(200, 200),\n        dtype=torch.float16,\n        tensor_type="general"\n    )\n\n    # Access tensor to trigger potential migration\n    retrieved_tensor = access_model_tensor(memory_optimizer, large_tensor_id)\n    print(f"      OK Large tensor accessed and managed by tiering system")\n\n    # Feature 3: Compression with threshold\n    print("\n   c) Adaptive compression:")\n    # Create a tensor that might benefit from compression\n    compressible_tensor = torch.randn(100, 100, dtype=torch.float16)\n    # Add some zero values to make it more compressible\n    compressible_tensor[compressible_tensor.abs() < 0.1] = 0\n\n    compressed_tensor, compressed_tensor_id = allocate_model_tensor(\n        memory_optimizer,\n        shape=(100, 100),\n        dtype=torch.float16,\n        tensor_type="general"\n    )\n\n    # Copy values to the allocated tensor\n    with torch.no_grad():\n        compressed_tensor.copy_(compressible_tensor)\n\n    print("      OK Tensor allocated with potential compression consideration")\n\n    # Clean up\n    deallocate_model_tensor(memory_optimizer, large_tensor_id)\n    deallocate_model_tensor(memory_optimizer, compressed_tensor_id)\n    memory_optimizer.cleanup()\n\n    print("      OK Advanced features demonstrated")\n\n\ndef main_demo():\n    """\n    Run the complete demonstration.\n    """\n    print("Qwen3-VL Advanced Memory Optimization System - Complete Demonstration")\n    print("=" * 80)\n\n    # 1. Create optimized model\n    model, memory_optimizer, config = create_optimized_model_demo()\n\n    # 2. Demonstrate tensor management\n    demonstrate_tensor_management_demo(model, memory_optimizer)\n\n    # 3. Demonstrate configuration options\n    demonstrate_configuration_options_demo()\n\n    # 4. Demonstrate model inference\n    demonstrate_model_inference_demo()\n\n    # 5. Demonstrate advanced features\n    demonstrate_advanced_features_demo()\n\n    print("\n" + "=" * 80)\n    print("DEMONSTRATION COMPLETE!")\n    print("All memory optimization systems have been successfully demonstrated.")\n    print("The system integrates memory pooling, hierarchical caching, compression,")\n    print("swapping, tiering with ML prediction, and advanced garbage collection.")\n    print("=" * 80)\n\n\nif __name__ == "__main__":\n    main_demo()