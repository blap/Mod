"""\nHardware-specific optimizations for Intel i5-10210U + NVIDIA SM61 + NVMe SSD.\n"""\n\nimport torch\nimport psutil\nimport threading\nimport time\nfrom typing import Dict, Any, Optional\nfrom dataclasses import dataclass\nfrom optimization.hierarchical_cache_manager import CacheConfig\n\n\n@dataclass\nclass HardwareSpecs:\n    """Hardware specifications for Intel i5-10210U + NVIDIA SM61 + NVMe SSD."""\n    # Intel i5-10210U specifications\n    cpu_cores: int = 4\n    cpu_threads: int = 8\n    cpu_l1_cache_per_core: int = 32 * 1024  # 32KB\n    cpu_l2_cache_per_core: int = 256 * 1024  # 256KB\n    cpu_l3_cache_total: int = 6 * 1024 * 1024  # 6MB\n    cpu_memory_bandwidth_gb_s: float = 42.7  # Approximate\n    \n    # NVIDIA SM61 specifications (approximated from similar architectures)\n    gpu_memory: int = 4 * 1024 * 1024 * 1024  # 4GB (common for SM61 class)\n    gpu_memory_bandwidth_gb_s: float = 128.0  # Approximate for SM61 class\n    gpu_compute_capability: str = "6.1"\n    \n    # NVMe SSD specifications (typical high-performance NVMe)\n    ssd_read_speed_mb_s: int = 3500  # Sequential read\n    ssd_write_speed_mb_s: int = 3000  # Sequential write\n    ssd_random_read_iops: int = 600000  # Random read IOPS\n    ssd_random_write_iops: int = 500000  # Random write IOPS\n\n\nclass HardwareOptimizer:\n    """\n    Optimizer for Intel i5-10210U + NVIDIA SM61 + NVMe SSD hardware configuration.\n    """\n    \n    def __init__(self):\n        self.hardware_specs = HardwareSpecs()\n        self.system_memory = psutil.virtual_memory().total\n        self.available_memory = psutil.virtual_memory().available\n        self.cpu_count = psutil.cpu_count(logical=True)\n        \n        # Calculate optimal cache sizes based on hardware\n        self.optimal_cache_config = self._calculate_optimal_cache_config()\n    \n    def _calculate_optimal_cache_config(self) -> CacheConfig:\n        """\n        Calculate optimal cache configuration based on hardware specifications.\n        """\n        # Calculate cache sizes as percentages of available resources\n        # L1 (GPU): Use up to 25% of GPU memory or 1GB, whichever is smaller\n        l1_size = min(\n            int(self.hardware_specs.gpu_memory * 0.25),\n            1024 * 1024 * 1024  # 1GB cap\n        )\n        \n        # L2 (CPU): Use up to 20% of system RAM or 2GB, whichever is smaller\n        l2_size = min(\n            int(self.available_memory * 0.20),\n            2 * 1024 * 1024 * 1024  # 2GB cap\n        )\n        \n        # L3 (SSD): Use up to 50% of available disk space or 10GB, whichever is smaller\n        # For this implementation, we'll use a reasonable default\n        l3_size = min(\n            int(self.available_memory * 0.50),  # Use 50% of available RAM as SSD cache\n            10 * 1024 * 1024 * 1024  # 10GB cap\n        )\n        \n        # Calculate migration thresholds based on hardware capabilities\n        # For i5-10210U with 4 cores, adjust thresholds to prevent excessive migrations\n        high_freq_threshold = max(3, self.hardware_specs.cpu_cores // 2)\n        medium_freq_threshold = max(1, self.hardware_specs.cpu_cores // 4)\n        \n        # Time thresholds based on SSD performance\n        # For high-performance NVMe, we can afford more aggressive caching\n        time_threshold = 180.0  # 3 minutes for L3 migration\n        \n        return CacheConfig(\n            l1_cache_size=l1_size,\n            l2_cache_size=l2_size,\n            l3_cache_size=l3_size,\n            l1_eviction_policy="lru",\n            l2_eviction_policy="lru", \n            l3_eviction_policy="lru",\n            l2_pin_memory=True,  # Use pinned memory for faster GPU transfers on i5-10210U\n            l3_compression=True,  # Enable compression for SSD efficiency\n            access_pattern_window=800,  # Adjust based on expected access patterns\n            migration_threshold_high_freq=high_freq_threshold,\n            migration_threshold_medium_freq=medium_freq_threshold,\n            migration_time_threshold=time_threshold,\n            prediction_window=150,  # Larger window for more accurate predictions\n            prediction_threshold=0.65  # Adjust threshold based on confidence\n        )\n    \n    def optimize_for_tensor_characteristics(self, \n                                          tensor_shape: tuple, \n                                          tensor_type: str) -> Dict[str, Any]:\n        """\n        Provide optimization suggestions based on tensor characteristics.\n        \n        Args:\n            tensor_shape: Shape of the tensor\n            tensor_type: Type of tensor ('attention', 'kv_cache', 'image_embeddings', etc.)\n            \n        Returns:\n            Dictionary with optimization suggestions\n        """\n        total_elements = 1\n        for dim in tensor_shape:\n            total_elements *= dim\n        \n        element_size = torch.tensor([], dtype=torch.float16).element_size()\n        tensor_size_bytes = total_elements * element_size\n        \n        optimizations = {\n            'preferred_cache_level': 1,  # Default to L1\n            'alignment_needed': False,\n            'batch_optimization': False,\n            'memory_layout': 'default'\n        }\n        \n        # Determine optimal cache level based on tensor size and type\n        if tensor_type == 'attention':\n            # Attention tensors are frequently accessed, prefer L1 if small enough\n            if tensor_size_bytes < self.optimal_cache_config.l1_cache_size * 0.1:  # < 10% of L1\n                optimizations['preferred_cache_level'] = 1\n            elif tensor_size_bytes < self.optimal_cache_config.l2_cache_size * 0.1:  # < 10% of L2\n                optimizations['preferred_cache_level'] = 2\n            else:\n                optimizations['preferred_cache_level'] = 3  # Large attention tensors to L3\n                \n            # Attention tensors benefit from specific alignments\n            if len(tensor_shape) >= 2:\n                optimizations['alignment_needed'] = True\n                optimizations['memory_layout'] = 'row_major'\n                \n        elif tensor_type == 'kv_cache':\n            # KV cache tensors are accessed sequentially, optimize for L2\n            if tensor_size_bytes < self.optimal_cache_config.l2_cache_size * 0.2:  # < 20% of L2\n                optimizations['preferred_cache_level'] = 2\n            else:\n                optimizations['preferred_cache_level'] = 3  # Large KV caches to L3\n                \n            optimizations['memory_layout'] = 'channel_last'  # Optimize for sequential access\n                \n        elif tensor_type == 'image_embeddings':\n            # Image embeddings often have consistent access patterns, L2 is good\n            if tensor_size_bytes < self.optimal_cache_config.l2_cache_size * 0.15:  # < 15% of L2\n                optimizations['preferred_cache_level'] = 2\n            else:\n                optimizations['preferred_cache_level'] = 3\n                \n        elif tensor_type == 'text_embeddings':\n            # Text embeddings vary in size, use adaptive approach\n            if tensor_size_bytes < self.optimal_cache_config.l1_cache_size * 0.2:  # < 20% of L1\n                optimizations['preferred_cache_level'] = 1\n            elif tensor_size_bytes < self.optimal_cache_config.l2_cache_size * 0.1:  # < 10% of L2\n                optimizations['preferred_cache_level'] = 2\n            else:\n                optimizations['preferred_cache_level'] = 3\n                \n        # For large tensors, consider batching optimizations\n        if tensor_size_bytes > 10 * 1024 * 1024:  # > 10MB\n            optimizations['batch_optimization'] = True\n            \n        return optimizations\n    \n    def get_memory_bandwidth_optimizations(self) -> Dict[str, float]:\n        """\n        Get memory bandwidth optimization parameters based on hardware.\n        """\n        return {\n            'cpu_to_gpu_bandwidth_ratio': (\n                self.hardware_specs.gpu_memory_bandwidth_gb_s / \n                self.hardware_specs.cpu_memory_bandwidth_gb_s\n            ),\n            'optimal_transfer_size_mb': 16.0,  # Optimal for pinned memory transfers\n            'gpu_cache_fill_rate_mb_s': self.hardware_specs.gpu_memory_bandwidth_gb_s / 4,  # Conservative estimate\n            'cpu_cache_fill_rate_mb_s': self.hardware_specs.cpu_memory_bandwidth_gb_s / 4  # Conservative estimate\n        }\n    \n    def get_threading_optimizations(self) -> Dict[str, int]:\n        """\n        Get threading optimization parameters based on CPU capabilities.\n        """\n        # For i5-10210U with 4 cores and 8 threads\n        # Limit concurrent cache operations to avoid thread contention\n        return {\n            'max_cache_threads': min(4, self.cpu_count // 2),  # Use up to half the threads\n            'prefetch_worker_threads': 2,  # Dedicated threads for prefetching\n            'migration_worker_threads': 1,  # Dedicated thread for migrations\n            'io_worker_threads': 2  # Threads for SSD I/O operations\n        }\n    \n    def optimize_cache_config_for_hardware(self, base_config: CacheConfig = None) -> CacheConfig:\n        """\n        Optimize a base cache configuration for the specific hardware.\n        \n        Args:\n            base_config: Base configuration to optimize, or None for defaults\n            \n        Returns:\n            Optimized cache configuration\n        """\n        if base_config is None:\n            base_config = CacheConfig()\n        \n        # Apply hardware-specific optimizations\n        optimized_config = CacheConfig(\n            # Use calculated optimal sizes\n            l1_cache_size=self.optimal_cache_config.l1_cache_size,\n            l2_cache_size=self.optimal_cache_config.l2_cache_size,\n            l3_cache_size=self.optimal_cache_config.l3_cache_size,\n            \n            # Use hardware-optimized policies\n            l1_eviction_policy=base_config.l1_eviction_policy,\n            l2_eviction_policy=base_config.l2_eviction_policy,\n            l3_eviction_policy=base_config.l3_eviction_policy,\n            \n            # Enable pinned memory for i5-10210U + NVIDIA setup\n            l2_pin_memory=True,\n            \n            # Enable compression for efficient SSD usage\n            l3_compression=True,\n            \n            # Adjust tracking windows based on hardware capabilities\n            access_pattern_window=min(base_config.access_pattern_window, 1000),\n            migration_threshold_high_freq=self.optimal_cache_config.migration_threshold_high_freq,\n            migration_threshold_medium_freq=self.optimal_cache_config.migration_threshold_medium_freq,\n            migration_time_threshold=self.optimal_cache_config.migration_time_threshold,\n            prediction_window=min(base_config.prediction_window, 200),\n            prediction_threshold=base_config.prediction_threshold\n        )\n        \n        return optimized_config\n\n\nclass IntelNvidiaCacheManager:\n    """\n    Cache manager optimized specifically for Intel i5-10210U + NVIDIA SM61 + NVMe SSD.\n    """\n    \n    def __init__(self, base_config: CacheConfig = None):\n        self.hardware_optimizer = HardwareOptimizer()\n        \n        # Get optimized configuration\n        self.optimized_config = self.hardware_optimizer.optimize_cache_config_for_hardware(base_config)\n        \n        # Initialize cache manager with optimized config\nfrom optimization.hierarchical_cache_manager import HierarchicalCacheManager\n        self.cache_manager = HierarchicalCacheManager(self.optimized_config)\n        \n        # Apply hardware-specific optimizations\n        self._apply_hardware_optimizations()\n    \n    def _apply_hardware_optimizations(self):\n        """\n        Apply hardware-specific optimizations to the cache manager.\n        """\n        # Set up threading based on CPU capabilities\n        threading_params = self.hardware_optimizer.get_threading_optimizations()\n        \n        # Note: In a real implementation, we would configure internal threading\n        # based on these parameters\n        \n        # Pre-allocate pinned memory buffers for efficient GPU transfers\n        self._setup_pinned_memory_buffers()\n    \n    def _setup_pinned_memory_buffers(self):\n        """\n        Set up pinned memory buffers for efficient CPU-GPU transfers.\n        """\n        # This is a placeholder - in a real implementation, we would pre-allocate\n        # pinned memory buffers for common tensor sizes\n        pass\n    \n    def get_tensor(self, \n                   shape: tuple, \n                   dtype: torch.dtype = torch.float16,\n                   tensor_type: str = "general",\n                   device: torch.device = None) -> torch.Tensor:\n        """\n        Get tensor with hardware-optimized caching.\n        """\n        # Get optimization suggestions for this tensor\n        optimizations = self.hardware_optimizer.optimize_for_tensor_characteristics(\n            shape, tensor_type\n        )\n        \n        # Determine if we should use a specific cache level\n        preferred_level = optimizations['preferred_cache_level']\n        \n        # First, try to get from hierarchical cache\n        tensor_key = f"{shape}_{str(dtype)}_{tensor_type}_{str(device)}"\n        tensor = self.cache_manager.get_tensor(tensor_key)\n        cache_level = None  # This cache manager doesn't return cache level\n        \n        if tensor is not None:\n            return tensor\n        \n        # If not in cache, create a new tensor\n        device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        new_tensor = torch.empty(shape, dtype=dtype, device=device)\n        new_tensor.zero_()\n        \n        # Put in cache with hardware-optimized settings\n        self.cache_manager.put_tensor(new_tensor, tensor_type, preferred_level=preferred_level)\n        \n        return new_tensor\n    \n    def get_optimized_tensor_config(self, shape: tuple, tensor_type: str) -> Dict[str, Any]:\n        """\n        Get optimized configuration for a specific tensor.\n        """\n        return self.hardware_optimizer.optimize_for_tensor_characteristics(shape, tensor_type)\n    \n    def get_hardware_optimization_report(self) -> Dict[str, Any]:\n        """\n        Get a report on hardware optimizations applied.\n        """\n        return {\n            'hardware_specs': self.hardware_optimizer.hardware_specs,\n            'calculated_cache_config': self.optimized_config,\n            'memory_bandwidth_ratios': self.hardware_optimizer.get_memory_bandwidth_optimizations(),\n            'threading_config': self.hardware_optimizer.get_threading_optimizations(),\n            'system_memory_info': {\n                'total_memory': self.hardware_optimizer.system_memory,\n                'available_memory': self.hardware_optimizer.available_memory,\n                'cpu_count': self.hardware_optimizer.cpu_count\n            }\n        }\n    \n    def get_stats(self) -> Dict[str, Any]:\n        """Get cache statistics."""\n        return self.cache_manager.get_stats()\n\n\ndef create_hardware_optimized_cache_manager(base_config: CacheConfig = None) -> IntelNvidiaCacheManager:\n    """\n    Factory function to create a hardware-optimized cache manager.\n    """\n    return IntelNvidiaCacheManager(base_config)\n\n\n# Example usage and optimization test\nif __name__ == "__main__":\n    print("Testing Hardware-Specific Optimizations...")\n    \n    # Create optimizer\n    optimizer = HardwareOptimizer()\n    \n    print(f"\nDetected Hardware:")\n    print(f"  CPU: Intel i5-10210U (4 cores, 8 threads)")\n    print(f"  GPU: NVIDIA SM61 (~4GB memory)")\n    print(f"  SSD: High-performance NVMe")\n    print(f"  System RAM: {optimizer.system_memory / (1024**3):.1f}GB")\n    print(f"  Available RAM: {optimizer.available_memory / (1024**3):.1f}GB")\n    \n    # Show calculated optimal cache configuration\n    optimal_config = optimizer.optimal_cache_config\n    print(f"\nOptimal Cache Configuration:")\n    print(f"  L1 (GPU): {optimal_config.l1_cache_size / (1024**2):.0f}MB")\n    print(f"  L2 (CPU): {optimal_config.l2_cache_size / (1024**2):.0f}MB")\n    print(f"  L3 (SSD): {optimal_config.l3_cache_size / (1024**3):.1f}GB")\n    print(f"  High frequency threshold: {optimal_config.migration_threshold_high_freq}")\n    print(f"  Medium frequency threshold: {optimal_config.migration_threshold_medium_freq}")\n    \n    # Test tensor optimization suggestions\n    print(f"\nTensor Optimization Suggestions:")\n    \n    # Attention tensor\n    attn_opts = optimizer.optimize_for_tensor_characteristics((8, 16, 512, 512), "attention")\n    print(f"  Attention (8,16,512,512): Level {attn_opts['preferred_cache_level']}, "\n          f"Alignment: {attn_opts['alignment_needed']}")\n    \n    # KV cache tensor\n    kv_opts = optimizer.optimize_for_tensor_characteristics((1, 32, 2048, 128), "kv_cache")\n    print(f"  KV Cache (1,32,2048,128): Level {kv_opts['preferred_cache_level']}, "\n          f"Layout: {kv_opts['memory_layout']}")\n    \n    # Image embedding tensor\n    img_opts = optimizer.optimize_for_tensor_characteristics((1, 576, 1152), "image_embeddings")\n    print(f"  Image Embeddings (1,576,1152): Level {img_opts['preferred_cache_level']}")\n    \n    # Create hardware-optimized cache manager\n    print(f"\nCreating hardware-optimized cache manager...")\n    hw_cache_manager = create_hardware_optimized_cache_manager()\n    \n    # Get optimization report\n    report = hw_cache_manager.get_hardware_optimization_report()\n    print(f"  Memory bandwidth ratio (GPU/CPU): {report['memory_bandwidth_ratios']['cpu_to_gpu_bandwidth_ratio']:.2f}")\n    print(f"  Recommended max cache threads: {report['threading_config']['max_cache_threads']}")\n    \n    # Test tensor operations\n    print(f"\nTesting optimized tensor operations...")\n    \n    # Create and cache an attention tensor\n    attn_tensor = hw_cache_manager.get_tensor((4, 8, 256, 256), torch.float16, "attention")\n    print(f"  Created attention tensor: {attn_tensor.shape}")\n    \n    # Create and cache a KV cache tensor\n    kv_tensor = hw_cache_manager.get_tensor((1, 16, 1024, 64), torch.float16, "kv_cache")\n    print(f"  Created KV cache tensor: {kv_tensor.shape}")\n    \n    # Get statistics\n    stats = hw_cache_manager.get_stats()\n    print(f"\nCache Statistics:")\n    print(f"  Global hit rate: {stats['global_stats']['global_hit_rate']:.2%}")\n    print(f"  L1 utilization: {stats['l1_stats']['utilization']:.2%}")\n    print(f"  L2 utilization: {stats['l2_stats']['utilization']:.2%}")\n    print(f"  L3 utilization: {stats['l3_stats']['utilization']:.2%}")\n    \n    print(f"\nHardware-specific optimizations test completed successfully!")