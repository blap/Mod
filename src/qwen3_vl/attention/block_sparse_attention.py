"""\nAdvanced Block-Sparse Attention Patterns for Qwen3-VL model.\nImplements hardware-optimized sparse attention with learned routing mechanisms.\n"""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nimport math\n\n# Import proper rotary embedding functions\nfrom attention.rotary_embeddings import Qwen3VLRotaryEmbedding, apply_rotary_pos_emb, repeat_kv, rotate_half\n\n\nclass BlockSparseAttention(nn.Module):\n    """\n    Advanced Block-Sparse Attention with learned routing mechanisms.\n    Optimized for hardware-specific efficiency on NVIDIA SM61 architecture.\n    """\n    def __init__(self, config, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads or self.num_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.sparsity_ratio = getattr(config, 'block_sparse_sparsity_ratio', 0.5)\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"\n                f" and `num_heads`: {self.num_heads})."\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=True)\n\n        # Block-sparse routing mechanism\n        self.block_size = getattr(config, 'block_sparse_block_size', 64)\n        self.routing_network = nn.Linear(self.hidden_size, self.num_heads, bias=False)\n        \n        # Learned sparsity pattern\n        self.sparsity_pattern = nn.Parameter(\n            torch.randn(self.num_heads, self.max_position_embeddings // self.block_size, \n                       self.max_position_embeddings // self.block_size)\n        )\n        \n        # Initialize sparsity pattern to encourage sparse connections\n        nn.init.uniform_(self.sparsity_pattern, -0.1, 0.1)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        # Project queries, keys, and values\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Reshape to multi-head format\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        # Apply rotary position embeddings\n        if position_ids is None:\n            position_ids = torch.arange(q_len, dtype=torch.long, device=hidden_states.device).unsqueeze(0).expand(bsz, -1)\n\n        # Use the proper rotary embedding implementation\n        rotary_emb = Qwen3VLRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n        cos, sin = rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        # Handle past key values for caching\n        if past_key_value is not None:\n            key_states = torch.cat([past_key_value[0], key_states], dim=-2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=-2)\n\n        # Repeat keys and values for GQA (Grouped Query Attention) if applicable\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        # Apply block-sparse attention pattern\n        attn_weights = self._apply_block_sparse_attention(query_states, key_states)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # Upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _apply_block_sparse_attention(self, query_states, key_states):\n        """\n        Apply block-sparse attention pattern to reduce computation.\n        """\n        bsz, num_heads, q_len, head_dim = query_states.shape\n        _, _, k_len, _ = key_states.shape\n\n        # Calculate block dimensions\n        block_size = self.block_size\n        num_q_blocks = math.ceil(q_len / block_size)\n        num_k_blocks = math.ceil(k_len / block_size)\n\n        # Pad sequences to be divisible by block size\n        padded_q_len = num_q_blocks * block_size\n        padded_k_len = num_k_blocks * block_size\n\n        if q_len != padded_q_len or k_len != padded_k_len:\n            query_states = F.pad(query_states, (0, 0, 0, padded_q_len - q_len), value=0)\n            key_states = F.pad(key_states, (0, 0, 0, padded_k_len - k_len), value=0)\n\n        # Reshape to block format\n        query_blocks = query_states.view(bsz, num_heads, num_q_blocks, block_size, head_dim)\n        key_blocks = key_states.view(bsz, num_heads, num_k_blocks, block_size, head_dim)\n\n        # Get sparsity pattern for current sequence length\n        current_sparsity_pattern = self.sparsity_pattern[:, :num_q_blocks, :num_k_blocks]\n\n        # Apply learned sparsity pattern\n        # Use top-k selection to enforce sparsity\n        sparsity_threshold = torch.topk(\n            current_sparsity_pattern.reshape(num_heads, -1),\n            k=int(current_sparsity_pattern.numel() * self.sparsity_ratio / num_heads),\n            dim=-1\n        )[0][:, -1].view(num_heads, 1, 1)\n\n        sparse_mask = (current_sparsity_pattern > sparsity_threshold).float()\n\n        # Compute attention scores with block-sparse pattern\n        attn_weights = torch.zeros(bsz, num_heads, padded_q_len, padded_k_len,\n                                   dtype=query_states.dtype, device=query_states.device)\n\n        # Compute attention only for non-zero blocks in the sparse pattern\n        for h_idx in range(num_heads):\n            for q_block_idx in range(num_q_blocks):\n                for k_block_idx in range(num_k_blocks):\n                    if sparse_mask[h_idx, q_block_idx, k_block_idx] > 0:\n                        # Compute attention for this block pair\n                        q_block = query_blocks[:, h_idx, q_block_idx, :, :]  # [bsz, block_size, head_dim]\n                        k_block = key_blocks[:, h_idx, k_block_idx, :, :]  # [bsz, block_size, head_dim]\n\n                        block_attn = torch.matmul(q_block, k_block.transpose(-1, -2)) / math.sqrt(self.head_dim)\n                        attn_weights[:, h_idx,\n                                    q_block_idx * block_size:(q_block_idx + 1) * block_size,\n                                    k_block_idx * block_size:(k_block_idx + 1) * block_size] = block_attn\n\n        # Trim back to original sequence length\n        attn_weights = attn_weights[:, :, :q_len, :k_len]\n\n        return attn_weights\n\n\nclass VisionBlockSparseAttention(nn.Module):\n    """\n    Block-sparse attention specifically for vision transformer components.\n    """\n    def __init__(self, config, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n\n        self.hidden_size = config.vision_hidden_size\n        self.num_heads = config.vision_num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f"vision_hidden_size must be divisible by vision_num_attention_heads"\n                f" (got `vision_hidden_size`: {self.hidden_size}"\n                f" and `vision_num_attention_heads`: {self.num_heads})."\n            )\n\n        self.qkv_proj = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=config.vision_qkv_bias)\n        self.proj = nn.Linear(self.hidden_size, self.hidden_size)\n\n        # Block-sparse routing mechanism for vision\n        self.block_size = getattr(config, 'vision_block_sparse_block_size', 32)  # Smaller blocks for vision\n        self.sparsity_ratio = getattr(config, 'vision_block_sparse_sparsity_ratio', 0.4)\n\n        # Learned sparsity pattern for vision\n        max_patches = (config.vision_image_size // config.vision_patch_size) ** 2\n        self.sparsity_pattern = nn.Parameter(\n            torch.randn(self.num_heads, max_patches // self.block_size, \n                       max_patches // self.block_size)\n        )\n        \n        # Initialize sparsity pattern\n        nn.init.uniform_(self.sparsity_pattern, -0.1, 0.1)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        B, N, C = hidden_states.shape\n        \n        # Project to Q, K, V\n        qkv = self.qkv_proj(hidden_states).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, B, num_heads, N, head_dim)\n        q, k, v = qkv.unbind(0)  # Make torchscript happy (cannot use tensor as tuple)\n\n        # Apply block-sparse attention pattern\n        attn_weights = self._apply_block_sparse_attention(q, k)\n\n        if attention_mask is not None:\n            attn_weights = attn_weights + attention_mask\n\n        # Apply softmax and compute output\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n        attn_output = torch.matmul(attn_weights, v)\n\n        # Reshape and project\n        attn_output = attn_output.transpose(1, 2).contiguous().reshape(B, N, C)\n        attn_output = self.proj(attn_output)\n\n        return attn_output, attn_weights if output_attentions else None\n\n    def _apply_block_sparse_attention(self, query_states, key_states):\n        """\n        Apply block-sparse attention pattern to reduce computation for vision.\n        """\n        B, num_heads, N, head_dim = query_states.shape\n\n        # Calculate block dimensions\n        block_size = self.block_size\n        num_blocks = math.ceil(N / block_size)\n\n        # Pad sequences to be divisible by block size\n        padded_N = num_blocks * block_size\n\n        if N != padded_N:\n            query_states = F.pad(query_states, (0, 0, 0, padded_N - N), value=0)\n            key_states = F.pad(key_states, (0, 0, 0, padded_N - N), value=0)\n\n        # Reshape to block format\n        query_blocks = query_states.view(B, num_heads, num_blocks, block_size, head_dim)\n        key_blocks = key_states.view(B, num_heads, num_blocks, block_size, head_dim)\n\n        # Get sparsity pattern for current sequence length\n        # Ensure we don't exceed the available pattern dimensions\n        available_blocks = min(num_blocks, self.sparsity_pattern.size(1), self.sparsity_pattern.size(2))\n        current_sparsity_pattern = self.sparsity_pattern[:, :available_blocks, :available_blocks]\n        \n        # Apply learned sparsity pattern\n        # Use top-k selection to enforce sparsity\n        if current_sparsity_pattern.numel() > 0:\n            sparsity_threshold = torch.topk(\n                current_sparsity_pattern.reshape(num_heads, -1),\n                k=min(int(current_sparsity_pattern.numel() * self.sparsity_ratio / num_heads),\n                      current_sparsity_pattern.numel()),\n                dim=-1\n            )[0][:, -1].view(num_heads, 1, 1)\n\n            sparse_mask = (current_sparsity_pattern > sparsity_threshold).float()\n        else:\n            # If no available blocks, create a full attention mask\n            sparse_mask = torch.ones(num_heads, num_blocks, num_blocks,\n                                   device=query_states.device, dtype=query_states.dtype)\n\n        # Compute attention scores with block-sparse pattern\n        attn_weights = torch.zeros(B, num_heads, padded_N, padded_N,\n                                   dtype=query_states.dtype, device=query_states.device)\n\n        # Compute attention only for non-zero blocks in the sparse pattern\n        for h_idx in range(num_heads):\n            for q_block_idx in range(available_blocks):\n                for k_block_idx in range(available_blocks):\n                    if sparse_mask[h_idx, q_block_idx, k_block_idx] > 0:\n                        # Compute attention for this block pair\n                        q_block = query_blocks[:, h_idx, q_block_idx, :, :]  # [B, block_size, head_dim]\n                        k_block = key_blocks[:, h_idx, k_block_idx, :, :]  # [B, block_size, head_dim]\n\n                        block_attn = torch.matmul(q_block, k_block.transpose(-1, -2)) / math.sqrt(head_dim)\n                        attn_weights[:, h_idx,\n                                    q_block_idx * block_size:min((q_block_idx + 1) * block_size, padded_N),\n                                    k_block_idx * block_size:min((k_block_idx + 1) * block_size, padded_N)] = block_attn\n\n        # Trim back to original sequence length\n        attn_weights = attn_weights[:, :, :N, :N]\n\n        return attn_weights\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    """\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).\n    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) to\n    (batch, num_attention_heads, seqlen, head_dim)\n    """\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\ndef rotate_half(x):\n    """Rotates half the hidden dims of the input."""\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    """Applies Rotary Position Embedding to the query and key tensors."""\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed