"""\nAttention mechanism interface for Qwen3-VL model.\n\nThis module provides a unified interface for different attention mechanisms\nwith proper configuration support.\n"""\nfrom typing import Optional, Tuple\nimport torch\nimport torch.nn as nn\nfrom config.attention_config import AttentionConfig\n\n\nclass AttentionMechanism:\n    """\n    Base attention mechanism class that implements configurable attention operations.\n    """\n    \n    def __init__(self, config: AttentionConfig):\n        """\n        Initialize the attention mechanism with the provided configuration.\n        \n        Args:\n            config: AttentionConfig instance containing attention settings\n        """\n        self.config = config\n        \n        # Initialize attention-specific parameters based on config\n        self.attention_dropout = nn.Dropout(config.attention_dropout_prob)\n        self.use_flash_attention = config.use_flash_attention_2\n        self.use_dynamic_sparse = config.use_dynamic_sparse_attention\n        self.sparse_ratio = config.sparse_attention_sparsity_ratio\n        self.vision_sparse_ratio = config.vision_sparse_attention_sparsity_ratio\n        self.rope_theta = config.rope_theta\n        \n        # Initialize flash attention if enabled\n        if self.use_flash_attention:\n            self._init_flash_attention()\n        \n        # Initialize sparse attention components if enabled\n        if self.use_dynamic_sparse:\n            self._init_sparse_attention()\n    \n    def _init_flash_attention(self):\n        """Initialize flash attention components."""\n        # This would normally check for flash attention availability\n        try:\n            from flash_attn import flash_attn_func\n            self.flash_attn_func = flash_attn_func\n            self.has_flash_attn = True\n        except ImportError:\n            self.has_flash_attn = False\n            print("Flash attention not available, using standard attention")\n    \n    def _init_sparse_attention(self):\n        """Initialize sparse attention components."""\n        # Initialize parameters for sparse attention\n        self.sparse_pattern = self.config.sparse_attention_pattern\n        self.num_blocks = self.config.sparse_attention_num_blocks\n    \n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n                attention_mask: Optional[torch.Tensor] = None,\n                causal: bool = False) -> torch.Tensor:\n        """\n        Forward pass for attention mechanism.\n\n        Args:\n            query: Query tensor\n            key: Key tensor\n            value: Value tensor\n            attention_mask: Optional attention mask\n            causal: Whether to apply causal masking\n\n        Returns:\n            Attention output tensor\n        """\n        # Use the configured attention implementation\n        if self.use_flash_attention and hasattr(self, 'flash_attn_func') and self.has_flash_attn:\n            return self._flash_attention_forward(query, key, value, causal=causal)\n        elif self.use_dynamic_sparse:\n            return self._sparse_attention_forward(query, key, value, attention_mask, causal)\n        else:\n            return self._standard_attention_forward(query, key, value, attention_mask, causal)\n    \n    def _flash_attention_forward(self, query: torch.Tensor, key: torch.Tensor, \n                                value: torch.Tensor, causal: bool = False) -> torch.Tensor:\n        """Forward pass using Flash Attention."""\n        # Reshape for flash attention: (batch, seqlen, nheads, headdim)\n        bsz, num_heads, seq_len, head_dim = query.size()\n        \n        # Flash attention expects (batch, seqlen, nheads, headdim)\n        query = query.transpose(1, 2)  # (batch, seqlen, nheads, headdim)\n        key = key.transpose(1, 2)\n        value = value.transpose(1, 2)\n        \n        # Apply flash attention\n        output = self.flash_attn_func(\n            query, key, value,\n            dropout_p=self.config.attention_dropout_prob if self.training else 0.0,\n            causal=causal and self.config.flash_attention_causal\n        )\n        \n        # Reshape back to (batch, nheads, seqlen, headdim)\n        output = output.transpose(1, 2)\n        \n        return output\n    \n    def _sparse_attention_forward(self, query: torch.Tensor, key: torch.Tensor,\n                                 value: torch.Tensor, attention_mask: Optional[torch.Tensor],\n                                 causal: bool) -> torch.Tensor:\n        """Forward pass using sparse attention."""\n        # Calculate attention scores\n        bsz, num_heads, seq_len, head_dim = query.size()\n        \n        # For sparse attention, we only compute attention for top-k tokens\n        scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n        \n        if causal:\n            # Apply causal mask\n            causal_mask = torch.triu(\n                torch.ones(seq_len, seq_len, dtype=torch.bool, device=query.device), \n                diagonal=1\n            )\n            scores.masked_fill_(causal_mask, float('-inf'))\n        \n        if attention_mask is not None:\n            scores = scores + attention_mask\n        \n        # Apply sparsity - keep only top-k values\n        sparse_ratio = self.sparse_ratio\n        if query.size(-2) != key.size(-2):  # Different sequence lengths might indicate vision tokens\n            sparse_ratio = self.vision_sparse_ratio\n        \n        k = max(1, int(seq_len * sparse_ratio))\n        top_k_scores, top_k_indices = torch.topk(scores, k=k, dim=-1)\n        \n        # Create sparse attention mask\n        sparse_mask = torch.zeros_like(scores)\n        sparse_mask.scatter_(-1, top_k_indices, torch.ones_like(top_k_scores))\n        sparse_mask = sparse_mask.masked_fill(sparse_mask == 0, float('-inf'))\n        \n        # Apply sparse mask to scores\n        scores = scores + sparse_mask\n        \n        # Apply softmax\n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n        \n        # Apply attention to values\n        output = torch.matmul(attn_weights, value)\n        \n        return output\n    \n    def _standard_attention_forward(self, query: torch.Tensor, key: torch.Tensor,\n                                   value: torch.Tensor, attention_mask: Optional[torch.Tensor],\n                                   causal: bool) -> torch.Tensor:\n        """Forward pass using standard attention."""\n        # Calculate attention scores\n        bsz, num_heads, seq_len, head_dim = query.size()\n        scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n        \n        if causal:\n            # Apply causal mask\n            causal_mask = torch.triu(\n                torch.ones(seq_len, seq_len, dtype=torch.bool, device=query.device), \n                diagonal=1\n            )\n            scores.masked_fill_(causal_mask, float('-inf'))\n        \n        if attention_mask is not None:\n            scores = scores + attention_mask\n        \n        # Apply softmax\n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n        \n        # Apply attention to values\n        output = torch.matmul(attn_weights, value)\n        \n        return output