"""\nMixture of Experts (MoE) MLP implementation for Qwen3-VL.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\nfrom system.interfaces import MLP\nfrom config.config import Qwen3VLConfig\n\n\nclass MoEMLP(MLP):\n    """\n    Mixture of Experts (MoE) MLP implementation for Qwen3-VL.\n    """\n    \n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.num_experts = config.moe_num_experts\n        self.top_k = config.moe_top_k\n\n        # Create experts\n        self.experts = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(self.hidden_size, self.intermediate_size, bias=False),\n                nn.SiLU(),\n                nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n            ) for _ in range(self.num_experts)\n        ])\n\n        # Router layer\n        self.gate = nn.Linear(self.hidden_size, self.num_experts, bias=False)\n\n        # Activation function\n        self.act_fn = nn.SiLU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Get routing weights\n        gate_logits = self.gate(x)  # [batch_size, seq_len, num_experts]\n        weights = torch.softmax(gate_logits, dim=-1)\n        \n        # Get top-k experts for each token\n        top_k_weights, top_k_indices = torch.topk(weights, self.top_k, dim=-1)\n        \n        # Normalize weights\n        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)\n        \n        # Process through selected experts\n        final_output = torch.zeros_like(x)\n        \n        for i in range(self.top_k):\n            # Get the indices and weights for the i-th best expert\n            expert_indices = top_k_indices[..., i].flatten()  # [batch_size * seq_len]\n            expert_weights = top_k_weights[..., i].unsqueeze(-1)  # [batch_size, seq_len, 1]\n            \n            # Reshape input for processing\n            flat_input = x.view(-1, x.size(-1))  # [batch_size * seq_len, hidden_size]\n            \n            # Process each token with its selected expert\n            for token_idx, expert_idx in enumerate(expert_indices):\n                expert_output = self.experts[expert_idx](flat_input[token_idx:token_idx+1])\n                final_output.view(-1, x.size(-1))[token_idx] += expert_output * expert_weights.view(-1, 1)[token_idx]\n        \n        return final_output