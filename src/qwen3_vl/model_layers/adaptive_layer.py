"""\nAdaptive transformer layer implementation for Qwen3-VL.\n"""\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple\nfrom system.interfaces import Layer, AttentionMechanism, MLP\nfrom config.config import Qwen3VLConfig\n\n\nclass AdaptiveLayer(Layer):\n    """\n    Adaptive transformer decoder layer implementation for Qwen3-VL.\n    Supports adaptive depth based on input complexity.\n    """\n    \n    def __init__(self, config: Qwen3VLConfig, layer_idx: int, attention: AttentionMechanism, mlp: MLP):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.config = config\n\n        # Use provided attention and MLP components\n        self.self_attn = attention\n        self.mlp = mlp\n\n        # Normalization layers\n        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        \n        # Adaptive depth components\n        if config.use_adaptive_depth:\n            # Add a complexity assessment head to determine if this layer should be skipped\n            self.complexity_assessment = nn.Linear(config.hidden_size, 1)\n            self.exit_threshold = config.exit_threshold\n            self.depth_temperature = config.depth_temperature\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # For adaptive depth, assess if we should skip this layer\n        if self.config.use_adaptive_depth and hasattr(self, 'complexity_assessment'):\n            # Assess complexity of input\n            complexity_score = torch.sigmoid(self.complexity_assessment(hidden_states.mean(dim=1, keepdim=True)))\n            \n            # If complexity is below threshold, return input as is (skip layer)\n            if complexity_score.mean() < self.config.exit_threshold:\n                outputs = (hidden_states,)\n                if output_attentions:\n                    outputs += (None,)\n                if use_cache:\n                    outputs += (past_key_value,)\n                return outputs\n\n        # Apply input layer norm with optimized operations\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self-attention\n        attn_output, attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n        )\n\n        # Add residual connection\n        hidden_states = residual + attn_output\n\n        # Apply post-attention layer norm\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n\n        # MLP\n        feed_forward_hidden_states = self.mlp(hidden_states)\n\n        # Add residual connection\n        hidden_states = residual + feed_forward_hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs