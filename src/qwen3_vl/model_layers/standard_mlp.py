"""\nStandard MLP implementation for Qwen3-VL.\n"""\nimport torch\nimport torch.nn as nn\nfrom system.interfaces import MLP\nfrom config.config import Qwen3VLConfig\n\n\nclass StandardMLP(MLP):\n    """\n    Standard MLP (feed-forward) layer implementation for Qwen3-VL.\n    """\n    \n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n\n        # Standard MLP components\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n\n        self.act_fn = nn.SiLU()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Use optimized computation with proper memory layout\n        gate_output = self.gate_proj(x)\n        up_output = self.up_proj(x)\n\n        # Apply activation function\n        activated_gate = self.act_fn(gate_output)\n\n        # Element-wise multiplication with optimized memory access\n        intermediate_output = activated_gate * up_output\n\n        # Down projection\n        output = self.down_proj(intermediate_output)\n\n        return output