"""\nMigration utilities to transition from the old modular configuration system \nto the new unified configuration system.\n"""\nfrom typing import Dict, Any, Optional\nfrom .unified_config import UnifiedQwen3VLConfig\nfrom .config import Qwen3VLConfig\n\n\ndef migrate_from_modular_to_unified(old_config: Qwen3VLConfig) -> UnifiedQwen3VLConfig:\n    """\n    Migrate from the old modular configuration system to the unified configuration system.\n    \n    Args:\n        old_config: An instance of the old Qwen3VLConfig with modular components\n        \n    Returns:\n        A new UnifiedQwen3VLConfig with all the same settings\n    """\n    # Create a dictionary of all parameters from the old config\n    config_dict = {}\n    \n    # Copy all basic parameters\n    for field_name in [\n        'vocab_size', 'hidden_size', 'num_hidden_layers', 'num_attention_heads',\n        'num_key_value_heads', 'intermediate_size', 'hidden_act', 'hidden_dropout_prob',\n        'max_position_embeddings', 'initializer_range', 'layer_norm_eps', 'pad_token_id',\n        'tie_word_embeddings', 'vision_model_type', 'vision_hidden_size', 'vision_num_hidden_layers',\n        'vision_num_attention_heads', 'vision_intermediate_size', 'vision_patch_size',\n        'vision_image_size', 'vision_window_size', 'vision_num_channels', 'vision_qkv_bias',\n        'num_query_tokens', 'vision_projection_dim', 'language_projection_dim', 'torch_dtype',\n        'pretraining_tp', 'use_cache', 'use_adapters', 'use_early_exit', 'exit_threshold',\n        'use_adaptive_depth', 'use_vision_adaptive_depth', 'use_multimodal_adaptive_depth',\n        'min_depth_ratio', 'max_depth_ratio', 'vision_min_depth_ratio', 'vision_max_depth_ratio',\n        'depth_temperature', 'use_context_adaptive_positional_encoding', 'use_cross_modal_positional_encoding',\n        'use_conditional_feature_extraction', 'use_moe', 'moe_num_experts', 'moe_top_k',\n        'use_sparsity', 'sparsity_ratio', 'use_adaptive_precision', 'use_cross_modal_compression',\n        'compression_ratio', 'use_cross_layer_memory_sharing', 'use_mixed_precision',\n        'gpu_memory_size', 'use_inference_memory_efficient', 'use_kv_cache_optimization',\n        'kv_cache_window_size', 'kv_low_rank_dimension', 'use_low_rank_kv_cache',\n        'use_flash_attention_2', 'attention_implementation', 'use_memory_efficient_attention',\n        'kv_cache_strategy', 'use_gradient_checkpointing'\n    ]:\n        if hasattr(old_config, field_name):\n            config_dict[field_name] = getattr(old_config, field_name)\n    \n    # Copy memory-related parameters from memory_config\n    if old_config.memory_config:\n        memory_fields = [\n            'use_memory_pooling', 'memory_pool_initial_size', 'memory_pool_max_size',\n            'memory_pool_growth_factor', 'use_buddy_allocation', 'memory_defragmentation_enabled',\n            'memory_defragmentation_threshold', 'kv_cache_strategy', 'use_low_rank_kv_cache',\n            'kv_cache_window_size', 'kv_low_rank_dimension', 'kv_cache_max_length',\n            'use_gradient_checkpointing', 'use_activation_sparsity', 'sparsity_ratio',\n            'use_pre_allocated_tensors', 'pre_allocated_cache_size', 'memory_efficient_attention',\n            'use_vision_memory_optimization', 'vision_memory_chunk_size', 'use_tensor_fusion'\n        ]\n        for field_name in memory_fields:\n            if hasattr(old_config.memory_config, field_name):\n                config_dict[field_name] = getattr(old_config.memory_config, field_name)\n    \n    # Copy attention-related parameters from attention_config\n    if old_config.attention_config:\n        attention_fields = [\n            'attention_implementation', 'use_flash_attention_2', 'flash_attention_causal',\n            'attention_dropout_prob', 'use_memory_efficient_attention', 'use_dynamic_sparse_attention',\n            'sparse_attention_sparsity_ratio', 'vision_sparse_attention_sparsity_ratio',\n            'sparse_attention_pattern', 'sparse_attention_num_blocks', 'rope_theta',\n            'use_rotary_embedding', 'use_approximated_rotary_embeddings', 'rotary_embedding_scaling_factor',\n            'num_attention_heads', 'num_key_value_heads', 'head_dim', 'use_block_sparse_attention',\n            'block_sparse_block_size', 'use_learned_attention_routing', 'learned_routing_temperature'\n        ]\n        for field_name in attention_fields:\n            if hasattr(old_config.attention_config, field_name):\n                config_dict[field_name] = getattr(old_config.attention_config, field_name)\n    \n    # Copy routing-related parameters from routing_config\n    if old_config.routing_config:\n        routing_fields = [\n            'use_moe', 'moe_num_experts', 'moe_top_k', 'moe_use_residual', 'moe_jitter_noise',\n            'moe_normalize_gate', 'moe_capacity_factor', 'moe_drop_tokens', 'moe_use_tutel',\n            'moe_router_zloss_coef', 'moe_router_aux_loss_coef', 'moe_label_smoothing',\n            'moe_router_dtype', 'use_token_level_routing', 'token_routing_temperature',\n            'token_routing_confidence_threshold', 'use_adaptive_routing',\n            'adaptive_routing_complexity_metric', 'adaptive_routing_temperature',\n            'use_cross_layer_recycling', 'cross_layer_recycling_ratio', 'cross_layer_adapter_dim',\n            'use_load_balancing', 'load_balancing_frequency', 'use_frequency_regularization'\n        ]\n        for field_name in routing_fields:\n            if hasattr(old_config.routing_config, field_name):\n                config_dict[field_name] = getattr(old_config.routing_config, field_name)\n    \n    # Copy hardware-related parameters from hardware_config\n    if old_config.hardware_config:\n        hardware_fields = [\n            'hardware_detection_timeout', 'hardware_fallback_enabled', 'hardware_monitoring_enabled',\n            'gpu_device_ids', 'gpu_memory_fraction', 'gpu_allow_growth', 'gpu_precision',\n            'cpu_threads', 'cpu_affinity_enabled', 'cpu_inter_op_parallelism', 'cpu_intra_op_parallelism',\n            'use_pinned_memory', 'use_cuda_streams', 'memory_transfer_overlap',\n            'hardware_target', 'enable_intel_optimizations', 'enable_nvidia_optimizations',\n            'enable_avx_instructions', 'enable_tensor_cores', 'nvme_cache_enabled',\n            'nvme_cache_path', 'nvme_cache_size', 'nvme_cache_policy', 'nvme_prefetch_enabled',\n            'power_management_enabled', 'thermal_throttling_enabled', 'power_limit_watts',\n            'thermal_limit_celsius', 'simd_optimization_enabled', 'vector_instruction_set',\n            'use_jit_compilation', 'distributed_enabled', 'pipeline_parallelism_degree',\n            'tensor_parallelism_degree', 'use_deepspeed', 'use_fairscale'\n        ]\n        for field_name in hardware_fields:\n            if hasattr(old_config.hardware_config, field_name):\n                config_dict[field_name] = getattr(old_config.hardware_config, field_name)\n    \n    # Create the new unified config\n    try:\n        new_config = UnifiedQwen3VLConfig(**config_dict)\n        return new_config\n    except TypeError as e:\n        # If there are any fields that don't match, we'll create the config with valid fields\n        # and warn about the ones that were ignored\n        valid_fields = set(UnifiedQwen3VLConfig.__dataclass_fields__.keys())\n        invalid_fields = set(config_dict.keys()) - valid_fields\n        \n        if invalid_fields:\n            print(f"Warning: The following fields from the old config are not supported in the new config: {invalid_fields}")\n        \n        # Filter the config dict to only include valid fields\n        filtered_config_dict = {k: v for k, v in config_dict.items() if k in valid_fields}\n        return UnifiedQwen3VLConfig(**filtered_config_dict)\n\n\ndef create_unified_config_from_dict(config_dict: Dict[str, Any]) -> UnifiedQwen3VLConfig:\n    """\n    Create a unified configuration from a dictionary, supporting both old and new field names.\n    \n    Args:\n        config_dict: Dictionary containing configuration parameters\n        \n    Returns:\n        A new UnifiedQwen3VLConfig instance\n    """\n    # This is essentially the same as the from_dict method but with backward compatibility\n    # mapping for any old field names that might have been used differently\n    return UnifiedQwen3VLConfig.from_dict(config_dict)\n\n\nclass ConfigConverterFactory:\n    """\n    Factory class to help with converting between old and new configuration systems.\n    """\n    \n    @staticmethod\n    def convert_to_unified(config_input) -> UnifiedQwen3VLConfig:\n        """\n        Convert various input types to the unified configuration format.\n        \n        Args:\n            config_input: Can be a dict, old Qwen3VLConfig, or UnifiedQwen3VLConfig\n            \n        Returns:\n            UnifiedQwen3VLConfig instance\n        """\n        if isinstance(config_input, UnifiedQwen3VLConfig):\n            return config_input\n        elif isinstance(config_input, dict):\n            return create_unified_config_from_dict(config_input)\n        elif hasattr(config_input, '__class__') and 'Qwen3VLConfig' in config_input.__class__.__name__:\n            return migrate_from_modular_to_unified(config_input)\n        else:\n            raise ValueError(f"Unsupported config input type: {type(config_input)}")\n    \n    @staticmethod\n    def create_default_config(config_type: str = "balanced") -> UnifiedQwen3VLConfig:\n        """\n        Create a default configuration of a specific type.\n        \n        Args:\n            config_type: One of "balanced", "performance_optimized", or "memory_efficient"\n            \n        Returns:\n            UnifiedQwen3VLConfig instance\n        """\n        if config_type == "balanced":\n            return UnifiedQwen3VLConfig.balanced()\n        elif config_type == "performance_optimized":\n            return UnifiedQwen3VLConfig.performance_optimized()\n        elif config_type == "memory_efficient":\n            return UnifiedQwen3VLConfig.memory_efficient()\n        else:\n            raise ValueError(f"Unknown config type: {config_type}. Use 'balanced', 'performance_optimized', or 'memory_efficient'")
