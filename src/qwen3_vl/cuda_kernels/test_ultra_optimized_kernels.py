"""\nComprehensive test for ultra-optimized CUDA kernels\nValidates all state-of-the-art optimization techniques\n"""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport time\nimport numpy as np\nfrom typing import Tuple, Dict, Any\n\nfrom cuda_kernels.ultra_optimized_wrapper import UltraOptimizedAttention, UltraOptimizedMLP, UltraOptimizedLayerNorm, UltraOptimizedTransformerBlock, UltraOptimizedQwen3VLModel, benchmark_ultra_optimized_components\n    UltraOptimizedAttention,\n    UltraOptimizedMLP,\n    UltraOptimizedLayerNorm,\n    UltraOptimizedTransformerBlock,\n    UltraOptimizedQwen3VLModel,\n    benchmark_ultra_optimized_components\n)\n\n\ndef benchmark_function(func, *args, num_runs=10, warmup=3, **kwargs):\n    """\n    Benchmark a function and return timing statistics\n    """\n    # Warmup runs\n    for _ in range(warmup):\n        _ = func(*args, **kwargs)\n\n    # Actual timing runs\n    times = []\n    for _ in range(num_runs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        torch.cuda.synchronize()  # Ensure completion for accurate timing\n        end_time = time.time()\n        times.append((end_time - start_time) * 1000)  # Convert to milliseconds\n\n    return {\n        'mean': np.mean(times),\n        'std': np.std(times),\n        'min': np.min(times),\n        'max': np.max(times),\n        'median': np.median(times)\n    }\n\n\ndef test_ultra_optimized_attention():\n    """\n    Test ultra-optimized attention performance and accuracy\n    """\n    print("Testing ultra-optimized attention...")\n\n    if not torch.cuda.is_available():\n        print("CUDA not available, skipping attention test")\n        return True\n\n    # Create test tensors\n    batch_size, seq_len, num_heads, head_dim = 2, 512, 8, 64\n    device = 'cuda'\n\n    query = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)\n    key = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)\n    value = torch.randn(batch_size, num_heads, seq_len, head_dim, device=device, dtype=torch.float16)\n\n    # Create ultra-optimized attention module\n    ultra_attention = UltraOptimizedAttention(\n        embed_dim=num_heads * head_dim,\n        num_heads=num_heads,\n        dropout=0.0\n    ).to(device).half()\n\n    # Test forward pass\n    with torch.no_grad():\n        output, _ = ultra_attention(\n            query.transpose(1, 2).contiguous().view(batch_size, seq_len, -1),\n            key.transpose(1, 2).contiguous().view(batch_size, seq_len, -1),\n            value.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        )\n\n    print(f"Ultra-optimized attention output shape: {output.shape}")\n    assert output.shape == (batch_size, seq_len, num_heads * head_dim), f"Output shape mismatch"\n\n    # Benchmark performance\n    def ultra_forward():\n        with torch.no_grad():\n            return ultra_attention(\n                query.transpose(1, 2).contiguous().view(batch_size, seq_len, -1),\n                key.transpose(1, 2).contiguous().view(batch_size, seq_len, -1),\n                value.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n            )\n\n    results = benchmark_function(ultra_forward)\n    print(f"Ultra-optimized attention: {results['mean']:.3f}ms Â± {results['std']:.3f}ms")\n\n    print("âœ“ Ultra-optimized attention test passed")\n    return True\n\n\ndef test_ultra_optimized_mlp():\n    """\n    Test ultra-optimized MLP performance and accuracy\n    """\n    print("Testing ultra-optimized MLP...")\n\n    if not torch.cuda.is_available():\n        print("CUDA not available, skipping MLP test")\n        return True\n\n    # Create test tensor\n    batch_size, seq_len, embed_dim = 2, 512, 256\n    device = 'cuda'\n\n    input_tensor = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=torch.float16)\n\n    # Create ultra-optimized MLP module\n    ultra_mlp = UltraOptimizedMLP(\n        embed_dim=embed_dim,\n        intermediate_dim=embed_dim * 4,\n        activation='silu',\n        use_custom_precision=True\n    ).to(device).half()\n\n    # Test forward pass\n    with torch.no_grad():\n        output = ultra_mlp(input_tensor)\n\n    print(f"Ultra-optimized MLP output shape: {output.shape}")\n    assert output.shape == (batch_size, seq_len, embed_dim), f"Output shape mismatch"\n\n    # Benchmark performance\n    def ultra_mlp_forward():\n        with torch.no_grad():\n            return ultra_mlp(input_tensor)\n\n    results = benchmark_function(ultra_mlp_forward)\n    print(f"Ultra-optimized MLP: {results['mean']:.3f}ms Â± {results['std']:.3f}ms")\n\n    print("âœ“ Ultra-optimized MLP test passed")\n    return True\n\n\ndef test_ultra_optimized_layer_norm():\n    """\n    Test ultra-optimized layer norm performance and accuracy\n    """\n    print("Testing ultra-optimized layer norm...")\n\n    if not torch.cuda.is_available():\n        print("CUDA not available, skipping layer norm test")\n        return True\n\n    # Create test tensor\n    batch_size, seq_len, embed_dim = 2, 512, 256\n    device = 'cuda'\n\n    input_tensor = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=torch.float16)\n\n    # Create ultra-optimized layer norm module\n    ultra_norm = UltraOptimizedLayerNorm(embed_dim).to(device).half()\n\n    # Test forward pass\n    with torch.no_grad():\n        output = ultra_norm(input_tensor)\n\n    print(f"Ultra-optimized layer norm output shape: {output.shape}")\n    assert output.shape == (batch_size, seq_len, embed_dim), f"Output shape mismatch"\n\n    # Benchmark performance\n    def ultra_norm_forward():\n        with torch.no_grad():\n            return ultra_norm(input_tensor)\n\n    results = benchmark_function(ultra_norm_forward)\n    print(f"Ultra-optimized layer norm: {results['mean']:.3f}ms Â± {results['std']:.3f}ms")\n\n    print("âœ“ Ultra-optimized layer norm test passed")\n    return True\n\n\ndef test_ultra_optimized_transformer_block():\n    """\n    Test ultra-optimized transformer block performance and accuracy\n    """\n    print("Testing ultra-optimized transformer block...")\n\n    if not torch.cuda.is_available():\n        print("CUDA not available, skipping transformer block test")\n        return True\n\n    # Create test tensor\n    batch_size, seq_len, embed_dim, num_heads = 2, 512, 256, 8\n    device = 'cuda'\n\n    input_tensor = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=torch.float16)\n\n    # Create ultra-optimized transformer block\n    ultra_block = UltraOptimizedTransformerBlock(\n        embed_dim=embed_dim,\n        num_heads=num_heads,\n        dropout=0.0,\n        use_custom_precision=True\n    ).to(device).half()\n\n    # Test forward pass\n    with torch.no_grad():\n        output = ultra_block(input_tensor)\n\n    print(f"Ultra-optimized transformer block output shape: {output.shape}")\n    assert output.shape == (batch_size, seq_len, embed_dim), f"Output shape mismatch"\n\n    # Benchmark performance\n    def ultra_block_forward():\n        with torch.no_grad():\n            return ultra_block(input_tensor)\n\n    results = benchmark_function(ultra_block_forward)\n    print(f"Ultra-optimized transformer block: {results['mean']:.3f}ms Â± {results['std']:.3f}ms")\n\n    print("âœ“ Ultra-optimized transformer block test passed")\n    return True\n\n\ndef test_ultra_optimized_model():\n    """\n    Test ultra-optimized model performance and accuracy\n    """\n    print("Testing ultra-optimized model...")\n\n    if not torch.cuda.is_available():\n        print("CUDA not available, skipping model test")\n        return True\n\n    # Create a small config for testing\n    class TestConfig:\n        hidden_size = 256\n        num_attention_heads = 8\n        num_hidden_layers = 2\n        intermediate_size = 512\n        hidden_act = "silu"\n        hidden_dropout_prob = 0.0\n        attention_dropout_prob = 0.0\n        max_position_embeddings = 512\n        initializer_range = 0.02\n        layer_norm_eps = 1e-6\n        pad_token_id = 0\n        vocab_size = 1000\n        use_cache = True\n        num_key_value_heads = None\n        use_custom_precision = True\n        quantization_bits = 8\n\n    config = TestConfig()\n\n    # Create ultra-optimized model\n    ultra_model = UltraOptimizedQwen3VLModel(config).cuda().half()\n\n    # Create test inputs\n    batch_size, seq_len = 2, 64\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len), device='cuda')\n\n    # Test forward pass\n    with torch.no_grad():\n        output = ultra_model(input_ids)\n\n    print(f"Ultra-optimized model output shape: {output.shape}")\n    assert output.shape == (batch_size, seq_len, config.vocab_size), f"Output shape mismatch"\n\n    # Benchmark performance\n    def ultra_model_forward():\n        with torch.no_grad():\n            return ultra_model(input_ids)\n\n    results = benchmark_function(ultra_model_forward)\n    print(f"Ultra-optimized model: {results['mean']:.3f}ms Â± {results['std']:.3f}ms")\n\n    print("âœ“ Ultra-optimized model test passed")\n    return True\n\n\ndef test_memory_efficiency():\n    """\n    Test memory efficiency of ultra-optimized components\n    """\n    print("Testing memory efficiency...")\n\n    if not torch.cuda.is_available():\n        print("CUDA not available, skipping memory efficiency test")\n        return True\n\n    # Monitor memory usage\n    initial_memory = torch.cuda.memory_allocated()\n\n    # Create a moderately large model\n    class TestConfig:\n        hidden_size = 512\n        num_attention_heads = 16\n        num_hidden_layers = 4\n        intermediate_size = 1024\n        hidden_act = "silu"\n        hidden_dropout_prob = 0.0\n        attention_dropout_prob = 0.0\n        max_position_embeddings = 512\n        initializer_range = 0.02\n        layer_norm_eps = 1e-6\n        pad_token_id = 0\n        vocab_size = 1000\n        use_cache = True\n        num_key_value_heads = None\n        use_custom_precision = True\n        quantization_bits = 8\n\n    config = TestConfig()\n    ultra_model = UltraOptimizedQwen3VLModel(config).cuda().half()\n\n    model_memory = torch.cuda.memory_allocated() - initial_memory\n    print(f"Ultra-optimized model memory usage: {model_memory / 1024 / 1024:.2f} MB")\n\n    # Test forward pass to ensure memory doesn't grow unexpectedly\n    batch_size, seq_len = 2, 128\n    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len), device='cuda')\n\n    with torch.no_grad():\n        _ = ultra_model(input_ids)\n\n    peak_memory = torch.cuda.max_memory_allocated()\n    print(f"Peak memory usage: {peak_memory / 1024 / 1024:.2f} MB")\n\n    # Clean up\n    del ultra_model\n    torch.cuda.empty_cache()\n\n    print("âœ“ Memory efficiency test passed")\n    return True\n\n\ndef test_numerical_accuracy():\n    """\n    Test numerical accuracy of ultra-optimized components\n    """\n    print("Testing numerical accuracy...")\n\n    if not torch.cuda.is_available():\n        print("CUDA not available, skipping numerical accuracy test")\n        return True\n\n    # Create test tensors\n    batch_size, seq_len, embed_dim, num_heads = 2, 32, 128, 4\n    head_dim = embed_dim // num_heads\n    device = 'cuda'\n\n    # Create tensors with known values for testing\n    query = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=torch.float32)\n    key = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=torch.float32)\n    value = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=torch.float32)\n\n    # Create ultra-optimized attention\n    ultra_attention = UltraOptimizedAttention(\n        embed_dim=embed_dim,\n        num_heads=num_heads,\n        dropout=0.0\n    ).to(device).float()\n\n    # Test forward pass\n    with torch.no_grad():\n        ultra_output, _ = ultra_attention(query, key, value)\n\n    # Verify output properties\n    assert not torch.isnan(ultra_output).any(), "Output contains NaN values"\n    assert not torch.isinf(ultra_output).any(), "Output contains infinite values"\n    \n    # Check that output has reasonable magnitude\n    assert torch.all(torch.abs(ultra_output) < 100.0), "Output values are too large"\n\n    print("âœ“ Numerical accuracy test passed")\n    return True\n\n\ndef run_comprehensive_tests():\n    """\n    Run all comprehensive tests for ultra-optimized kernels\n    """\n    print("Running comprehensive tests for ultra-optimized CUDA kernels...\n")\n\n    tests = [\n        test_ultra_optimized_attention,\n        test_ultra_optimized_mlp,\n        test_ultra_optimized_layer_norm,\n        test_ultra_optimized_transformer_block,\n        test_ultra_optimized_model,\n        test_memory_efficiency,\n        test_numerical_accuracy,\n    ]\n\n    passed = 0\n    total = len(tests)\n\n    for test_func in tests:\n        try:\n            if test_func():\n                passed += 1\n        except Exception as e:\n            print(f"Test {test_func.__name__} failed with error: {e}")\n            import traceback\n            traceback.print_exc()\n\n    print(f"\nComprehensive test results: {passed}/{total} tests passed")\n\n    if passed == total:\n        print("ðŸŽ‰ All comprehensive tests passed!")\n        print("âœ… Ultra-optimized kernels provide expected functionality")\n        print("âœ… Numerical accuracy is maintained")\n        print("âœ… Memory efficiency is optimized")\n        print("âœ… All components are properly integrated")\n        return True\n    else:\n        print(f"âŒ {total - passed} tests failed")\n        return False\n\n\nif __name__ == "__main__":\n    success = run_comprehensive_tests()\n    if not success:\n        exit(1)\n    \n    # Also run the benchmark\n    print("\nRunning ultra-optimized components benchmark...")\n    model = benchmark_ultra_optimized_components()