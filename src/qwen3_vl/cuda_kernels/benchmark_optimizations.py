"""\nBenchmark script to compare performance of original vs optimized CUDA kernels\nfor the Qwen3-VL-2B-Instruct model on SM61 architecture\n"""\nimport torch\nimport time\nimport numpy as np\nfrom typing import Tuple\nimport logging\n\n# Import the original and optimized CUDA wrappers\nfrom cuda_kernels.cuda_wrapper import SM61AttentionWrapper, SM61TensorOpsWrapper\nfrom cuda_kernels.optimized_cuda_wrapper import OptimizedSM61AttentionWrapper, OptimizedSM61TensorOpsWrapper\n\nlogger = logging.getLogger(__name__)\n\ndef benchmark_attention_kernels(\n    batch_size: int = 2,\n    seq_len: int = 512,\n    num_heads: int = 8,\n    head_dim: int = 64,\n    num_iterations: int = 10,\n    warmup_iterations: int = 3\n) -> Tuple[float, float]:\n    """\n    Benchmark original vs optimized attention kernels\n    """\n    print(f"Benchmarking attention kernels: batch_size={batch_size}, seq_len={seq_len}, "\n          f"num_heads={num_heads}, head_dim={head_dim}")\n    \n    # Create input tensors\n    q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n    k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n    v = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n    \n    # Initialize wrappers\n    original_wrapper = SM61AttentionWrapper()\n    optimized_wrapper = OptimizedSM61AttentionWrapper()\n    \n    # Warmup runs\n    print("Running warmup iterations...")\n    for _ in range(warmup_iterations):\n        _ = original_wrapper.forward(q, k, v)\n        _ = optimized_wrapper.forward(q, k, v)\n        torch.cuda.synchronize()\n    \n    # Benchmark original kernel\n    print("Benchmarking original attention kernel...")\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    for i in range(num_iterations):\n        output = original_wrapper.forward(q, k, v)\n        torch.cuda.synchronize()\n    \n    original_time = time.time() - start_time\n    original_avg_time = original_time / num_iterations\n    \n    # Benchmark optimized kernel\n    print("Benchmarking optimized attention kernel...")\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    for i in range(num_iterations):\n        output = optimized_wrapper.forward(q, k, v)\n        torch.cuda.synchronize()\n    \n    optimized_time = time.time() - start_time\n    optimized_avg_time = optimized_time / num_iterations\n    \n    # Calculate speedup\n    speedup = original_avg_time / optimized_avg_time\n    \n    print(f"Results:")\n    print(f"  Original: {original_avg_time*1000:.3f} ms/iteration")\n    print(f"  Optimized: {optimized_avg_time*1000:.3f} ms/iteration")\n    print(f"  Speedup: {speedup:.2f}x")\n    \n    return original_avg_time, optimized_avg_time\n\n\ndef benchmark_matmul_kernels(\n    m: int = 1024,\n    n: int = 1024,\n    k: int = 512,\n    num_iterations: int = 10,\n    warmup_iterations: int = 3\n) -> Tuple[float, float]:\n    """\n    Benchmark original vs optimized matmul kernels\n    """\n    print(f"Benchmarking matmul kernels: m={m}, n={n}, k={k}")\n    \n    # Create input tensors\n    a = torch.randn(m, k, device='cuda', dtype=torch.float32)\n    b = torch.randn(k, n, device='cuda', dtype=torch.float32)\n    \n    # Initialize wrappers\n    original_wrapper = SM61TensorOpsWrapper()\n    optimized_wrapper = OptimizedSM61TensorOpsWrapper()\n    \n    # Warmup runs\n    print("Running warmup iterations...")\n    for _ in range(warmup_iterations):\n        _ = original_wrapper.matmul(a, b)\n        _ = optimized_wrapper.matmul(a, b)\n        torch.cuda.synchronize()\n    \n    # Benchmark original kernel\n    print("Benchmarking original matmul kernel...")\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    for i in range(num_iterations):\n        output = original_wrapper.matmul(a, b)\n        torch.cuda.synchronize()\n    \n    original_time = time.time() - start_time\n    original_avg_time = original_time / num_iterations\n    \n    # Benchmark optimized kernel\n    print("Benchmarking optimized matmul kernel...")\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    for i in range(num_iterations):\n        output = optimized_wrapper.matmul(a, b)\n        torch.cuda.synchronize()\n    \n    optimized_time = time.time() - start_time\n    optimized_avg_time = optimized_time / num_iterations\n    \n    # Calculate speedup\n    speedup = original_avg_time / optimized_avg_time\n    \n    print(f"Results:")\n    print(f"  Original: {original_avg_time*1000:.3f} ms/iteration")\n    print(f"  Optimized: {optimized_avg_time*1000:.3f} ms/iteration")\n    print(f"  Speedup: {speedup:.2f}x")\n    \n    return original_avg_time, optimized_avg_time\n\n\ndef benchmark_mixed_precision_attention(\n    batch_size: int = 2,\n    seq_len: int = 512,\n    num_heads: int = 8,\n    head_dim: int = 64,\n    num_iterations: int = 10,\n    warmup_iterations: int = 3\n) -> Tuple[float, float, float]:\n    """\n    Benchmark float32 vs half precision attention kernels\n    """\n    print(f"Benchmarking mixed precision attention: batch_size={batch_size}, seq_len={seq_len}, "\n          f"num_heads={num_heads}, head_dim={head_dim}")\n    \n    # Create input tensors in float32\n    q_f32 = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n    k_f32 = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n    v_f32 = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n    \n    # Convert to half precision\n    q_f16 = q_f32.half()\n    k_f16 = k_f32.half()\n    v_f16 = v_f32.half()\n    \n    # Initialize wrappers\n    float_wrapper = OptimizedSM61AttentionWrapper(use_mixed_precision=False)\n    half_wrapper = OptimizedSM61AttentionWrapper(use_mixed_precision=True)\n    \n    # Warmup runs\n    print("Running warmup iterations...")\n    for _ in range(warmup_iterations):\n        _ = float_wrapper.forward(q_f32, k_f32, v_f32)\n        _ = half_wrapper.forward(q_f16, k_f16, v_f16)\n        torch.cuda.synchronize()\n    \n    # Benchmark float32 kernel\n    print("Benchmarking float32 attention kernel...")\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    for i in range(num_iterations):\n        output = float_wrapper.forward(q_f32, k_f32, v_f32)\n        torch.cuda.synchronize()\n    \n    float32_time = time.time() - start_time\n    float32_avg_time = float32_time / num_iterations\n    \n    # Benchmark half precision kernel\n    print("Benchmarking half precision attention kernel...")\n    torch.cuda.synchronize()\n    start_time = time.time()\n    \n    for i in range(num_iterations):\n        output = half_wrapper.forward(q_f16, k_f16, v_f16)\n        torch.cuda.synchronize()\n    \n    half_time = time.time() - start_time\n    half_avg_time = half_time / num_iterations\n    \n    # Calculate speedup\n    speedup = float32_avg_time / half_avg_time\n    \n    print(f"Results:")\n    print(f"  Float32: {float32_avg_time*1000:.3f} ms/iteration")\n    print(f"  Half precision: {half_avg_time*1000:.3f} ms/iteration")\n    print(f"  Speedup: {speedup:.2f}x")\n    \n    return float32_avg_time, half_avg_time, speedup\n\n\ndef run_comprehensive_benchmark():\n    """\n    Run comprehensive benchmark of all kernel optimizations\n    """\n    print("="*60)\n    print("COMPREHENSIVE CUDA KERNEL OPTIMIZATION BENCHMARK")\n    print("="*60)\n    \n    print("\n1. Attention Kernel Benchmark")\n    print("-" * 40)\n    original_time, optimized_time = benchmark_attention_kernels(\n        batch_size=2, seq_len=512, num_heads=8, head_dim=64\n    )\n    \n    print("\n2. Matrix Multiplication Benchmark")\n    print("-" * 40)\n    original_matmul_time, optimized_matmul_time = benchmark_matmul_kernels(\n        m=1024, n=1024, k=512\n    )\n    \n    print("\n3. Mixed Precision Attention Benchmark")\n    print("-" * 40)\n    float32_time, half_time, mp_speedup = benchmark_mixed_precision_attention(\n        batch_size=2, seq_len=512, num_heads=8, head_dim=64\n    )\n    \n    print("\n4. Additional Benchmark Configurations")\n    print("-" * 40)\n    \n    # Different attention configurations\n    configs = [\n        (1, 1024, 12, 64),  # Larger sequence length\n        (4, 256, 16, 64),   # More batch size and heads\n        (2, 256, 8, 128),   # Larger head dimension\n    ]\n    \n    for batch_size, seq_len, num_heads, head_dim in configs:\n        print(f"\nAttention config: batch={batch_size}, seq={seq_len}, heads={num_heads}, head_dim={head_dim}")\n        try:\n            orig_time, opt_time = benchmark_attention_kernels(\n                batch_size=batch_size, seq_len=seq_len, \n                num_heads=num_heads, head_dim=head_dim\n            )\n            speedup = orig_time / opt_time\n            print(f"  Speedup: {speedup:.2f}x")\n        except Exception as e:\n            print(f"  Error: {e}")\n    \n    print("\n" + "="*60)\n    print("BENCHMARK SUMMARY")\n    print("="*60)\n    \n    attention_speedup = original_time / optimized_time\n    matmul_speedup = original_matmul_time / optimized_matmul_time\n    \n    print(f"Attention kernel optimization: {attention_speedup:.2f}x speedup")\n    print(f"Matmul kernel optimization: {matmul_speedup:.2f}x speedup")\n    print(f"Mixed precision optimization: {mp_speedup:.2f}x speedup")\n    \n    print(f"\nOverall performance improvement: "\n          f"{(attention_speedup + matmul_speedup + mp_speedup) / 3:.2f}x average speedup")\n    \n    print("\nNote: Actual performance gains may vary depending on:")\n    print("- Hardware specifications (GPU memory bandwidth, compute capability)")\n    print("- Input tensor sizes and shapes")\n    print("- Memory access patterns")\n    print("- Compiler optimizations and register usage")\n\n\nif __name__ == "__main__":\n    # Set up logging\n    logging.basicConfig(level=logging.INFO)\n    \n    # Check CUDA availability\n    if not torch.cuda.is_available():\n        print("CUDA is not available. Please ensure you have a CUDA-capable GPU and PyTorch with CUDA support.")\n        exit(1)\n    \n    # Print GPU information\n    gpu_name = torch.cuda.get_device_name(0)\n    compute_capability = torch.cuda.get_device_capability(0)\n    print(f"GPU: {gpu_name}")\n    print(f"Compute Capability: {compute_capability[0]}.{compute_capability[1]}")\n    \n    # Run the comprehensive benchmark\n    run_comprehensive_benchmark()