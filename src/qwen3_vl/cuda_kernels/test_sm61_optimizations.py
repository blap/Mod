"""\nComprehensive Test Suite for SM61-Optimized CUDA Kernels\nTests all implemented functionality with proper validation for NVIDIA SM61 architecture\n"""\nimport unittest\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nimport os\nfrom typing import Optional, Dict, Any\nimport time\n\n# Import the SM61 CUDA kernels wrapper\nfrom cuda_kernels.cuda_wrapper import SM61KernelManager, SM61Attention, SM61MLP, SM61TransformerBlock, SM61OptimizedQwen3VLModel, create_sm61_optimized_model, get_hardware_info, test_sm61_kernels\n    SM61KernelManager,\n    SM61Attention,\n    SM61MLP,\n    SM61TransformerBlock,\n    SM61OptimizedQwen3VLModel,\n    create_sm61_optimized_model,\n    get_hardware_info,\n    test_sm61_kernels\n)\n\nclass TestSM61CUDAIntegration(unittest.TestCase):\n    """Test SM61 CUDA kernel integration and functionality"""\n    \n    def setUp(self):\n        """Set up test environment"""\n        self.kernel_manager = SM61KernelManager()\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Set up test parameters for SM61\n        self.batch_size = 2\n        self.seq_len = 64\n        self.num_heads = 8\n        self.head_dim = 64\n        self.hidden_dim = self.num_heads * self.head_dim\n        self.intermediate_dim = self.hidden_dim * 4\n        \n        # Test hardware info\n        self.hw_info = get_hardware_info()\n    \n    def test_sm61_hardware_detection(self):\n        """Test that hardware detection works correctly"""\n        self.assertIsInstance(self.hw_info, dict)\n        self.assertIn('cuda_available', self.hw_info)\n        \n        # Check if we have CUDA available\n        if self.hw_info['cuda_available']:\n            self.assertIn('compute_capability', self.hw_info)\n            self.assertIn('total_memory_gb', self.hw_info)\n            \n            # Verify we're detecting SM61 or compatible architecture\n            capability = self.hw_info.get('compute_capability', (0, 0))\n            is_sm61_compatible = capability[0] >= 6  # SM61 is compute capability 6.1\n            print(f"Compute capability: {capability}, SM61 compatible: {is_sm61_compatible}")\n    \n    def test_kernel_manager_initialization(self):\n        """Test that the kernel manager initializes correctly"""\n        self.assertIsInstance(self.kernel_manager, SM61KernelManager)\n        self.assertEqual(self.kernel_manager.cuda_available, torch.cuda.is_available())\n    \n    def test_scaled_dot_product_attention_kernel(self):\n        """Test the SM61-optimized scaled dot-product attention kernel"""\n        if not torch.cuda.is_available():\n            self.skipTest("CUDA not available")\n        \n        # Create test tensors\n        query = torch.randn(\n            self.batch_size, self.num_heads, self.seq_len, self.head_dim,\n            device=self.device, dtype=torch.float16\n        )\n        key = torch.randn(\n            self.batch_size, self.num_heads, self.seq_len, self.head_dim,\n            device=self.device, dtype=torch.float16\n        )\n        value = torch.randn(\n            self.batch_size, self.num_heads, self.seq_len, self.head_dim,\n            device=self.device, dtype=torch.float16\n        )\n        \n        # Test SM61-optimized attention\n        start_time = time.time()\n        output = self.kernel_manager.scaled_dot_product_attention(query, key, value)\n        elapsed_time = time.time() - start_time\n        \n        # Validate output shape\n        self.assertEqual(output.shape, query.shape)\n        self.assertEqual(output.device, query.device)\n        print(f"SM61 attention kernel time: {elapsed_time:.4f}s")\n        \n        # Compare with PyTorch implementation for correctness\n        pytorch_output = torch.nn.functional.scaled_dot_product_attention(\n            query, key, value, dropout_p=0.0, is_causal=False\n        )\n        \n        # Allow for small numerical differences due to different implementations\n        max_diff = torch.max(torch.abs(output - pytorch_output)).item()\n        print(f"Max difference from PyTorch implementation: {max_diff}")\n        self.assertLess(max_diff, 1e-2, f"Output differs too much from PyTorch: {max_diff}")\n    \n    def test_high_performance_matmul_kernel(self):\n        """Test the SM61-optimized high-performance matmul kernel"""\n        if not torch.cuda.is_available():\n            self.skipTest("CUDA not available")\n        \n        # Create test tensors\n        m, n, k = 256, 512, 128\n        a = torch.randn(m, k, device=self.device, dtype=torch.float16)\n        b = torch.randn(k, n, device=self.device, dtype=torch.float16)\n        \n        # Test SM61-optimized matmul\n        start_time = time.time()\n        result = self.kernel_manager.high_performance_matmul(a, b)\n        elapsed_time = time.time() - start_time\n        \n        # Validate output shape\n        self.assertEqual(result.shape, (m, n))\n        self.assertEqual(result.device, a.device)\n        print(f"SM61 matmul kernel time: {elapsed_time:.4f}s")\n        \n        # Compare with PyTorch implementation\n        pytorch_result = torch.matmul(a, b)\n        max_diff = torch.max(torch.abs(result - pytorch_result)).item()\n        print(f"Max difference from PyTorch matmul: {max_diff}")\n        self.assertLess(max_diff, 1e-2, f"Output differs too much from PyTorch: {max_diff}")\n    \n    def test_memory_efficient_operations_kernel(self):\n        """Test the SM61-optimized memory-efficient operations kernel"""\n        if not torch.cuda.is_available():\n            self.skipTest("CUDA not available")\n        \n        # Create test tensors\n        batch_size, seq_len, hidden_dim = 4, 32, 256\n        input_tensor = torch.randn(batch_size, seq_len, hidden_dim, device=self.device, dtype=torch.float16)\n        weight = torch.randn(hidden_dim, hidden_dim, device=self.device, dtype=torch.float16)\n        \n        # Test different operation types\n        for op_type in [0, 1, 2, 3]:  # matmul, add, mul, activation\n            start_time = time.time()\n            result = self.kernel_manager.memory_efficient_ops(input_tensor, weight, op_type)\n            elapsed_time = time.time() - start_time\n            \n            # Validate output shape\n            self.assertEqual(result.shape, input_tensor.shape)\n            self.assertEqual(result.device, input_tensor.device)\n            print(f"SM61 memory-efficient op {op_type} time: {elapsed_time:.4f}s")\n    \n    def test_coalesced_copy_kernel(self):\n        """Test the SM61-optimized coalesced memory copy kernel"""\n        if not torch.cuda.is_available():\n            self.skipTest("CUDA not available")\n        \n        # Create test tensor\n        tensor = torch.randn(100, 100, device=self.device, dtype=torch.float16)\n        \n        # Test coalesced copy\n        start_time = time.time()\n        copied_tensor = self.kernel_manager.coalesced_copy(tensor)\n        elapsed_time = time.time() - start_time\n        \n        # Validate copy correctness\n        self.assertTrue(torch.equal(tensor, copied_tensor))\n        self.assertEqual(copied_tensor.device, tensor.device)\n        print(f"SM61 coalesced copy time: {elapsed_time:.4f}s")\n    \n    def test_transpose_kernel(self):\n        """Test the SM61-optimized transpose kernel with bank conflict avoidance"""\n        if not torch.cuda.is_available():\n            self.skipTest("CUDA not available")\n        \n        # Create test tensor\n        tensor = torch.randn(128, 256, device=self.device, dtype=torch.float16)\n        \n        # Test transpose\n        start_time = time.time()\n        transposed_tensor = self.kernel_manager.transpose(tensor)\n        elapsed_time = time.time() - start_time\n        \n        # Validate transpose correctness\n        expected_shape = (tensor.shape[1], tensor.shape[0])\n        self.assertEqual(transposed_tensor.shape, expected_shape)\n        self.assertTrue(torch.allclose(transposed_tensor, tensor.transpose(-2, -1), atol=1e-3))\n        print(f"SM61 transpose kernel time: {elapsed_time:.4f}s")\n    \n    def test_memory_pool_allocation(self):\n        """Test the SM61-optimized memory pool allocation"""\n        # Test tensor allocation from pool\n        sizes = (64, 128)\n        dtype = torch.float16\n        \n        try:\n            tensor = self.kernel_manager.allocate_tensor_from_pool(sizes, dtype)\n            self.assertEqual(tensor.shape, sizes)\n            self.assertEqual(tensor.dtype, dtype)\n            if torch.cuda.is_available():\n                self.assertEqual(tensor.device.type, 'cuda')\n            print("Memory pool allocation test passed")\n        except Exception as e:\n            # If memory pool is not available, that's acceptable\n            print(f"Memory pool not available (expected in some environments): {e}")\n    \n    def test_memory_pool_statistics(self):\n        """Test the memory pool statistics functionality"""\n        stats = self.kernel_manager.get_memory_pool_stats()\n        if stats is not None:\n            self.assertIsInstance(stats, dict)\n            self.assertIn('total_size', stats)\n            self.assertIn('allocated', stats)\n            print("Memory pool statistics test passed")\n        else:\n            print("Memory pool statistics not available (expected if memory pool not initialized)")\n\n\nclass TestSM61OptimizedModules(unittest.TestCase):\n    """Test SM61-optimized neural network modules"""\n    \n    def setUp(self):\n        """Set up test environment"""\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Test parameters\n        self.hidden_dim = 512\n        self.num_heads = 8\n        self.intermediate_dim = 2048\n        self.seq_len = 32\n        self.batch_size = 2\n    \n    def test_sm61_attention_module(self):\n        """Test the SM61-optimized attention module"""\n        attention = SM61Attention(\n            embed_dim=self.hidden_dim,\n            num_heads=self.num_heads,\n            dropout=0.1\n        ).to(self.device)\n        \n        # Create test inputs\n        query = torch.randn(self.seq_len, self.batch_size, self.hidden_dim, device=self.device)\n        key = torch.randn(self.seq_len, self.batch_size, self.hidden_dim, device=self.device)\n        value = torch.randn(self.seq_len, self.batch_size, self.hidden_dim, device=self.device)\n        \n        # Forward pass\n        output, attn_weights = attention(query, key, value, need_weights=True)\n        \n        # Validate output shape\n        self.assertEqual(output.shape, (self.seq_len, self.batch_size, self.hidden_dim))\n        if attn_weights is not None:\n            expected_attn_shape = (self.batch_size * self.num_heads, self.seq_len, self.seq_len)\n            self.assertEqual(attn_weights.shape, expected_attn_shape)\n        print("SM61 attention module test passed")\n    \n    def test_sm61_mlp_module(self):\n        """Test the SM61-optimized MLP module"""\n        mlp = SM61MLP(\n            hidden_dim=self.hidden_dim,\n            intermediate_dim=self.intermediate_dim,\n            activation="silu"\n        ).to(self.device)\n        \n        # Create test input\n        x = torch.randn(self.batch_size, self.seq_len, self.hidden_dim, device=self.device)\n        \n        # Forward pass\n        output = mlp(x)\n        \n        # Validate output shape\n        self.assertEqual(output.shape, x.shape)\n        print("SM61 MLP module test passed")\n    \n    def test_sm61_transformer_block(self):\n        """Test the SM61-optimized transformer block"""\n        block = SM61TransformerBlock(\n            hidden_dim=self.hidden_dim,\n            num_heads=self.num_heads,\n            intermediate_dim=self.intermediate_dim,\n            attention_dropout=0.1,\n            mlp_dropout=0.1\n        ).to(self.device)\n        \n        # Create test input\n        x = torch.randn(self.batch_size, self.seq_len, self.hidden_dim, device=self.device)\n        \n        # Forward pass\n        output = block(x)\n        \n        # Validate output shape\n        self.assertEqual(output.shape, x.shape)\n        print("SM61 transformer block test passed")\n\n\nclass TestSM61ModelIntegration(unittest.TestCase):\n    """Test the complete SM61-optimized model integration"""\n    \n    def setUp(self):\n        """Set up test environment"""\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Create a minimal config for testing\n        class MinimalConfig:\n            hidden_size = 512\n            num_attention_heads = 8\n            num_hidden_layers = 2\n            vocab_size = 1000\n            intermediate_size = 2048\n            attention_dropout = 0.1\n            hidden_dropout = 0.1\n        \n        self.config = MinimalConfig()\n    \n    def test_sm61_optimized_model_creation(self):\n        """Test that the SM61-optimized model can be created"""\n        model = create_sm61_optimized_model(self.config)\n        self.assertIsInstance(model, SM61OptimizedQwen3VLModel)\n        print("SM61 optimized model creation test passed")\n    \n    def test_sm61_model_forward_pass(self):\n        """Test forward pass of the SM61-optimized model"""\n        model = create_sm61_optimized_model(self.config).to(self.device)\n        \n        # Create test inputs\n        batch_size, seq_len = 2, 32\n        input_ids = torch.randint(0, self.config.vocab_size, (batch_size, seq_len), device=self.device)\n        \n        # Forward pass\n        with torch.no_grad():  # Don't compute gradients for this test\n            output = model(input_ids)\n        \n        # Validate output shape\n        expected_shape = (batch_size, seq_len, self.config.vocab_size)\n        self.assertEqual(output.shape, expected_shape)\n        print("SM61 model forward pass test passed")\n\n\nclass TestSM61ErrorHandling(unittest.TestCase):\n    """Test error handling and fallback mechanisms"""\n    \n    def setUp(self):\n        """Set up test environment"""\n        self.kernel_manager = SM61KernelManager()\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    def test_cuda_fallback_mechanisms(self):\n        """Test that fallback mechanisms work when CUDA operations fail"""\n        # Create tensors on CPU to test fallback behavior\n        query = torch.randn(2, 8, 32, 64, device='cpu')  # Use CPU instead of CUDA\n        key = torch.randn(2, 8, 32, 64, device='cpu')\n        value = torch.randn(2, 8, 32, 64, device='cpu')\n        \n        # This should fall back to PyTorch implementation\n        output = self.kernel_manager.scaled_dot_product_attention(query, key, value)\n        \n        # Validate output shape\n        self.assertEqual(output.shape, query.shape)\n        self.assertEqual(output.device, query.device)\n        print("CUDA fallback mechanism test passed")\n    \n    def test_invalid_tensor_shapes(self):\n        """Test error handling for invalid tensor shapes"""\n        if not torch.cuda.is_available():\n            self.skipTest("CUDA not available")\n        \n        # Create tensors with incompatible shapes\n        query = torch.randn(2, 8, 32, 64, device=self.device)\n        key = torch.randn(2, 8, 16, 64, device=self.device)  # Different sequence length\n        value = torch.randn(2, 8, 32, 64, device=self.device)\n        \n        # This should handle the shape mismatch gracefully\n        try:\n            output = self.kernel_manager.scaled_dot_product_attention(query, key, value)\n            # If it doesn't raise an error, check that it produces reasonable output\n            self.assertEqual(output.shape, query.shape)\n        except Exception as e:\n            # If an error is raised, it should be handled appropriately\n            print(f"Shape mismatch handled with error: {e}")\n    \n    def test_memory_pool_overflow(self):\n        """Test memory pool behavior with large allocations"""\n        if not torch.cuda.is_available():\n            self.skipTest("CUDA not available")\n        \n        # Try to allocate a very large tensor\n        huge_size = (10000, 10000)  # This will likely exceed GPU memory\n        dtype = torch.float16\n        \n        try:\n            # This might fail due to memory constraints, which is expected\n            tensor = self.kernel_manager.allocate_tensor_from_pool(huge_size, dtype)\n            if tensor is not None:\n                self.assertEqual(tensor.shape, huge_size)\n                print("Large tensor allocation handled")\n        except Exception as e:\n            # Expected to fail in many cases due to memory constraints\n            print(f"Large tensor allocation failed as expected: {e}")\n\n\ndef run_comprehensive_tests():\n    """Run all SM61 CUDA optimization tests"""\n    print("="*60)\n    print("COMPREHENSIVE SM61 CUDA OPTIMIZATION TEST SUITE")\n    print("="*60)\n    \n    # Test hardware detection\n    print("\n1. Testing Hardware Detection...")\n    hw_info = get_hardware_info()\n    print(f"   CUDA Available: {hw_info.get('cuda_available', 'N/A')}")\n    print(f"   Compute Capability: {hw_info.get('compute_capability', 'N/A')}")\n    print(f"   Total GPU Memory: {hw_info.get('total_memory_gb', 'N/A')} GB")\n    print(f"   Is SM61 Compatible: {hw_info.get('is_sm61', 'N/A')}")\n    \n    # Run integration tests\n    print("\n2. Running CUDA Integration Tests...")\n    integration_suite = unittest.TestLoader().loadTestsFromTestCase(TestSM61CUDAIntegration)\n    integration_runner = unittest.TextTestRunner(verbosity=2)\n    integration_result = integration_runner.run(integration_suite)\n    \n    # Run module tests\n    print("\n3. Running Module Tests...")\n    module_suite = unittest.TestLoader().loadTestsFromTestCase(TestSM61OptimizedModules)\n    module_runner = unittest.TextTestRunner(verbosity=2)\n    module_result = module_runner.run(module_suite)\n    \n    # Run model integration tests\n    print("\n4. Running Model Integration Tests...")\n    model_suite = unittest.TestLoader().loadTestsFromTestCase(TestSM61ModelIntegration)\n    model_runner = unittest.TextTestRunner(verbosity=2)\n    model_result = model_runner.run(model_suite)\n    \n    # Run error handling tests\n    print("\n5. Running Error Handling Tests...")\n    error_suite = unittest.TestLoader().loadTestsFromTestCase(TestSM61ErrorHandling)\n    error_runner = unittest.TextTestRunner(verbosity=2)\n    error_result = error_runner.run(error_suite)\n    \n    # Summary\n    total_tests = (\n        integration_result.testsRun + \n        module_result.testsRun + \n        model_result.testsRun + \n        error_result.testsRun\n    )\n    total_failures = (\n        len(integration_result.failures) + \n        len(module_result.failures) + \n        len(model_result.failures) + \n        len(error_result.failures)\n    )\n    total_errors = (\n        len(integration_result.errors) + \n        len(module_result.errors) + \n        len(model_result.errors) + \n        len(error_result.errors)\n    )\n    \n    print("\n" + "="*60)\n    print("TEST RESULTS SUMMARY")\n    print("="*60)\n    print(f"Total Tests Run: {total_tests}")\n    print(f"Failures: {total_failures}")\n    print(f"Errors: {total_errors}")\n    \n    if total_failures == 0 and total_errors == 0:\n        print("\n✅ ALL TESTS PASSED! SM61 CUDA optimizations are working correctly.")\n        \n        # Run the kernel functionality test\n        print("\n6. Testing SM61 Kernel Functionality...")\n        kernel_test_success = test_sm61_kernels()\n        if kernel_test_success:\n            print("✅ SM61 kernel functionality test passed!")\n        else:\n            print("❌ SM61 kernel functionality test failed!")\n        \n        return True\n    else:\n        print(f"\n❌ {total_failures + total_errors} tests failed or had errors.")\n        return False\n\n\nif __name__ == "__main__":\n    success = run_comprehensive_tests()\n    sys.exit(0 if success else 1)