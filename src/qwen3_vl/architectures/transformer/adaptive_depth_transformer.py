"""\nAdaptive Depth Transformer Implementation for Qwen3-VL Architecture\nThis module implements the core adaptive depth mechanism that adjusts the number\nof transformer layers used based on input complexity.\n"""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Union\nfrom config.config import Qwen3VLConfig\nfrom optimization.adaptive_depth import InputComplexityAssessor, AdaptiveDepthController\n\n\nclass AdaptiveDepthTransformer(nn.Module):\n    """\n    Transformer with adaptive depth mechanism that adjusts the number of layers\n    used based on input complexity assessment.\n    """\n    def __init__(self, config: Qwen3VLConfig, complexity_assessor: InputComplexityAssessor):\n        super().__init__()\n        self.config = config\n        self.complexity_assessor = complexity_assessor\n        self.adaptive_controller = AdaptiveDepthController(config, complexity_assessor)\n\n        # Import inside the method to avoid circular import\nfrom architectures.architectures.modeling_qwen3_vl_phase2 import Qwen3VLDecoderLayer\n\n        # Create the transformer layers\n        self.layers = nn.ModuleList([\n            Qwen3VLDecoderLayer(config, layer_idx)\n            for layer_idx in range(config.num_hidden_layers)\n        ])\n        \n        # Layer selection mechanism\n        self.layer_gating = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(config.hidden_size, config.hidden_size // 4),\n                nn.ReLU(),\n                nn.Linear(config.hidden_size // 4, 2),  # gate and confidence\n                nn.Softmax(dim=-1)\n            ) for _ in range(config.num_hidden_layers)\n        ])\n        \n        # Complexity threshold for early exit\n        self.exit_threshold = getattr(config, 'exit_threshold', 0.8)\n        \n        # Layer utilization tracking (for analysis)\n        self.register_buffer('layer_utilization', \n                           torch.zeros(config.num_hidden_layers), persistent=False)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        input_ids: Optional[torch.LongTensor] = None,  # For text complexity\n        pixel_values: Optional[torch.FloatTensor] = None,  # For image complexity\n        return_depth_used: bool = False,\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, int]]:\n        """\n        Forward pass with adaptive depth selection.\n        \n        Args:\n            hidden_states: Input hidden states\n            attention_mask: Attention mask\n            position_ids: Position IDs\n            past_key_values: Past key values for caching\n            output_attentions: Whether to output attention weights\n            use_cache: Whether to use cache\n            input_ids: Input token IDs for complexity assessment\n            pixel_values: Pixel values for complexity assessment\n            return_depth_used: Whether to return the number of layers used\n        \n        Returns:\n            Output hidden states, optionally with number of layers used\n        """\n        # Determine number of layers to use based on input complexity\n        num_layers_to_use, complexity_score = self.adaptive_controller(input_ids, pixel_values)\n        \n        all_hidden_states = () if output_attentions else None\n        all_self_attns = () if output_attentions else None\n        \n        current_hidden = hidden_states\n        \n        # Track which layers were actually used\n        layers_used = 0\n        \n        for layer_idx in range(min(num_layers_to_use, len(self.layers))):\n            layer = self.layers[layer_idx]\n            \n            # Apply the layer\n            layer_outputs = layer(\n                current_hidden,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_values[layer_idx] if past_key_values is not None else None,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n            )\n            \n            current_hidden = layer_outputs[0]\n            layers_used += 1\n            \n            # Update layer utilization statistics\n            if self.training:  # Only update during training\n                self.layer_utilization[layer_idx] = self.layer_utilization[layer_idx] * 0.9 + 0.1  # Moving average\n            \n            if output_attentions:\n                all_hidden_states += (current_hidden,)\n                if len(layer_outputs) > 1:\n                    all_self_attns += (layer_outputs[1],)\n        \n        # If we're using fewer layers than total, we can return early\n        # This provides computational savings for simpler inputs\n        \n        if return_depth_used:\n            return current_hidden, layers_used\n        else:\n            return current_hidden\n\n\nclass VisionAdaptiveDepthTransformer(nn.Module):\n    """\n    Adaptive depth mechanism for vision transformer components.\n    This is primarily a container for the complexity-to-depth mapping components.\n    """\n    def __init__(self, config: Qwen3VLConfig, complexity_assessor: InputComplexityAssessor):\n        super().__init__()\n        self.config = config\n        self.complexity_assessor = complexity_assessor\n\n        # Vision-specific complexity to depth mapping\n        self.vision_complexity_to_depth = nn.Sequential(\n            nn.Linear(1, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1),\n            nn.Sigmoid()\n        )\n\n        self.min_depth_ratio = getattr(config, 'vision_min_depth_ratio', 0.3)\n        self.max_depth_ratio = getattr(config, 'vision_max_depth_ratio', 1.0)\n\n    def forward(\n        self,\n        hidden_states: torch.FloatTensor,\n        pixel_values: torch.FloatTensor,\n        layers: nn.ModuleList,  # The actual vision transformer layers\n        return_depth_used: bool = False,\n    ) -> Union[torch.FloatTensor, Tuple[torch.FloatTensor, int]]:\n        """\n        Forward pass with adaptive depth selection for vision components.\n        This method is meant to be called from within the main vision transformer\n        after patch embedding is done.\n        """\n        # Assess image complexity based on the original pixel values\n        complexity_score = self.complexity_assessor.assess_image_complexity(pixel_values)\n\n        # Ensure complexity score is at least 1D for linear layer\n        if complexity_score.dim() == 0:\n            complexity_score = complexity_score.unsqueeze(0)  # Make it 1D\n\n        # Calculate target depth based on complexity\n        depth_ratio = self.vision_complexity_to_depth(complexity_score)\n        depth_ratio = torch.clamp(depth_ratio, self.min_depth_ratio, self.max_depth_ratio)\n        target_depth = int(torch.round(depth_ratio * self.config.vision_num_hidden_layers).item())\n        num_layers_to_use = max(1, min(target_depth, self.config.vision_num_hidden_layers))\n\n        # Process through the required number of vision transformer layers\n        layers_used = 0\n        current_hidden = hidden_states\n\n        for layer_idx in range(num_layers_to_use):\n            layer = layers[layer_idx]\n            current_hidden = layer(current_hidden)\n            layers_used += 1\n\n        if return_depth_used:\n            return current_hidden, layers_used\n        else:\n            return current_hidden\n\n\nclass MultimodalAdaptiveDepthFusion(nn.Module):\n    """\n    Adaptive depth mechanism for fusing vision and language components.\n    """\n    def __init__(self, config: Qwen3VLConfig, complexity_assessor: InputComplexityAssessor):\n        super().__init__()\n        self.config = config\n        self.complexity_assessor = complexity_assessor\n        \n        # Fusion layers\n        self.fusion_layers = nn.ModuleList([\n            nn.TransformerEncoderLayer(\n                d_model=config.hidden_size,\n                nhead=min(8, config.num_attention_heads),  # Use fewer heads for fusion\n                batch_first=True,\n                dropout=0.1\n            ) for _ in range(config.num_hidden_layers // 2)  # Fewer fusion layers\n        ])\n        \n        # Complexity-based fusion depth controller\n        self.fusion_complexity_to_depth = nn.Sequential(\n            nn.Linear(1, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1),\n            nn.Sigmoid()\n        )\n        \n        self.min_fusion_depth = 1\n        self.max_fusion_depth = len(self.fusion_layers)\n\n    def forward(\n        self,\n        vision_features: torch.Tensor,\n        language_features: torch.Tensor,\n        input_ids: Optional[torch.LongTensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        return_depth_used: bool = False,\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, int]]:\n        """\n        Forward pass to fuse vision and language features with adaptive depth.\n        """\n        # Assess multimodal complexity\n        if input_ids is not None and pixel_values is not None:\n            complexity_score = self.complexity_assessor.assess_multimodal_complexity(input_ids, pixel_values)\n        elif pixel_values is not None:\n            complexity_score = self.complexity_assessor.assess_image_complexity(pixel_values)\n        elif input_ids is not None:\n            complexity_score = self.complexity_assessor.assess_text_complexity(input_ids)\n        else:\n            complexity_score = torch.tensor(0.5, device=vision_features.device)\n        \n        # Calculate fusion depth\n        fusion_depth_ratio = self.fusion_complexity_to_depth(complexity_score)\n        target_fusion_depth = int(torch.round(\n            fusion_depth_ratio * (self.max_fusion_depth - self.min_fusion_depth) + self.min_fusion_depth\n        ).item())\n        \n        num_fusion_layers = max(self.min_fusion_depth, \n                               min(target_fusion_depth, self.max_fusion_depth))\n        \n        # Combine vision and language features\n        # Simple concatenation approach - in practice this would be more sophisticated\n        combined_features = torch.cat([vision_features, language_features], dim=1)\n        \n        # Apply fusion layers adaptively\n        current_features = combined_features\n        layers_used = 0\n        \n        for layer_idx in range(num_fusion_layers):\n            fusion_layer = self.fusion_layers[layer_idx]\n            current_features = fusion_layer(current_features)\n            layers_used += 1\n        \n        # Split back into vision and language components if needed\n        vision_len = vision_features.size(1)\n        output_vision = current_features[:, :vision_len, :]\n        output_language = current_features[:, vision_len:, :]\n        \n        # Return combined or separate outputs based on requirements\n        if return_depth_used:\n            return output_language, layers_used  # Return language output and depth used\n        else:\n            return output_language\n\n\ndef apply_adaptive_depth_with_early_exit(\n    transformer_layers: nn.ModuleList,\n    hidden_states: torch.Tensor,\n    complexity_score: float,\n    exit_threshold: float = 0.8,\n    min_layers: int = 1,\n    max_layers: int = None\n) -> Tuple[torch.Tensor, int]:\n    """\n    Apply transformer layers with early exit based on complexity and confidence.\n    \n    Args:\n        transformer_layers: List of transformer layers to apply\n        hidden_states: Input hidden states\n        complexity_score: Complexity score to determine layer usage\n        exit_threshold: Threshold for early exit based on confidence\n        min_layers: Minimum number of layers to apply\n        max_layers: Maximum number of layers to apply (defaults to all)\n    \n    Returns:\n        Tuple of (output hidden states, number of layers used)\n    """\n    if max_layers is None:\n        max_layers = len(transformer_layers)\n    \n    # Calculate number of layers to use based on complexity\n    # Higher complexity may need more layers, but with early exit capability\n    num_layers_to_try = max(min_layers, min(max_layers, len(transformer_layers)))\n    \n    current_hidden = hidden_states\n    layers_used = 0\n    \n    for layer_idx in range(num_layers_to_try):\n        layer = transformer_layers[layer_idx]\n        \n        # Apply the layer\n        layer_output = layer(current_hidden)\n        if isinstance(layer_output, tuple):\n            current_hidden = layer_output[0]\n        else:\n            current_hidden = layer_output\n        \n        layers_used += 1\n        \n        # Check for early exit based on complexity and confidence\n        # For simplicity, we'll use a basic heuristic based on complexity\n        if layer_idx >= min_layers - 1:  # At least use minimum layers\n            # Calculate a simple confidence measure based on output variance\n            output_variance = current_hidden.var(dim=-1).mean()\n            confidence = torch.sigmoid(output_variance)  # Convert to [0,1] range\n            \n            if confidence > exit_threshold and complexity_score < 0.5:  # Simple heuristic\n                break  # Early exit for low complexity inputs with high confidence\n    \n    return current_hidden, layers_used