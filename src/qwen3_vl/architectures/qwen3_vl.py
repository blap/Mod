"""\nMain Qwen3-VL model interface\n"""\nfrom typing import Optional, Union\nimport torch\nfrom transformers import PreTrainedModel\nfrom transformers.modeling_outputs import BaseModelOutputWithPast\n\nfrom config.config import Qwen3VLConfig\nfrom architectures.modeling_qwen3_vl_integrated import Qwen3VLForConditionalGeneration\n\n\nclass Qwen3VLProcessor:\n    """\n    Processor for Qwen3-VL model that handles both text and image preprocessing.\n    """\n    def __init__(self, tokenizer, image_processor):\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n\n    def __call__(self, \n                 text: Optional[str] = None, \n                 images=None, \n                 return_tensors: str = "pt", \n                 padding: bool = True,\n                 truncation: bool = True,\n                 max_length: Optional[int] = None,\n                 **kwargs):\n        """\n        Process text and images for the Qwen3-VL model.\n        \n        Args:\n            text: Input text to process\n            images: Input images to process\n            return_tensors: Type of tensors to return ("pt", "np", etc.)\n            padding: Whether to pad sequences\n            truncation: Whether to truncate sequences\n            max_length: Maximum sequence length\n            **kwargs: Additional arguments\n        \n        Returns:\n            Dictionary containing processed inputs\n        """\n        inputs = {}\n        \n        # Process text if provided\n        if text is not None:\n            tokenized = self.tokenizer(\n                text,\n                return_tensors=return_tensors,\n                padding=padding,\n                truncation=truncation,\n                max_length=max_length,\n                **kwargs\n            )\n            inputs.update(tokenized)\n        \n        # Process images if provided\n        if images is not None:\n            image_processed = self.image_processor(\n                images=images,\n                return_tensors=return_tensors,\n                **kwargs\n            )\n            inputs.update(image_processed)\n        \n        return inputs\n\n\nclass Qwen3VLModel:\n    """\n    Main Qwen3-VL model class that provides a high-level interface for loading and inference.\n    Maintains full capacity with 32 transformer layers and 32 attention heads.\n    """\n    def __init__(self, config: Qwen3VLConfig, model: Qwen3VLForConditionalGeneration):\n        self.config = config\n        self.model = model\n        self.device = next(model.parameters()).device\n\n    @classmethod\n    def from_pretrained(\n        cls, \n        pretrained_model_name_or_path: str,\n        config: Optional[Qwen3VLConfig] = None,\n        torch_dtype: Optional[torch.dtype] = None,\n        device_map: Optional[Union[str, dict]] = None,\n        low_cpu_mem_usage: bool = False,\n        **kwargs\n    ):\n        """\n        Load a pretrained Qwen3-VL model.\n        \n        Args:\n            pretrained_model_name_or_path: Path to pretrained model or model identifier\n            config: Model configuration (optional)\n            torch_dtype: Torch data type for model\n            device_map: Device mapping for model sharding\n            low_cpu_mem_usage: Whether to use low CPU memory loading\n            **kwargs: Additional arguments\n        \n        Returns:\n            Qwen3VLModel instance\n        """\n        # Load configuration if not provided\n        if config is None:\n            config = Qwen3VLConfig.from_pretrained(pretrained_model_name_or_path)\n        \n        # Update config with provided arguments\n        if torch_dtype is not None:\n            config.torch_dtype = str(torch_dtype).replace('torch.', '')\n        \n        if 'use_gradient_checkpointing' in kwargs:\n            config.use_gradient_checkpointing = kwargs['use_gradient_checkpointing']\n        \n        # Load the actual model\n        model = Qwen3VLForConditionalGeneration.from_pretrained(\n            pretrained_model_name_or_path,\n            config=config,\n            torch_dtype=torch_dtype,\n            device_map=device_map,\n            low_cpu_mem_usage=low_cpu_mem_usage,\n            **kwargs\n        )\n        \n        return cls(config, model)\n\n    def to(self, device):\n        """Move model to specified device."""\n        self.model = self.model.to(device)\n        self.device = device\n        return self\n\n    def eval(self):\n        """Set model to evaluation mode."""\n        self.model.eval()\n        return self\n\n    def train(self, mode: bool = True):\n        """Set model to training mode."""\n        self.model.train(mode)\n        return self\n\n    def generate(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        pixel_values: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        **kwargs\n    ):\n        """\n        Generate text using the Qwen3-VL model.\n        \n        Args:\n            input_ids: Input token IDs\n            pixel_values: Input pixel values for images\n            attention_mask: Attention mask\n            **kwargs: Additional generation arguments\n        \n        Returns:\n            Generated token IDs\n        """\n        with torch.no_grad():\n            return self.model.generate(\n                input_ids=input_ids,\n                pixel_values=pixel_values,\n                attention_mask=attention_mask,\n                **kwargs\n            )\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        pixel_values: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        **kwargs\n    ):\n        """\n        Forward pass through the model.\n        \n        Args:\n            input_ids: Input token IDs\n            pixel_values: Input pixel values for images\n            attention_mask: Attention mask\n            **kwargs: Additional arguments\n        \n        Returns:\n            Model outputs\n        """\n        return self.model(\n            input_ids=input_ids,\n            pixel_values=pixel_values,\n            attention_mask=attention_mask,\n            **kwargs\n        )\n\n    def save_pretrained(self, save_directory: str, **kwargs):\n        """Save the model to a directory."""\n        self.model.save_pretrained(save_directory, **kwargs)\n\n    @property\n    def num_parameters(self):\n        """Get the number of parameters in the model."""\n        return sum(p.numel() for p in self.model.parameters())\n\n\ndef load_qwen3_vl_model(\n    model_name_or_path: str,\n    device: Optional[Union[str, torch.device]] = None,\n    torch_dtype: Optional[torch.dtype] = None,\n    use_gradient_checkpointing: bool = False\n):\n    """\n    Convenience function to load a Qwen3-VL model.\n    \n    Args:\n        model_name_or_path: Path to model or model identifier\n        device: Device to load model on\n        torch_dtype: Torch data type for model\n        use_gradient_checkpointing: Whether to use gradient checkpointing\n    \n    Returns:\n        Tuple of (model, processor)\n    """\n    # Create configuration\n    config = Qwen3VLConfig(\n        use_gradient_checkpointing=use_gradient_checkpointing\n    )\n    \n    # Load model\n    model = Qwen3VLModel.from_pretrained(\n        model_name_or_path,\n        config=config,\n        torch_dtype=torch_dtype\n    )\n    \n    # Move to specified device\n    if device is not None:\n        model = model.to(device)\n    elif torch.cuda.is_available():\n        model = model.to('cuda')\n    else:\n        model = model.to('cpu')\n    \n    # Load actual tokenizer and image processor\n    from transformers import AutoTokenizer, AutoImageProcessor\n\n    try:\n        # Try to use the model's own tokenizer and image processor if available\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        image_processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n    except:\n        # Fallback to standard models if specific ones aren't available\n        tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")\n        image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")\n\n    # Ensure tokenizer has pad token\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    processor = Qwen3VLProcessor(tokenizer, image_processor)\n\n    return model, processor