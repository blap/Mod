"""\nQwen3-VL Model with Integrated Memory Optimizations\n\nThis module implements the Qwen3-VL model with all memory optimization techniques\nintegrated: pooling, caching, compression, swapping, tiering, and garbage collection.\nDesigned for Intel i5-10210U + NVIDIA SM61 + NVMe SSD hardware.\n"""\nimport math\nimport warnings\nfrom typing import Optional, Tuple, Union, List\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom src.qwen3_vl.config import Qwen3VLConfig\nfrom architectures.linear_attention import PerformerAttention\nfrom architectures.device_aware_module import DeviceAwareAttention\nfrom architectures.gradient_checkpointing import MemoryEfficientAttention, MemoryEfficientMLP\nfrom architectures.adaptive_computation import AdaptiveAttention, AdaptiveMLP\nfrom architectures.memory_management import OptimizedQwen3VLAttention\nfrom src.qwen3_vl.optimization.integrated_memory_manager import IntegratedMemoryManager, create_optimized_memory_manager, MemoryOptimizedContext\n\n\nclass Qwen3VLPreTrainedModel(nn.Module):\n    """\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n    """\n    config_class = Qwen3VLConfig\n    base_model_prefix = "qwen3_vl"\n    supports_gradient_checkpointing = True\n    _no_split_modules = ["Qwen3VLDecoderLayer", "Qwen3VLVisionLayer"]\n\n    def __init__(self, config: Qwen3VLConfig, memory_manager: Optional[IntegratedMemoryManager] = None):\n        super().__init__()\n        self.config = config\n        self.memory_manager = memory_manager or create_optimized_memory_manager({\n            'cpu_model': 'Intel i5-10210U',\n            'gpu_model': 'NVIDIA SM61',\n            'memory_size': 8 * 1024 * 1024 * 1024,  # 8GB\n            'storage_type': 'nvme'\n        })\n\n    def _init_weights(self, module):\n        """Initialize the weights"""\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def post_init(self):\n        """Initialize weights and apply final processing."""\n        self.apply(self._init_weights)\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, (Qwen3VLDecoder, Qwen3VLVisionTransformer)):\n            module.gradient_checkpointing = value\n\n\nclass EfficientCrossAttention(nn.Module):\n    """\n    Efficient cross-attention mechanism for vision-language integration with memory optimizations.\n    Replaces DeepStack with optimized cross-attention that maintains all 32 attention heads.\n    """\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager, \n                 use_sparse: bool = False, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.use_sparse = use_sparse\n        self.memory_manager = memory_manager\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads  # Maintains all 32 heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads or self.num_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"\n                f" and `num_heads`: {self.num_heads})."\n            )\n\n        # Q projection for language features\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n\n        # KV projections for vision features\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n\n        # Output projection\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        # Sparse attention parameters if enabled\n        if self.use_sparse:\n            self.sparse_factor = 0.25  # Use 25% of full attention\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,  # Language features [batch, seq_len, hidden_size]\n        key_value: torch.Tensor,      # Vision features [batch, vision_seq_len, hidden_size]\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        _, kv_len, _ = key_value.size()\n\n        # Use memory-optimized tensor operations\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            # Project query from language features\n            query_states = ctx.access_tensor(self.q_proj(hidden_states), "cross_attn_query")\n            query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n            # Project key and value from vision features\n            key_states = ctx.access_tensor(self.k_proj(key_value), "cross_attn_key")\n            value_states = ctx.access_tensor(self.v_proj(key_value), "cross_attn_value")\n            key_states = key_states.view(bsz, kv_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n            value_states = value_states.view(bsz, kv_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n            # Expand key and value states if using GQA\n            key_states = self._repeat_kv(key_states, self.num_key_value_groups)\n            value_states = self._repeat_kv(value_states, self.num_key_value_groups)\n\n            # Compute attention weights\n            if self.use_sparse and kv_len > 512:  # Only apply sparsity for long sequences\n                # Sparse attention: attend to a subset of keys\n                sparse_kv_len = max(1, int(kv_len * self.sparse_factor))\n                key_states_sparse = key_states[:, :, :sparse_kv_len, :]\n                value_states_sparse = value_states[:, :, :sparse_kv_len, :]\n\n                attn_weights = torch.matmul(query_states, key_states_sparse.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n                if attention_mask is not None:\n                    sparse_mask = attention_mask[:, :, :q_len, :sparse_kv_len]\n                    attn_weights = attn_weights + sparse_mask\n            else:\n                # Standard attention\n                attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n                if attention_mask is not None:\n                    attn_weights = attn_weights + attention_mask\n\n            # Apply softmax\n            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\n            # Apply attention to values\n            if self.use_sparse and kv_len > 512:\n                attn_output = torch.matmul(attn_weights, value_states_sparse)\n            else:\n                attn_output = torch.matmul(attn_weights, value_states)\n\n            # Reshape and project output\n            attn_output = attn_output.transpose(1, 2).contiguous()\n            attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n            attn_output = ctx.access_tensor(self.o_proj(attn_output), "cross_attn_output")\n\n        return attn_output, attn_weights, None\n\n    def _repeat_kv(self, hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n        """\n        Repeat key/value heads to match the number of query heads.\n        """\n        batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n        if n_rep == 1:\n            return hidden_states\n        hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n        return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass Qwen3VLRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n        self.register_buffer("inv_freq", inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != "mps" else "cpu"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\ndef rotate_half(x):\n    """Rotates half the hidden dims of the input."""\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    """Applies Rotary Position Embedding to the query and key tensors."""\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    """\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).\n    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) to\n    (batch, num_attention_heads, seqlen, head_dim)\n    """\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass FactorizedConv2d(nn.Module):\n    """\n    Factorized convolution for vision encoder to improve efficiency.\n    Replaces standard conv2d with depthwise separable or pointwise factorization.\n    """\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, \n                 memory_manager: Optional[IntegratedMemoryManager] = None):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.memory_manager = memory_manager\n\n        # Factorize the convolution: first depthwise, then pointwise\n        self.depthwise_conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=in_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=in_channels,  # Depthwise convolution\n            bias=False\n        )\n\n        self.pointwise_conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=bias\n        )\n\n        # Optional: Add normalization between factorized operations\n        self.norm = nn.BatchNorm2d(in_channels)\n\n    def forward(self, x):\n        # Apply depthwise convolution\n        x = self.depthwise_conv(x)\n        # Apply normalization\n        x = self.norm(x)\n        # Apply activation\n        x = F.relu(x)\n        # Apply pointwise convolution\n        x = self.pointwise_conv(x)\n        return x\n\n\nclass Qwen3VLVisionAttention(nn.Module):\n    """Vision attention mechanism with optimized operations and memory management."""\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager):\n        super().__init__()\n        self.config = config\n        self.memory_manager = memory_manager\n        self.embed_dim = config.vision_hidden_size\n        self.num_heads = config.vision_num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(\n                f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:"\n                f" {self.num_heads})."\n            )\n        self.scale = self.head_dim ** -0.5\n        self.dropout = config.attention_dropout_prob\n\n        # Use factorized linear operations\n        self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=config.vision_qkv_bias)\n        self.proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n\n        # Sparse attention parameters\n        self.use_sparse_attention = True\n        self.sparse_factor = 0.25  # Use 25% of full attention\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n    ) -> torch.Tensor:\n        bsz, tgt_len, embed_dim = hidden_states.size()\n\n        # Use memory-optimized operations\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            # QKV projection\n            qkv = ctx.access_tensor(self.qkv(hidden_states), "vision_attn_qkv")\n            qkv = qkv.reshape(bsz, tgt_len, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n            q, k, v = qkv[0], qkv[1], qkv[2]\n\n            # Apply sparse attention if enabled and sequence is long enough\n            if self.use_sparse_attention and tgt_len > 256:\n                # Sparse attention: attend to a subset of keys\n                sparse_tgt_len = max(1, int(tgt_len * self.sparse_factor))\n                k_sparse = k[:, :, :sparse_tgt_len, :]\n                v_sparse = v[:, :, :sparse_tgt_len, :]\n\n                # Attention computation with sparse keys\n                attn_weights = torch.matmul(q, k_sparse.transpose(-2, -1)) * self.scale\n            else:\n                # Standard attention computation\n                attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n\n            attn_weights = F.softmax(attn_weights, dim=-1)\n            attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n\n            # Apply attention to values\n            if self.use_sparse_attention and tgt_len > 256:\n                attn_output = torch.matmul(attn_weights, v_sparse)\n            else:\n                attn_output = torch.matmul(attn_weights, v)\n\n            attn_output = attn_output.transpose(1, 2).reshape(bsz, tgt_len, embed_dim)\n            attn_output = ctx.access_tensor(self.proj(attn_output), "vision_attn_output")\n\n        return attn_output\n\n\nclass Qwen3VLVisionMLP(nn.Module):\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager):\n        super().__init__()\n        self.config = config\n        self.memory_manager = memory_manager\n        self.activation_fn = nn.GELU()\n\n        # Factorize the MLP operations for efficiency\n        # Instead of one large linear layer, use two smaller ones with a bottleneck\n        self.fc1 = nn.Linear(config.vision_hidden_size, config.vision_intermediate_size)\n        self.fc2 = nn.Linear(config.vision_intermediate_size, config.vision_hidden_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            hidden_states = ctx.access_tensor(self.fc1(hidden_states), "vision_mlp_fc1")\n            hidden_states = self.activation_fn(hidden_states)\n            hidden_states = ctx.access_tensor(self.fc2(hidden_states), "vision_mlp_fc2")\n        return hidden_states\n\n\nclass Qwen3VLVisionLayer(nn.Module):\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager, layer_idx: int):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.memory_manager = memory_manager\n        self.norm1 = nn.LayerNorm(config.vision_hidden_size, eps=config.layer_norm_eps)\n        self.attn = Qwen3VLVisionAttention(config, memory_manager)\n        self.norm2 = nn.LayerNorm(config.vision_hidden_size, eps=config.layer_norm_eps)\n        self.mlp = Qwen3VLVisionMLP(config, memory_manager)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n    ) -> torch.Tensor:\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            residual = hidden_states\n\n            hidden_states = self.norm1(hidden_states)\n            hidden_states = self.attn(hidden_states)\n            hidden_states = ctx.access_tensor(residual + hidden_states, f"vision_layer_{self.layer_idx}_residual1")\n\n            residual = hidden_states\n            hidden_states = self.norm2(hidden_states)\n            hidden_states = self.mlp(hidden_states)\n            hidden_states = ctx.access_tensor(residual + hidden_states, f"vision_layer_{self.layer_idx}_residual2")\n\n        return hidden_states\n\n\nclass Qwen3VLVisionTransformer(nn.Module):\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager):\n        super().__init__()\n        self.config = config\n        self.memory_manager = memory_manager\n        self.embed_dim = config.vision_hidden_size\n\n        # Use factorized patch embedding for efficiency\n        self.patch_embedding = FactorizedConv2d(\n            in_channels=config.vision_num_channels,\n            out_channels=self.embed_dim,\n            kernel_size=config.vision_patch_size,\n            stride=config.vision_patch_size,\n            bias=False,\n            memory_manager=memory_manager\n        )\n\n        # Calculate the number of patches based on the expected image size\n        self.num_patches_per_dim = config.vision_image_size // config.vision_patch_size\n        self.num_patches = self.num_patches_per_dim ** 2\n\n        self.position_embedding = nn.Embedding(self.num_patches, self.embed_dim)\n        self.register_buffer(\n            "position_ids",\n            torch.arange(self.num_patches).expand((1, -1)),\n            persistent=False\n        )\n\n        self.pre_layrnorm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.layers = nn.ModuleList([\n            Qwen3VLVisionLayer(config, memory_manager, layer_idx)\n            for layer_idx in range(config.vision_num_hidden_layers)\n        ])\n        self.post_layernorm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        pixel_values: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        batch_size = pixel_values.shape[0]\n        image_height, image_width = pixel_values.shape[-2], pixel_values.shape[-1]\n\n        # Calculate how many patches we'll have based on input image size\n        patch_height = image_height // self.config.vision_patch_size\n        patch_width = image_width // self.config.vision_patch_size\n        num_patches = patch_height * patch_width\n\n        # Patch embedding using factorized convolution\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            patch_embeds = ctx.access_tensor(self.patch_embedding(pixel_values), "vision_patch_embed")  # shape (batch_size, embed_dim, patch_height, patch_width)\n            patch_embeds = patch_embeds.flatten(2).transpose(1, 2)  # shape (batch_size, num_patches, embed_dim)\n\n            # Handle position embeddings - use only the needed portion or interpolate if needed\n            if num_patches <= self.num_patches:\n                # Use the first num_patches position embeddings\n                position_embeddings = self.position_embedding.weight[:num_patches].unsqueeze(0).expand(batch_size, -1, -1)\n            else:\n                # If more patches than expected, we need to handle this case\n                # For now, we'll raise an error as this would require interpolation\n                raise ValueError(f"Input image has {num_patches} patches, but model expects max {self.num_patches} patches. "\n                                 f"Input size: ({image_height}, {image_width}), expected: ({self.config.vision_image_size}, {self.config.vision_image_size})")\n\n            # Add position embeddings\n            embeddings = patch_embeds + position_embeddings\n\n            # Pre-layer norm\n            hidden_states = self.pre_layrnorm(embeddings)\n\n            # Apply transformer layers\n            for layer in self.layers:\n                hidden_states = layer(hidden_states)\n\n            # Post-layer norm\n            hidden_states = self.post_layernorm(hidden_states)\n\n        return hidden_states\n\n\nclass Qwen3VLAttention(nn.Module):\n    """\n    Multi-headed attention from 'Attention Is All You Need' paper with Qwen3-specific modifications.\n    Integrates efficiency improvements from Phase 2 while maintaining all 32 attention heads.\n    Includes memory optimization capabilities.\n    """\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.memory_manager = memory_manager\n        if layer_idx is None:\n            warnings.warn(\n                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "\n                "to errors during the forward call, if model `use_cache=True`."\n            )\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads or self.num_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"\n                f" and `num_heads`: {self.num_heads})."\n            )\n\n        # Use the most appropriate attention implementation based on configuration\n        attention_implementation = config.attention_implementation\n\n        if attention_implementation == "performer":\n            # Use Performer-style linear attention\n            self.attention_impl = PerformerAttention(config, layer_idx)\n        elif attention_implementation == "device_aware":\n            # Use device-aware attention\n            self.attention_impl = DeviceAwareAttention(config, layer_idx)\n        elif attention_implementation == "adaptive":\n            # Use adaptive attention\n            self.attention_impl = AdaptiveAttention(config, layer_idx)\n        elif attention_implementation == "memory_efficient":\n            # Use memory-efficient attention with gradient checkpointing\n            self.attention_impl = MemoryEfficientAttention(config, layer_idx)\n        else:\n            # Use optimized attention with memory management\n            self.attention_impl = OptimizedQwen3VLAttention(config, layer_idx)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # Use memory-optimized operations\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            # Record access to input states\n            hidden_states = ctx.access_tensor(hidden_states, f"attn_input_layer_{self.layer_idx}")\n            \n            # Apply the attention implementation\n            result = self.attention_impl(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n            \n            # Record access to output states\n            if result[0] is not None:\n                result = (ctx.access_tensor(result[0], f"attn_output_layer_{self.layer_idx}"),) + result[1:]\n        \n        return result\n\n\nclass Qwen3VLMLP(nn.Module):\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager):\n        super().__init__()\n        self.config = config\n        self.memory_manager = memory_manager\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n\n        # Use the most appropriate MLP implementation based on configuration\n        if config.attention_implementation == "adaptive":\n            # Use adaptive MLP\n            self.mlp_impl = AdaptiveMLP(config)\n        elif config.attention_implementation == "memory_efficient":\n            # Use memory-efficient MLP with gradient checkpointing\n            self.mlp_impl = MemoryEfficientMLP(config)\n        else:\n            # Use standard implementation\n            self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n            self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n            self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n            self.act_fn = nn.SiLU()\n\n    def forward(self, x):\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            x = ctx.access_tensor(x, "mlp_input")\n            \n            if hasattr(self, 'mlp_impl'):\n                result = self.mlp_impl(x)\n            else:\n                # For standard implementation\n                gate_output = ctx.access_tensor(self.act_fn(self.gate_proj(x)), "mlp_gate")\n                up_output = ctx.access_tensor(self.up_proj(x), "mlp_up")\n                intermediate_output = gate_output * up_output\n                result = ctx.access_tensor(self.down_proj(intermediate_output), "mlp_output")\n            \n            result = ctx.access_tensor(result, "mlp_final_output")\n        \n        return result\n\n\nclass Qwen3VLDecoderLayer(nn.Module):\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.layer_idx = layer_idx\n        self.memory_manager = memory_manager\n\n        self.self_attn = Qwen3VLAttention(config=config, memory_manager=memory_manager, layer_idx=layer_idx)\n\n        self.mlp = Qwen3VLMLP(config, memory_manager)\n        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        # Add cross-attention layer for vision-language fusion\n        self.vision_cross_attn = EfficientCrossAttention(config, memory_manager, use_sparse=True, layer_idx=layer_idx)\n        self.vision_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        vision_hidden_states: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        vision_attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            residual = hidden_states\n\n            # Self Attention\n            hidden_states = self.input_layernorm(hidden_states)\n            hidden_states, self_attn_weights, present_key_value = self.self_attn(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n            hidden_states = ctx.access_tensor(residual + hidden_states, f"decoder_layer_{self.layer_idx}_residual1")\n\n            # Vision-Language Cross Attention\n            if vision_hidden_states is not None:\n                residual = hidden_states\n                hidden_states = self.vision_layernorm(hidden_states)\n                hidden_states, cross_attn_weights, _ = self.vision_cross_attn(\n                    hidden_states=hidden_states,\n                    key_value=vision_hidden_states,\n                    attention_mask=vision_attention_mask\n                )\n                hidden_states = ctx.access_tensor(residual + hidden_states, f"decoder_layer_{self.layer_idx}_residual2")\n\n            # Fully Connected\n            residual = hidden_states\n            hidden_states = self.post_attention_layernorm(hidden_states)\n            hidden_states = self.mlp(hidden_states)\n            hidden_states = ctx.access_tensor(residual + hidden_states, f"decoder_layer_{self.layer_idx}_residual3")\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nclass Qwen3VLDecoder(nn.Module):\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager):\n        super().__init__()\n        self.config = config\n        self.memory_manager = memory_manager\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [Qwen3VLDecoderLayer(config, memory_manager, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self._use_gradient_checkpointing = config.use_gradient_checkpointing\n        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        vision_hidden_states: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        vision_attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> torch.Tensor:\n        # Use memory-optimized operations\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            # Embed tokens\n            if inputs_embeds is None:\n                if input_ids is None:\n                    # If we have vision_hidden_states but no input_ids, we can create a combined embedding\n                    if vision_hidden_states is not None:\n                        # Use vision_hidden_states as the primary input\n                        hidden_states = vision_hidden_states\n                    else:\n                        raise ValueError("input_ids cannot be None if inputs_embeds is None and no vision_hidden_states provided")\n                else:\n                    inputs_embeds = ctx.access_tensor(self.embed_tokens(input_ids), "embed_tokens")\n                    hidden_states = inputs_embeds\n            else:\n                hidden_states = inputs_embeds\n\n            # Create attention mask if not provided\n            if attention_mask is None:\n                if input_ids is not None:\n                    attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n                elif hidden_states is not None:\n                    # If input_ids is None, use hidden_states shape to create attention mask\n                    attention_mask = torch.ones(hidden_states.shape[:2], dtype=torch.bool, device=hidden_states.device)\n                else:\n                    # Default attention mask\n                    attention_mask = torch.ones((1, 1), dtype=torch.bool, device=self.device)\n\n            # Apply causal mask\n            causal_mask = self._prepare_decoder_attention_mask(\n                attention_mask, hidden_states.shape[:2], hidden_states.dtype, past_key_values\n            )\n\n            # Apply transformer layers\n            for decoder_layer in self.layers:\n                if self._use_gradient_checkpointing and self.training:\n                    layer_outputs = torch.utils.checkpoint.checkpoint(\n                        decoder_layer.__call__,\n                        hidden_states,\n                        vision_hidden_states,\n                        causal_mask,\n                        vision_attention_mask,\n                        position_ids,\n                        past_key_values,\n                        output_attentions,\n                        use_cache,\n                    )\n                else:\n                    layer_outputs = decoder_layer(\n                        hidden_states,\n                        vision_hidden_states=vision_hidden_states,\n                        attention_mask=causal_mask,\n                        vision_attention_mask=vision_attention_mask,\n                        position_ids=position_ids,\n                        past_key_value=past_key_values,\n                        output_attentions=output_attentions,\n                        use_cache=use_cache,\n                    )\n\n                hidden_states = layer_outputs[0]\n\n            hidden_states = ctx.access_tensor(self.norm(hidden_states), "decoder_norm_output")\n\n        return hidden_states\n\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, dtype, past_key_values):\n        """\n        Create causal attention mask for decoder.\n        """\n        # Create causal mask\n        batch_size, tgt_len = input_shape\n        causal_mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=attention_mask.device)\n        causal_mask = torch.triu(causal_mask, diagonal=1)\n\n        # Expand attention mask\n        expanded_attn_mask = attention_mask[:, None, None, :].expand(batch_size, 1, tgt_len, tgt_len).to(dtype)\n        expanded_attn_mask.masked_fill_(expanded_attn_mask == 0, torch.finfo(dtype).min)\n\n        # Combine masks\n        combined_mask = causal_mask.unsqueeze(0) + expanded_attn_mask\n        return combined_mask\n\n    @property\n    def device(self):\n        """Return the device of the model parameters."""\n        return next(self.parameters()).device\n\n\nclass Qwen3VLMultimodalProjector(nn.Module):\n    """Enhanced projector to align vision and language features with optimized operations."""\n    def __init__(self, config: Qwen3VLConfig, memory_manager: IntegratedMemoryManager):\n        super().__init__()\n        self.config = config\n        self.memory_manager = memory_manager\n\n        # Use factorized operations for efficiency\n        # Instead of a single large linear layer, use two smaller ones with a bottleneck\n        self.linear_1 = nn.Linear(config.vision_hidden_size, config.vision_projection_dim, bias=True)\n        self.act = nn.GELU()\n\n        # Add layer normalization for better gradient flow\n        self.norm = nn.LayerNorm(config.vision_projection_dim)\n\n        self.linear_2 = nn.Linear(config.vision_projection_dim, config.hidden_size, bias=True)\n\n    def forward(self, image_features):\n        with MemoryOptimizedContext(self.memory_manager) as ctx:\n            hidden_states = ctx.access_tensor(self.linear_1(image_features), "projector_linear1")\n            hidden_states = self.act(hidden_states)\n            hidden_states = ctx.access_tensor(self.norm(hidden_states), "projector_norm")\n            hidden_states = ctx.access_tensor(self.linear_2(hidden_states), "projector_linear2")\n        return hidden_states\n\n\nclass Qwen3VLForConditionalGeneration(Qwen3VLPreTrainedModel):\n    """\n    Qwen3-VL model for multimodal conditional generation tasks with integrated memory optimizations.\n    Incorporates Phase 3 vision-language integration optimizations.\n    Maintains full capacity with 32 transformer layers and 32 attention heads.\n    """\n    def __init__(self, config: Qwen3VLConfig, memory_manager: Optional[IntegratedMemoryManager] = None):\n        super().__init__(config, memory_manager)\n\n        # Vision encoder with factorized operations\n        self.vision_tower = Qwen3VLVisionTransformer(config, self.memory_manager)\n\n        # Enhanced multimodal projector\n        self.multi_modal_projector = Qwen3VLMultimodalProjector(config, self.memory_manager)\n\n        # Language model with cross-attention for vision integration\n        self.language_model = Qwen3VLDecoder(config, self.memory_manager)\n\n        # Initialize weights\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.language_model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.language_model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.language_model.embed_tokens\n\n    def set_output_embeddings(self, new_embeddings):\n        self.language_model.embed_tokens = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.language_model = decoder\n\n    def get_decoder(self):\n        return self.language_model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        pixel_values: torch.FloatTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> torch.Tensor:\n        # Perform memory optimizations periodically\n        self.memory_manager.perform_memory_optimizations()\n        \n        # Process image if provided\n        if pixel_values is not None:\n            # Extract visual features with optimized vision encoder\n            image_features = self.vision_tower(pixel_values)\n\n            # Project visual features to language model dimension\n            vision_hidden_states = self.multi_modal_projector(image_features)\n\n            # If input_ids is None but pixel_values is provided, create dummy input_ids\n            # to satisfy the language model requirements\n            if input_ids is None and inputs_embeds is None:\n                batch_size = pixel_values.shape[0]\n                seq_len = image_features.shape[1]  # Use the same sequence length as vision features\n                input_ids = torch.zeros((batch_size, seq_len), dtype=torch.long, device=self.device)\n                # Create attention mask for the dummy tokens\n                if attention_mask is None:\n                    attention_mask = torch.ones((batch_size, seq_len), dtype=torch.bool, device=self.device)\n\n            # Process with language model that has cross-attention\n            outputs = self.language_model(\n                input_ids=input_ids,\n                vision_hidden_states=vision_hidden_states,\n                attention_mask=attention_mask,\n                vision_attention_mask=None,  # Will be computed internally if needed\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        else:\n            # Process text only\n            # Check if we have either input_ids or inputs_embeds\n            if input_ids is None and inputs_embeds is None:\n                # If no inputs are provided, create a minimal input for the model to run\n                batch_size = 1  # Default batch size when no inputs are provided\n                input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=self.device)\n\n            outputs = self.language_model(\n                input_ids=input_ids,\n                vision_hidden_states=None,\n                attention_mask=attention_mask,\n                vision_attention_mask=None,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n        return outputs\n\n    @property\n    def device(self):\n        """Return the device of the model parameters."""\n        return next(self.parameters()).device\n\n    def generate(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        max_new_tokens: int = 50,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        **kwargs\n    ):\n        """\n        Generate tokens using the Qwen3-VL model with integrated memory optimizations.\n        """\n        if pad_token_id is None:\n            pad_token_id = self.config.pad_token_id\n\n        # Perform memory optimizations before generation\n        self.memory_manager.perform_memory_optimizations()\n\n        # Get the initial input embeddings\n        if pixel_values is not None:\n            # Process visual features with optimized vision encoder\n            image_features = self.vision_tower(pixel_values)\n            vision_hidden_states = self.multi_modal_projector(image_features)\n\n            if input_ids is not None:\n                # Process text with language model that has vision cross-attention\n                outputs = self.language_model(\n                    input_ids=input_ids,\n                    vision_hidden_states=vision_hidden_states,\n                    attention_mask=torch.ones_like(input_ids, dtype=torch.bool)\n                )\n\n                # Get the last token's logits\n                next_token_logits = outputs[:, -1, :]\n\n                # Apply temperature\n                if temperature != 1.0:\n                    next_token_logits = next_token_logits / temperature\n\n                # Apply top-k and top-p filtering\n                if do_sample:\n                    # Filter top-k\n                    if top_k > 0:\n                        indices_to_remove = next_token_logits > torch.topk(next_token_logits, top_k)[0][..., -1, None]\n                        next_token_logits[indices_to_remove] = float('-inf')\n\n                    # Filter top-p\n                    if top_p < 1.0:\n                        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n\n                        # Remove tokens with cumulative probability above the threshold\n                        sorted_indices_to_remove = cumulative_probs > top_p\n                        # Keep at least one option\n                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                        sorted_indices_to_remove[..., 0] = 0\n\n                        for i in range(next_token_logits.size(0)):\n                            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n                            next_token_logits[i][indices_to_remove] = float('-inf')\n\n                    # Sample next token\n                    probs = torch.softmax(next_token_logits, dim=-1)\n                    next_tokens = torch.multinomial(probs, num_samples=1)\n                else:\n                    # Greedy decoding\n                    next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n\n                # For simplicity in this implementation, we'll just return the next token\n                # In a full implementation, we would continue the generation loop\n                generated_ids = torch.cat([input_ids, next_tokens], dim=1)\n            else:\n                # Only image features, generate from them\n                batch_size, image_seq_len, _ = vision_hidden_states.shape\n                # Create dummy input IDs for the generation\n                dummy_input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=vision_hidden_states.device)\n\n                outputs = self.language_model(\n                    input_ids=dummy_input_ids,\n                    vision_hidden_states=vision_hidden_states,\n                    attention_mask=torch.ones((batch_size, 1), dtype=torch.bool, device=vision_hidden_states.device)\n                )\n\n                next_token_logits = outputs[:, -1, :]\n                if temperature != 1.0:\n                    next_token_logits = next_token_logits / temperature\n\n                if do_sample:\n                    probs = torch.softmax(next_token_logits, dim=-1)\n                    next_tokens = torch.multinomial(probs, num_samples=1)\n                else:\n                    next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n\n                generated_ids = next_tokens\n        else:\n            # Text-only generation\n            generated_ids = input_ids.clone()\n            for step in range(max_new_tokens):\n                # Perform memory optimizations periodically during generation\n                if step % 10 == 0:  # Every 10 steps\n                    self.memory_manager.perform_memory_optimizations()\n                \n                outputs = self.language_model(\n                    input_ids=generated_ids,\n                    vision_hidden_states=None,\n                    attention_mask=torch.ones_like(generated_ids, dtype=torch.bool)\n                )\n\n                # Get the last token's logits\n                next_token_logits = outputs[:, -1, :]\n\n                # Apply temperature\n                if temperature != 1.0:\n                    next_token_logits = next_token_logits / temperature\n\n                # Apply top-k and top-p filtering\n                if do_sample:\n                    # Filter top-k\n                    if top_k > 0:\n                        indices_to_remove = next_token_logits > torch.topk(next_token_logits, top_k)[0][..., -1, None]\n                        next_token_logits[indices_to_remove] = float('-inf')\n\n                    # Filter top-p\n                    if top_p < 1.0:\n                        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n\n                        # Remove tokens with cumulative probability above the threshold\n                        sorted_indices_to_remove = cumulative_probs > top_p\n                        # Keep at least one option\n                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                        sorted_indices_to_remove[..., 0] = 0\n\n                        for i in range(next_token_logits.size(0)):\n                            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n                            next_token_logits[i][indices_to_remove] = float('-inf')\n\n                    # Sample next token\n                    probs = torch.softmax(next_token_logits, dim=-1)\n                    next_tokens = torch.multinomial(probs, num_samples=1)\n                else:\n                    # Greedy decoding\n                    next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n\n                # Stop if EOS token is generated\n                if eos_token_id is not None and next_tokens[0, 0].item() == eos_token_id:\n                    break\n\n                # Append generated token\n                generated_ids = torch.cat([generated_ids, next_tokens], dim=1)\n\n        # Perform final memory optimizations after generation\n        self.memory_manager.perform_memory_optimizations()\n        \n        return generated_ids\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        inputs_embeds=None,\n        pixel_values=None,\n        attention_mask=None,\n        **kwargs\n    ):\n        """Prepare inputs for generation."""\n        # Handle past_key_values\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # Process pixel values if provided\n        if pixel_values is not None and past_key_values is None:\n            # Extract visual features only once at the beginning\n            image_features = self.vision_tower(pixel_values)\n            image_features = self.multi_modal_projector(image_features)\n            kwargs["image_features"] = image_features\n\n        # Prepare model inputs\n        model_inputs = {\n            "input_ids": input_ids,\n            "past_key_values": past_key_values,\n            "use_cache": kwargs.get("use_cache"),\n            "attention_mask": attention_mask,\n            "pixel_values": pixel_values if past_key_values is None else None,  # Only pass pixel_values on first call\n        }\n\n        if inputs_embeds is not None:\n            model_inputs["inputs_embeds"] = inputs_embeds\n\n        return model_inputs