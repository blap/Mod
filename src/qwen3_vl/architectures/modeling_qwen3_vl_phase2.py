"""\nUpdated Qwen3-VL Model implementation with Phase 2 efficiency improvements\n"""\nimport math\nimport warnings\nfrom typing import Optional, Tuple, Union, List\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom src.qwen3_vl.config import Qwen3VLConfig\nfrom architectures.linear_attention import PerformerAttention\nfrom architectures.device_aware_module import DeviceAwareAttention\nfrom architectures.gradient_checkpointing import MemoryEfficientAttention, MemoryEfficientMLP, apply_gradient_checkpointing\nfrom architectures.adaptive_computation import AdaptiveAttention, AdaptiveMLP\nfrom architectures.memory_management import OptimizedQwen3VLAttention\n\n\nclass Qwen3VLPreTrainedModel(nn.Module):\n    """\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n    """\n    config_class = Qwen3VLConfig\n    base_model_prefix = "qwen3_vl"\n    supports_gradient_checkpointing = True\n    _no_split_modules = ["Qwen3VLDecoderLayer", "Qwen3VLVisionLayer"]\n\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n\n    def _init_weights(self, module):\n        """Initialize the weights"""\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def post_init(self):\n        """Initialize weights and apply final processing."""\n        self.apply(self._init_weights)\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, (Qwen3VLDecoder, Qwen3VLVisionTransformer)):\n            module.gradient_checkpointing = value\n\n\nclass Qwen3VLAttention(nn.Module):\n    """\n    Multi-headed attention from 'Attention Is All You Need' paper with Qwen3-specific modifications.\n    Integrates efficiency improvements from Phase 2 while maintaining all 32 attention heads.\n    """\n    def __init__(self, config: Qwen3VLConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            warnings.warn(\n                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "\n                "to errors during the forward call, if model `use_cache=True`."\n            )\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads or self.num_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"\n                f" and `num_heads`: {self.num_heads})."\n            )\n\n        # Use the most appropriate attention implementation based on configuration\n        attention_implementation = config.attention_implementation\n        \n        if attention_implementation == "performer":\n            # Use Performer-style linear attention\n            self.attention_impl = PerformerAttention(config, layer_idx)\n        elif attention_implementation == "device_aware":\n            # Use device-aware attention\n            self.attention_impl = DeviceAwareAttention(config, layer_idx)\n        elif attention_implementation == "adaptive":\n            # Use adaptive attention\n            self.attention_impl = AdaptiveAttention(config, layer_idx)\n        elif attention_implementation == "memory_efficient":\n            # Use memory-efficient attention with gradient checkpointing\n            self.attention_impl = MemoryEfficientAttention(config, layer_idx)\n        else:\n            # Use optimized attention with memory management\n            self.attention_impl = OptimizedQwen3VLAttention(config, layer_idx)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        return self.attention_impl(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n        )\n\n\nclass Qwen3VLRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n        self.register_buffer("inv_freq", inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != "mps" else "cpu"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\ndef rotate_half(x):\n    """Rotates half the hidden dims of the input."""\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    """Applies Rotary Position Embedding to the query and key tensors."""\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    """\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).\n    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) to\n    (batch, num_attention_heads, seqlen, head_dim)\n    """\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass Qwen3VLMLP(nn.Module):\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        \n        # Use the most appropriate MLP implementation based on configuration\n        if config.attention_implementation == "adaptive":\n            # Use adaptive MLP\nfrom architectures.adaptive_computation import AdaptiveMLP\n            self.mlp_impl = AdaptiveMLP(config)\n        elif config.attention_implementation == "memory_efficient":\n            # Use memory-efficient MLP with gradient checkpointing\nfrom architectures.gradient_checkpointing import MemoryEfficientMLP\n            self.mlp_impl = MemoryEfficientMLP(config)\n        else:\n            # Use standard implementation\n            self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n            self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n            self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n            self.act_fn = nn.SiLU()\n\n    def forward(self, x):\n        if hasattr(self, 'mlp_impl'):\n            return self.mlp_impl(x)\n        else:\n            # For standard implementation\n            return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass Qwen3VLDecoderLayer(nn.Module):\n    def __init__(self, config: Qwen3VLConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.layer_idx = layer_idx\n\n        self.self_attn = Qwen3VLAttention(config=config, layer_idx=layer_idx)\n\n        self.mlp = Qwen3VLMLP(config)\n        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nclass Qwen3VLVisionAttention(nn.Module):\n    """Vision attention mechanism for the vision encoder."""\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.vision_hidden_size\n        self.num_heads = config.vision_num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        if self.head_dim * self.num_heads != self.embed_dim:\n            raise ValueError(\n                f"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:"\n                f" {self.num_heads})."\n            )\n        self.scale = self.head_dim ** -0.5\n        self.dropout = config.attention_dropout_prob\n\n        self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=config.vision_qkv_bias)\n        self.proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n    ) -> torch.Tensor:\n        bsz, tgt_len, embed_dim = hidden_states.size()\n\n        # QKV projection\n        qkv = self.qkv(hidden_states)\n        qkv = qkv.reshape(bsz, tgt_len, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        # Attention computation\n        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n        attn_weights = F.softmax(attn_weights, dim=-1)\n        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n\n        attn_output = torch.matmul(attn_weights, v)\n        attn_output = attn_output.transpose(1, 2).reshape(bsz, tgt_len, embed_dim)\n        attn_output = self.proj(attn_output)\n\n        return attn_output\n\n\nclass Qwen3VLVisionMLP(nn.Module):\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.activation_fn = nn.GELU()\n        self.fc1 = nn.Linear(config.vision_hidden_size, config.vision_intermediate_size)\n        self.fc2 = nn.Linear(config.vision_intermediate_size, config.vision_hidden_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.activation_fn(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        return hidden_states\n\n\nclass Qwen3VLVisionLayer(nn.Module):\n    def __init__(self, config: Qwen3VLConfig, layer_idx: int):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.norm1 = nn.LayerNorm(config.vision_hidden_size, eps=config.layer_norm_eps)\n        self.attn = Qwen3VLVisionAttention(config)\n        self.norm2 = nn.LayerNorm(config.vision_hidden_size, eps=config.layer_norm_eps)\n        self.mlp = Qwen3VLVisionMLP(config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n    ) -> torch.Tensor:\n        residual = hidden_states\n\n        hidden_states = self.norm1(hidden_states)\n        hidden_states = self.attn(hidden_states)\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n        hidden_states = self.norm2(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\n\nclass Qwen3VLVisionTransformer(nn.Module):\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.vision_hidden_size\n\n        # Patch embedding\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.vision_num_channels,\n            out_channels=self.embed_dim,\n            kernel_size=config.vision_patch_size,\n            stride=config.vision_patch_size,\n            bias=False\n        )\n\n        # Calculate the number of patches based on the expected image size\n        self.num_patches_per_dim = config.vision_image_size // config.vision_patch_size\n        self.num_patches = self.num_patches_per_dim ** 2\n\n        self.position_embedding = nn.Embedding(self.num_patches, self.embed_dim)\n        self.register_buffer(\n            "position_ids",\n            torch.arange(self.num_patches).expand((1, -1)),\n            persistent=False\n        )\n\n        self.pre_layrnorm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.layers = nn.ModuleList([\n            Qwen3VLVisionLayer(config, layer_idx)\n            for layer_idx in range(config.vision_num_hidden_layers)\n        ])\n        self.post_layernorm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        pixel_values: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        batch_size = pixel_values.shape[0]\n        image_height, image_width = pixel_values.shape[-2], pixel_values.shape[-1]\n\n        # Calculate how many patches we'll have based on input image size\n        patch_height = image_height // self.config.vision_patch_size\n        patch_width = image_width // self.config.vision_patch_size\n        num_patches = patch_height * patch_width\n\n        # Patch embedding\n        patch_embeds = self.patch_embedding(pixel_values)  # shape (batch_size, embed_dim, patch_height, patch_width)\n        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)  # shape (batch_size, num_patches, embed_dim)\n\n        # Handle position embeddings - use only the needed portion or interpolate if needed\n        if num_patches <= self.num_patches:\n            # Use the first num_patches position embeddings\n            position_embeddings = self.position_embedding.weight[:num_patches].unsqueeze(0).expand(batch_size, -1, -1)\n        else:\n            # If more patches than expected, we need to handle this case\n            # For now, we'll raise an error as this would require interpolation\n            raise ValueError(f"Input image has {num_patches} patches, but model expects max {self.num_patches} patches. "\n                             f"Input size: ({image_height}, {image_width}), expected: ({self.config.vision_image_size}, {self.config.vision_image_size})")\n\n        # Add position embeddings\n        embeddings = patch_embeds + position_embeddings\n\n        # Pre-layer norm\n        hidden_states = self.pre_layrnorm(embeddings)\n\n        # Apply transformer layers\n        for layer in self.layers:\n            hidden_states = layer(hidden_states)\n\n        # Post-layer norm\n        hidden_states = self.post_layernorm(hidden_states)\n\n        return hidden_states\n\n\nclass Qwen3VLDecoder(nn.Module):\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [Qwen3VLDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self._use_gradient_checkpointing = config.use_gradient_checkpointing\n        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> torch.Tensor:\n        # Embed tokens\n        if inputs_embeds is None:\n            if input_ids is None:\n                raise ValueError("input_ids cannot be None if inputs_embeds is None")\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        hidden_states = inputs_embeds\n\n        # Create attention mask if not provided\n        if attention_mask is None:\n            if input_ids is not None:\n                attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n            else:\n                # If input_ids is None, use inputs_embeds shape to create attention mask\n                attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.bool, device=inputs_embeds.device)\n\n        # Apply causal mask\n        causal_mask = self._prepare_decoder_attention_mask(\n            attention_mask, hidden_states.shape[:2], hidden_states.dtype, past_key_values\n        )\n\n        # Apply transformer layers\n        for decoder_layer in self.layers:\n            if self._use_gradient_checkpointing and self.training:\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n\n            hidden_states = layer_outputs[0]\n\n        hidden_states = self.norm(hidden_states)\n\n        return hidden_states\n\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, dtype, past_key_values):\n        """\n        Create causal attention mask for decoder.\n        """\n        # Create causal mask\n        batch_size, tgt_len = input_shape\n        causal_mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=attention_mask.device)\n        causal_mask = torch.triu(causal_mask, diagonal=1)\n\n        # Expand attention mask\n        expanded_attn_mask = attention_mask[:, None, None, :].expand(batch_size, 1, tgt_len, tgt_len).to(dtype)\n        expanded_attn_mask.masked_fill_(expanded_attn_mask == 0, torch.finfo(dtype).min)\n\n        # Combine masks\n        combined_mask = causal_mask.unsqueeze(0) + expanded_attn_mask\n        return combined_mask\n\n\nclass Qwen3VLMultimodalProjector(nn.Module):\n    """Projector to align vision and language features."""\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.linear_1 = nn.Linear(config.vision_hidden_size, config.vision_projection_dim, bias=True)\n        self.act = nn.GELU()\n        self.linear_2 = nn.Linear(config.vision_projection_dim, config.hidden_size, bias=True)\n\n    def forward(self, image_features):\n        hidden_states = self.linear_1(image_features)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.linear_2(hidden_states)\n        return hidden_states\n\n\nclass Qwen3VLForConditionalGeneration(Qwen3VLPreTrainedModel):\n    """\n    Qwen3-VL model for multimodal conditional generation tasks.\n    Maintains full capacity with 32 transformer layers and 32 attention heads.\n    Incorporates Phase 2 efficiency improvements.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__(config)\n        self.config = config\n\n        # Vision encoder\n        self.vision_tower = Qwen3VLVisionTransformer(config)\n\n        # Multimodal projector\n        self.multi_modal_projector = Qwen3VLMultimodalProjector(config)\n\n        # Language model with efficiency improvements\n        self.language_model = Qwen3VLDecoder(config)\n\n        # Initialize weights\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.language_model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.language_model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.language_model.embed_tokens\n\n    def set_output_embeddings(self, new_embeddings):\n        self.language_model.embed_tokens = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.language_model = decoder\n\n    def get_decoder(self):\n        return self.language_model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        pixel_values: torch.FloatTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> torch.Tensor:\n        # Process image if provided\n        if pixel_values is not None:\n            # Extract visual features\n            image_features = self.vision_tower(pixel_values)\n\n            # Project visual features to language model dimension\n            image_features = self.multi_modal_projector(image_features)\n\n            # Process text with language model\n            if input_ids is not None and inputs_embeds is None:\n                inputs_embeds = self.language_model.embed_tokens(input_ids)\n\n            # Combine visual and text features if both are available\n            if inputs_embeds is not None:\n                # For simplicity, we'll concatenate them (in practice, this would be more complex)\n                combined_embeds = torch.cat([image_features, inputs_embeds], dim=1)\n\n                # Create appropriate attention mask for combined embeddings\n                if attention_mask is not None:\n                    # Expand attention mask to account for image features\n                    batch_size, seq_len = attention_mask.shape\n                    image_seq_len = image_features.size(1)\n                    image_attn_mask = torch.ones(batch_size, image_seq_len, dtype=attention_mask.dtype, device=attention_mask.device)\n                    combined_attention_mask = torch.cat([image_attn_mask, attention_mask], dim=1)\n                else:\n                    combined_attention_mask = None\n\n                # Forward through language model\n                outputs = self.language_model(\n                    input_ids=None,  # We're using combined_embeds\n                    attention_mask=combined_attention_mask,\n                    position_ids=position_ids,\n                    past_key_values=past_key_values,\n                    inputs_embeds=combined_embeds,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                    output_hidden_states=output_hidden_states,\n                    return_dict=return_dict,\n                )\n            else:\n                # Only image features, no text input\n                # Create dummy input_ids to satisfy language model requirements\n                batch_size, image_seq_len, embed_dim = image_features.shape\n                dummy_input_ids = torch.zeros(batch_size, image_seq_len, dtype=torch.long, device=image_features.device)\n\n                # Create attention mask for image features\n                attention_mask = torch.ones(batch_size, image_seq_len, dtype=torch.bool, device=image_features.device)\n\n                outputs = self.language_model(\n                    input_ids=dummy_input_ids,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_values=past_key_values,\n                    inputs_embeds=image_features,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                    output_hidden_states=output_hidden_states,\n                    return_dict=return_dict,\n                )\n        else:\n            # Process text only\n            outputs = self.language_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n        return outputs\n\n    def generate(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        max_new_tokens: int = 50,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        **kwargs\n    ):\n        """\n        Generate tokens using the Qwen3-VL model with efficiency improvements.\n        """\n        if pad_token_id is None:\n            pad_token_id = self.config.pad_token_id\n\n        # Get the initial input embeddings\n        if pixel_values is not None:\n            # Process visual features\n            image_features = self.vision_tower(pixel_values)\n            image_features = self.multi_modal_projector(image_features)\n\n            if input_ids is not None:\n                # Combine image and text features\n                text_embeds = self.language_model.embed_tokens(input_ids)\n                combined_embeds = torch.cat([image_features, text_embeds], dim=1)\n\n                # Create attention mask for combined embeddings\n                batch_size, text_seq_len = input_ids.shape\n                image_seq_len = image_features.size(1)\n                text_attn_mask = torch.ones(batch_size, text_seq_len, dtype=torch.bool, device=input_ids.device)\n                image_attn_mask = torch.ones(batch_size, image_seq_len, dtype=torch.bool, device=input_ids.device)\n                attention_mask = torch.cat([image_attn_mask, text_attn_mask], dim=1)\n            else:\n                # Only image features\n                combined_embeds = image_features\n                batch_size, image_seq_len = image_features.shape[:2]\n                attention_mask = torch.ones(batch_size, image_seq_len, dtype=torch.bool, device=image_features.device)\n\n            # Generate tokens using the language model\n            generated_ids = input_ids if input_ids is not None else torch.zeros((batch_size, 0), dtype=torch.long, device=image_features.device)\n            current_embeds = combined_embeds if input_ids is not None else image_features\n\n            for _ in range(max_new_tokens):\n                # Forward pass\n                outputs = self.language_model(\n                    input_ids=None,\n                    inputs_embeds=current_embeds,\n                    attention_mask=attention_mask\n                )\n\n                # Get the last token's logits\n                next_token_logits = outputs[:, -1, :]\n\n                # Apply temperature\n                if temperature != 1.0:\n                    next_token_logits = next_token_logits / temperature\n\n                # Apply top-k and top-p filtering\n                if do_sample:\n                    # Filter top-k\n                    if top_k > 0:\n                        indices_to_remove = next_token_logits > torch.topk(next_token_logits, top_k)[0][..., -1, None]\n                        next_token_logits[indices_to_remove] = float('-inf')\n\n                    # Filter top-p\n                    if top_p < 1.0:\n                        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n\n                        # Remove tokens with cumulative probability above the threshold\n                        sorted_indices_to_remove = cumulative_probs > top_p\n                        # Keep at least one option\n                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                        sorted_indices_to_remove[..., 0] = 0\n\n                        for i in range(next_token_logits.size(0)):\n                            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n                            next_token_logits[i][indices_to_remove] = float('-inf')\n\n                    # Sample next token\n                    probs = torch.softmax(next_token_logits, dim=-1)\n                    next_tokens = torch.multinomial(probs, num_samples=1)\n                else:\n                    # Greedy decoding\n                    next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n\n                # Stop if EOS token is generated\n                if eos_token_id is not None and next_tokens[0, 0].item() == eos_token_id:\n                    break\n\n                # Append generated token\n                generated_ids = torch.cat([generated_ids, next_tokens], dim=1)\n\n                # Get embeddings for the new token\n                new_token_embeds = self.language_model.embed_tokens(next_tokens)\n                current_embeds = new_token_embeds  # Only the new token embeddings for the next step\n\n                # Extend attention mask\n                new_attn_mask = torch.ones((batch_size, 1), dtype=torch.bool, device=attention_mask.device)\n                attention_mask = torch.cat([attention_mask, new_attn_mask], dim=1)\n\n        else:\n            # Text-only generation\n            generated_ids = input_ids.clone()\n            for _ in range(max_new_tokens):\n                outputs = self.language_model(\n                    input_ids=generated_ids,\n                    attention_mask=torch.ones_like(generated_ids, dtype=torch.bool)\n                )\n\n                # Get the last token's logits\n                next_token_logits = outputs[:, -1, :]\n\n                # Apply temperature\n                if temperature != 1.0:\n                    next_token_logits = next_token_logits / temperature\n\n                # Apply top-k and top-p filtering\n                if do_sample:\n                    # Filter top-k\n                    if top_k > 0:\n                        indices_to_remove = next_token_logits > torch.topk(next_token_logits, top_k)[0][..., -1, None]\n                        next_token_logits[indices_to_remove] = float('-inf')\n\n                    # Filter top-p\n                    if top_p < 1.0:\n                        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n\n                        # Remove tokens with cumulative probability above the threshold\n                        sorted_indices_to_remove = cumulative_probs > top_p\n                        # Keep at least one option\n                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                        sorted_indices_to_remove[..., 0] = 0\n\n                        for i in range(next_token_logits.size(0)):\n                            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n                            next_token_logits[i][indices_to_remove] = float('-inf')\n\n                    # Sample next token\n                    probs = torch.softmax(next_token_logits, dim=-1)\n                    next_tokens = torch.multinomial(probs, num_samples=1)\n                else:\n                    # Greedy decoding\n                    next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n\n                # Stop if EOS token is generated\n                if eos_token_id is not None and next_tokens[0, 0].item() == eos_token_id:\n                    break\n\n                # Append generated token\n                generated_ids = torch.cat([generated_ids, next_tokens], dim=1)\n\n        return generated_ids\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        inputs_embeds=None,\n        pixel_values=None,\n        attention_mask=None,\n        **kwargs\n    ):\n        """Prepare inputs for generation."""\n        # Handle past_key_values\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # Process pixel values if provided\n        if pixel_values is not None and past_key_values is None:\n            # Extract visual features only once at the beginning\n            image_features = self.vision_tower(pixel_values)\n            image_features = self.multi_modal_projector(image_features)\n            kwargs["image_features"] = image_features\n\n        # Prepare model inputs\n        model_inputs = {\n            "input_ids": input_ids,\n            "past_key_values": past_key_values,\n            "use_cache": kwargs.get("use_cache"),\n            "attention_mask": attention_mask,\n            "pixel_values": pixel_values if past_key_values is None else None,  # Only pass pixel_values on first call\n        }\n\n        if inputs_embeds is not None:\n            model_inputs["inputs_embeds"] = inputs_embeds\n\n        return model_inputs