"""\nAdvanced Architecture Implementation for Qwen3-VL Model\nThis module integrates all the efficiency improvements and architectural innovations\nfor the Qwen3-VL model, focusing on computational efficiency while preserving\nfull model capacity (32 transformer layers and 32 attention heads).\n"""\nimport math\nimport warnings\nfrom typing import Optional, Tuple, Union, List\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom config import Qwen3VLConfig\nfrom architectures.linear_attention import PerformerAttention\nfrom architectures.device_aware_module import DeviceAwareAttention\nfrom architectures.gradient_checkpointing import MemoryEfficientAttention, MemoryEfficientMLP\nfrom architectures.adaptive_computation import AdaptiveAttention, AdaptiveMLP\nfrom architectures.memory_management import OptimizedQwen3VLAttention\nfrom architectures.dynamic_sparse_attention import DynamicSparseAttention\nfrom architectures.moe_flash_attention import FlashAttention, MoeLayer\nfrom architectures.kv_cache_optimization_multi_strategy import OptimizedAttentionWithKVCache\nfrom architectures.learned_activation_routing import LearnedActivationRouter, AdaptiveMLPWithLearnedRouting\nfrom architectures.block_sparse_attention import BlockSparseAttention\nfrom architectures.adaptive_precision import AdaptivePrecisionController, PrecisionAdaptiveLayer, Qwen3VLRotaryEmbedding, apply_rotary_pos_emb, repeat_kv, rotate_half\nfrom architectures.cross_layer_parameter_recycling import CrossLayerParameterRecycler, ParameterRecyclingAdapter\nfrom architectures.hierarchical_memory_compression import HierarchicalMemoryCompressor\nfrom architectures.cross_modal_token_merging import CrossModalTokenMerger\nfrom architectures.adaptive_depth import AdaptiveDepthController, InputComplexityAssessor\n\n\nclass Qwen3VLPreTrainedModel(nn.Module):\n    """\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n    """\n    config_class = Qwen3VLConfig\n    base_model_prefix = "qwen3_vl"\n    supports_gradient_checkpointing = True\n    _no_split_modules = ["Qwen3VLDecoderLayer", "Qwen3VLVisionLayer"]\n\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n\n    def _init_weights(self, module):\n        """Initialize the weights"""\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def post_init(self):\n        """Initialize weights and apply final processing."""\n        self.apply(self._init_weights)\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, (Qwen3VLDecoder, Qwen3VLVisionTransformer)):\n            module.gradient_checkpointing = value\n\n\nclass AdvancedAttention(nn.Module):\n    """\n    Advanced attention mechanism that integrates multiple efficiency improvements:\n    - Dynamic sparse attention with learned routing\n    - Block-sparse attention for hardware optimization\n    - KV cache optimization with multiple strategies\n    - Adaptive precision selection\n    """\n    def __init__(self, config: Qwen3VLConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            warnings.warn(\n                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "\n                "to errors during the forward call, if model `use_cache=True`."\n            )\n\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads or self.num_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"\n                f" and `num_heads`: {self.num_heads})."\n            )\n\n        # Adaptive attention based on configuration\n        attention_implementation = getattr(config, 'attention_implementation', 'standard')\n        \n        if getattr(config, 'use_dynamic_sparse_attention', False):\n            self.attention_impl = DynamicSparseAttention(config, layer_idx)\n        elif getattr(config, 'use_block_sparse_attention', False):\n            self.attention_impl = BlockSparseAttention(config, layer_idx)\n        elif getattr(config, 'use_flash_attention_2', False):\n            self.attention_impl = FlashAttention(config, layer_idx)\n        elif attention_implementation == "performer":\n            self.attention_impl = PerformerAttention(config, layer_idx)\n        elif attention_implementation == "device_aware":\n            self.attention_impl = DeviceAwareAttention(config, layer_idx)\n        elif attention_implementation == "adaptive":\n            self.attention_impl = AdaptiveAttention(config, layer_idx)\n        elif attention_implementation == "memory_efficient":\n            self.attention_impl = MemoryEfficientAttention(config, layer_idx)\n        elif attention_implementation == "kv_cache_optimized":\n            self.attention_impl = OptimizedAttentionWithKVCache(config, layer_idx)\n        else:\n            # Standard optimized attention\n            self.attention_impl = OptimizedQwen3VLAttention(config, layer_idx)\n\n        # Adaptive precision controller\n        if getattr(config, 'use_adaptive_precision', False):\n            self.precision_controller = AdaptivePrecisionController(config)\n            self.layer_idx = layer_idx\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # Apply adaptive precision if enabled\n        if hasattr(self, 'precision_controller'):\n            # Apply precision adaptation\n            result = self.attention_impl(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n        else:\n            result = self.attention_impl(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n        \n        return result\n\n\nclass AdvancedMLP(nn.Module):\n    """\n    Advanced MLP with multiple efficiency improvements:\n    - Mixture of Experts (MoE)\n    - Learned activation routing\n    - Adaptive precision\n    - Parameter recycling\n    """\n    def __init__(self, config: Qwen3VLConfig, layer_idx: int):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n\n        # Use the most appropriate MLP implementation based on configuration\n        if getattr(config, 'use_moe', False):\n            # Use Mixture of Experts\n            self.mlp_impl = MoeLayer(\n                config,\n                num_experts=getattr(config, 'moe_num_experts', 4),\n                top_k=getattr(config, 'moe_top_k', 2)\n            )\n        elif getattr(config, 'use_learned_activation_routing', False):\n            # Use learned activation routing\n            self.mlp_impl = AdaptiveMLPWithLearnedRouting(config, layer_idx)\n        elif getattr(config, 'use_adaptive_precision', False):\n            # Use adaptive precision MLP\n            self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n            self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n            self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n            self.act_fn = nn.SiLU()\n            self.precision_controller = AdaptivePrecisionController(config)\n        elif getattr(config, 'use_parameter_recycling', False):\n            # Use parameter recycling with adapters\n            self.param_recycler = CrossLayerParameterRecycler(config)\n            self.up_adapter = ParameterRecyclingAdapter(config, layer_idx)\n            self.down_adapter = ParameterRecyclingAdapter(config, layer_idx)\n            self.act_fn = nn.SiLU()\n        else:\n            # Standard implementation\n            self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n            self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n            self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n            self.act_fn = nn.SiLU()\n\n    def forward(self, x, position_ids: Optional[torch.Tensor] = None):\n        if hasattr(self, 'mlp_impl'):\n            # Use advanced implementation (MoE or learned routing)\n            if isinstance(self.mlp_impl, MoeLayer):\n                # MoeLayer doesn't accept position_ids\n                return self.mlp_impl(x)\n            elif isinstance(self.mlp_impl, AdaptiveMLPWithLearnedRouting):\n                # AdaptiveMLPWithLearnedRouting may accept position_ids\n                return self.mlp_impl(x, position_ids)\n            else:\n                # Other implementations\n                return self.mlp_impl(x)\n        elif hasattr(self, 'precision_controller'):\n            # Use adaptive precision implementation\n            gate_states = self.gate_proj(x)\n            up_states = self.up_proj(x)\n            gate_states = self.precision_controller.layer_precisions.get(self.layer_idx, 'fp16') == 'fp16' and torch.cuda.is_available() \
                and hasattr(torch.cuda, 'amp') \
                and torch.cuda.amp.autocast()(self.act_fn(gate_states)) or self.act_fn(gate_states)\n            up_states = self.precision_controller.layer_precisions.get(self.layer_idx, 'fp16') == 'fp16' and torch.cuda.is_available() \
                and hasattr(torch.cuda, 'amp') \
                and torch.cuda.amp.autocast()(up_states) or up_states\n            return self.down_proj(gate_states * up_states)\n        elif hasattr(self, 'param_recycler'):\n            # Use parameter recycling implementation\n            up_states = self.param_recycler(x, self.layer_idx)\n            up_states = self.up_adapter(up_states)\n            up_states = self.act_fn(up_states)\n            down_states = self.param_recycler(up_states, self.layer_idx)\n            down_states = self.down_adapter(down_states)\n            return down_states\n        else:\n            # Standard implementation\n            return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass AdvancedTransformerLayer(nn.Module):\n    """\n    Advanced transformer layer with multiple efficiency improvements:\n    - Adaptive computation paths\n    - Dynamic layer activation\n    - Cross-layer memory sharing\n    - Hierarchical memory compression\n    """\n    def __init__(self, config: Qwen3VLConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.layer_idx = layer_idx\n\n        # Advanced attention mechanism\n        self.self_attn = AdvancedAttention(config=config, layer_idx=layer_idx)\n\n        # Advanced MLP with efficiency improvements\n        self.mlp = AdvancedMLP(config, layer_idx)\n        \n        # Layer normalization\n        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        # Adaptive depth and early exit mechanisms\n        if getattr(config, 'use_adaptive_depth', False):\n            self.complexity_assessor = InputComplexityAssessor(config)\n            self.exit_layer = nn.Linear(config.hidden_size, 1)\n            self.exit_threshold = getattr(config, 'exit_threshold', 0.5)\n\n        # Cross-layer memory sharing if enabled\n        if getattr(config, 'use_cross_layer_sharing', False):\n            self.cross_layer_adapter = ParameterRecyclingAdapter(config, layer_idx)\n\n        # Hierarchical memory compression if enabled\n        if getattr(config, 'use_hierarchical_compression', False):\n            self.memory_compressor = HierarchicalMemoryCompressor(config)\n\n        # Gradient checkpointing\n        self._use_gradient_checkpointing = config.use_gradient_checkpointing\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n        )\n        hidden_states = residual + hidden_states\n\n        # Check for early exit if enabled\n        if hasattr(self, 'complexity_assessor'):\n            exit_prob = torch.sigmoid(self.exit_layer(hidden_states.mean(dim=1))).squeeze(-1)\n            if (exit_prob > self.exit_threshold).all():\n                # Early exit - return current state without further processing\n                outputs = (hidden_states,)\n                if output_attentions:\n                    outputs += (self_attn_weights,)\n                if use_cache:\n                    outputs += (present_key_value,)\n                return outputs\n\n        # Cross-layer memory sharing if enabled\n        if hasattr(self, 'cross_layer_adapter'):\n            hidden_states = self.cross_layer_adapter(hidden_states)\n\n        # Hierarchical memory compression if enabled\n        if hasattr(self, 'memory_compressor'):\n            hidden_states, _ = self.memory_compressor(hidden_states)\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states, position_ids)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nclass AdvancedVisionTransformer(nn.Module):\n    """\n    Advanced vision transformer with efficiency improvements:\n    - Cross-modal token merging\n    - Hierarchical vision processing\n    - Adaptive precision for vision components\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.vision_hidden_size\n\n        # Patch embedding\n        self.patch_embedding = nn.Conv2d(\n            in_channels=config.vision_num_channels,\n            out_channels=self.embed_dim,\n            kernel_size=config.vision_patch_size,\n            stride=config.vision_patch_size,\n            bias=False\n        )\n\n        # Calculate the number of patches based on the expected image size\n        self.num_patches_per_dim = config.vision_image_size // config.vision_patch_size\n        self.num_patches = self.num_patches_per_dim ** 2\n\n        self.position_embedding = nn.Embedding(self.num_patches, self.embed_dim)\n        self.register_buffer(\n            "position_ids",\n            torch.arange(self.num_patches).expand((1, -1)),\n            persistent=False\n        )\n\n        self.pre_layrnorm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n        \n        # Create vision layers with advanced optimizations\n        self.layers = nn.ModuleList([\n            self._create_vision_layer(config, layer_idx)\n            for layer_idx in range(config.vision_num_hidden_layers)\n        ])\n        \n        self.post_layernorm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n\n        # Cross-modal token merging if enabled\n        if getattr(config, 'use_cross_modal_token_merging', False):\n            self.token_merger = CrossModalTokenMerger(config)\n\n    def _create_vision_layer(self, config, layer_idx):\n        """Create a vision layer with appropriate optimizations."""\nfrom architectures.dynamic_sparse_attention import VisionDynamicSparseAttention\nfrom architectures.block_sparse_attention import VisionBlockSparseAttention\n        \n        # Use the appropriate attention mechanism based on configuration\n        if getattr(config, 'use_dynamic_sparse_attention', False):\n            attn_module = VisionDynamicSparseAttention(config)\n        elif getattr(config, 'use_block_sparse_attention', False):\n            attn_module = VisionBlockSparseAttention(config, layer_idx)\n        else:\n            # Standard vision attention\nfrom architectures.modeling_qwen3_vl_phase3 import Qwen3VLVisionAttention\n            attn_module = Qwen3VLVisionAttention(config)\n        \n        return nn.ModuleDict({\n            'norm1': nn.LayerNorm(config.vision_hidden_size, eps=config.layer_norm_eps),\n            'attn': attn_module,\n            'norm2': nn.LayerNorm(config.vision_hidden_size, eps=config.layer_norm_eps),\n            'mlp': nn.Sequential(  # Simplified MLP for vision\n                nn.Linear(config.vision_hidden_size, config.vision_intermediate_size),\n                nn.GELU(),\n                nn.Linear(config.vision_intermediate_size, config.vision_hidden_size)\n            )\n        })\n\n    def forward(\n        self,\n        pixel_values: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        batch_size = pixel_values.shape[0]\n        image_height, image_width = pixel_values.shape[-2], pixel_values.shape[-1]\n\n        # Calculate how many patches we'll have based on input image size\n        patch_height = image_height // self.config.vision_patch_size\n        patch_width = image_width // self.config.vision_patch_size\n        num_patches = patch_height * patch_width\n\n        # Patch embedding\n        patch_embeds = self.patch_embedding(pixel_values)  # shape (batch_size, embed_dim, patch_height, patch_width)\n        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)  # shape (batch_size, num_patches, embed_dim)\n\n        # Handle position embeddings - use only the needed portion or interpolate if needed\n        if num_patches <= self.num_patches:\n            # Use the first num_patches position embeddings\n            position_embeddings = self.position_embedding.weight[:num_patches].unsqueeze(0).expand(batch_size, -1, -1)\n        else:\n            # If more patches than expected, we need to handle this case\n            raise ValueError(f"Input image has {num_patches} patches, but model expects max {self.num_patches} patches. "\n                             f"Input size: ({image_height}, {image_width}), expected: ({self.config.vision_image_size}, {self.config.vision_image_size})")\n\n        # Add position embeddings\n        embeddings = patch_embeds + position_embeddings\n\n        # Pre-layer norm\n        hidden_states = self.pre_layrnorm(embeddings)\n\n        # Apply transformer layers\n        for layer in self.layers:\n            residual = hidden_states\n            hidden_states = layer['norm1'](hidden_states)\n            if hasattr(layer['attn'], 'forward'):\n                # For custom attention implementations\n                hidden_states = layer['attn'](hidden_states)\n            else:\n                # For standard attention\n                hidden_states = layer['attn'](hidden_states)\n            hidden_states = residual + hidden_states\n\n            residual = hidden_states\n            hidden_states = layer['norm2'](hidden_states)\n            hidden_states = layer['mlp'](hidden_states)\n            hidden_states = residual + hidden_states\n\n        # Post-layer norm\n        hidden_states = self.post_layernorm(hidden_states)\n\n        return hidden_states\n\n\nclass AdvancedQwen3VLForConditionalGeneration(Qwen3VLPreTrainedModel):\n    """\n    Advanced Qwen3-VL model for multimodal conditional generation tasks.\n    Incorporates all efficiency improvements while maintaining full capacity.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__(config)\n        self.config = config\n\n        # Advanced vision encoder with efficiency improvements\n        self.vision_tower = AdvancedVisionTransformer(config)\n\n        # Multimodal projector with efficiency improvements\nfrom architectures.modeling_qwen3_vl_phase3 import Qwen3VLMultimodalProjector\n        self.multi_modal_projector = Qwen3VLMultimodalProjector(config)\n\n        # Advanced language model with all optimizations\n        self.language_model = AdvancedQwen3VLDecoder(config)\n\n        # Initialize weights\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.language_model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.language_model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.language_model.embed_tokens\n\n    def set_output_embeddings(self, new_embeddings):\n        self.language_model.embed_tokens = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.language_model = decoder\n\n    def get_decoder(self):\n        return self.language_model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        pixel_values: torch.FloatTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> torch.Tensor:\n        # Process image if provided\n        if pixel_values is not None:\n            # Extract visual features with advanced vision encoder\n            image_features = self.vision_tower(pixel_values)\n\n            # Project visual features to language model dimension\n            image_features = self.multi_modal_projector(image_features)\n\n            # Process text with advanced language model\n            if input_ids is not None and inputs_embeds is None:\n                inputs_embeds = self.language_model.embed_tokens(input_ids)\n\n            # Combine visual and text features if both are available\n            if inputs_embeds is not None:\n                # For simplicity, we'll concatenate them (in practice, this would be more complex)\n                combined_embeds = torch.cat([image_features, inputs_embeds], dim=1)\n\n                # Create appropriate attention mask for combined embeddings\n                if attention_mask is not None:\n                    # Expand attention mask to account for image features\n                    batch_size, seq_len = attention_mask.shape\n                    image_seq_len = image_features.size(1)\n                    image_attn_mask = torch.ones(batch_size, image_seq_len, dtype=attention_mask.dtype, device=attention_mask.device)\n                    combined_attention_mask = torch.cat([image_attn_mask, attention_mask], dim=1)\n                else:\n                    combined_attention_mask = None\n\n                # Forward through language model\n                outputs = self.language_model(\n                    input_ids=None,  # We're using combined_embeds\n                    attention_mask=combined_attention_mask,\n                    position_ids=position_ids,\n                    past_key_values=past_key_values,\n                    inputs_embeds=combined_embeds,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                    output_hidden_states=output_hidden_states,\n                    return_dict=return_dict,\n                )\n            else:\n                # Only image features, no text input\n                # Create dummy input_ids to satisfy language model requirements\n                batch_size, image_seq_len, embed_dim = image_features.shape\n                dummy_input_ids = torch.zeros(batch_size, image_seq_len, dtype=torch.long, device=image_features.device)\n\n                # Create attention mask for image features\n                attention_mask = torch.ones(batch_size, image_seq_len, dtype=torch.bool, device=image_features.device)\n\n                outputs = self.language_model(\n                    input_ids=dummy_input_ids,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_values=past_key_values,\n                    inputs_embeds=image_features,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                    output_hidden_states=output_hidden_states,\n                    return_dict=return_dict,\n                )\n        else:\n            # Process text only\n            outputs = self.language_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n        return outputs\n\n    def generate(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        pixel_values: Optional[torch.FloatTensor] = None,\n        max_new_tokens: int = 50,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        **kwargs\n    ):\n        """\n        Generate tokens using the advanced Qwen3-VL model with all optimizations.\n        """\n        if pad_token_id is None:\n            pad_token_id = self.config.pad_token_id\n\n        # Get the initial input embeddings\n        if pixel_values is not None:\n            # Process visual features\n            image_features = self.vision_tower(pixel_values)\n            image_features = self.multi_modal_projector(image_features)\n\n            if input_ids is not None:\n                # Combine image and text features\n                text_embeds = self.language_model.embed_tokens(input_ids)\n                combined_embeds = torch.cat([image_features, text_embeds], dim=1)\n\n                # Create attention mask for combined embeddings\n                batch_size, text_seq_len = input_ids.shape\n                image_seq_len = image_features.size(1)\n                text_attn_mask = torch.ones(batch_size, text_seq_len, dtype=torch.bool, device=input_ids.device)\n                image_attn_mask = torch.ones(batch_size, image_seq_len, dtype=torch.bool, device=input_ids.device)\n                attention_mask = torch.cat([image_attn_mask, text_attn_mask], dim=1)\n            else:\n                # Only image features\n                combined_embeds = image_features\n                batch_size, image_seq_len = image_features.shape[:2]\n                attention_mask = torch.ones(batch_size, image_seq_len, dtype=torch.bool, device=image_features.device)\n\n            # Generate tokens using the language model\n            generated_ids = input_ids if input_ids is not None else torch.zeros((batch_size, 0), dtype=torch.long, device=image_features.device)\n            current_embeds = combined_embeds if input_ids is not None else image_features\n\n            for _ in range(max_new_tokens):\n                # Forward pass\n                outputs = self.language_model(\n                    input_ids=None,\n                    inputs_embeds=current_embeds,\n                    attention_mask=attention_mask\n                )\n\n                # Get the last token's logits\n                next_token_logits = outputs[:, -1, :]\n\n                # Apply temperature\n                if temperature != 1.0:\n                    next_token_logits = next_token_logits / temperature\n\n                # Apply top-k and top-p filtering\n                if do_sample:\n                    # Filter top-k\n                    if top_k > 0:\n                        indices_to_remove = next_token_logits > torch.topk(next_token_logits, top_k)[0][..., -1, None]\n                        next_token_logits[indices_to_remove] = float('-inf')\n\n                    # Filter top-p\n                    if top_p < 1.0:\n                        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n\n                        # Remove tokens with cumulative probability above the threshold\n                        sorted_indices_to_remove = cumulative_probs > top_p\n                        # Keep at least one option\n                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                        sorted_indices_to_remove[..., 0] = 0\n\n                        for i in range(next_token_logits.size(0)):\n                            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n                            next_token_logits[i][indices_to_remove] = float('-inf')\n\n                    # Sample next token\n                    probs = torch.softmax(next_token_logits, dim=-1)\n                    next_tokens = torch.multinomial(probs, num_samples=1)\n                else:\n                    # Greedy decoding\n                    next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n\n                # Stop if EOS token is generated\n                if eos_token_id is not None and next_tokens[0, 0].item() == eos_token_id:\n                    break\n\n                # Append generated token\n                generated_ids = torch.cat([generated_ids, next_tokens], dim=1)\n\n                # Get embeddings for the new token\n                new_token_embeds = self.language_model.embed_tokens(next_tokens)\n                current_embeds = new_token_embeds  # Only the new token embeddings for the next step\n\n                # Extend attention mask\n                new_attn_mask = torch.ones((batch_size, 1), dtype=torch.bool, device=attention_mask.device)\n                attention_mask = torch.cat([attention_mask, new_attn_mask], dim=1)\n\n        else:\n            # Text-only generation\n            generated_ids = input_ids.clone()\n            for _ in range(max_new_tokens):\n                outputs = self.language_model(\n                    input_ids=generated_ids,\n                    attention_mask=torch.ones_like(generated_ids, dtype=torch.bool)\n                )\n\n                # Get the last token's logits\n                next_token_logits = outputs[:, -1, :]\n\n                # Apply temperature\n                if temperature != 1.0:\n                    next_token_logits = next_token_logits / temperature\n\n                # Apply top-k and top-p filtering\n                if do_sample:\n                    # Filter top-k\n                    if top_k > 0:\n                        indices_to_remove = next_token_logits > torch.topk(next_token_logits, top_k)[0][..., -1, None]\n                        next_token_logits[indices_to_remove] = float('-inf')\n\n                    # Filter top-p\n                    if top_p < 1.0:\n                        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n\n                        # Remove tokens with cumulative probability above the threshold\n                        sorted_indices_to_remove = cumulative_probs > top_p\n                        # Keep at least one option\n                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                        sorted_indices_to_remove[..., 0] = 0\n\n                        for i in range(next_token_logits.size(0)):\n                            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]\n                            next_token_logits[i][indices_to_remove] = float('-inf')\n\n                    # Sample next token\n                    probs = torch.softmax(next_token_logits, dim=-1)\n                    next_tokens = torch.multinomial(probs, num_samples=1)\n                else:\n                    # Greedy decoding\n                    next_tokens = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n\n                # Stop if EOS token is generated\n                if eos_token_id is not None and next_tokens[0, 0].item() == eos_token_id:\n                    break\n\n                # Append generated token\n                generated_ids = torch.cat([generated_ids, next_tokens], dim=1)\n\n        return generated_ids\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        inputs_embeds=None,\n        pixel_values=None,\n        attention_mask=None,\n        **kwargs\n    ):\n        """Prepare inputs for generation."""\n        # Handle past_key_values\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # Process pixel values if provided\n        if pixel_values is not None and past_key_values is None:\n            # Extract visual features only once at the beginning\n            image_features = self.vision_tower(pixel_values)\n            image_features = self.multi_modal_projector(image_features)\n            kwargs["image_features"] = image_features\n\n        # Prepare model inputs\n        model_inputs = {\n            "input_ids": input_ids,\n            "past_key_values": past_key_values,\n            "use_cache": kwargs.get("use_cache"),\n            "attention_mask": attention_mask,\n            "pixel_values": pixel_values if past_key_values is None else None,  # Only pass pixel_values on first call\n        }\n\n        if inputs_embeds is not None:\n            model_inputs["inputs_embeds"] = inputs_embeds\n\n        return model_inputs\n\n\nclass AdvancedQwen3VLDecoder(nn.Module):\n    """\n    Advanced language model decoder with all efficiency improvements integrated.\n    """\n    def __init__(self, config: Qwen3VLConfig):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        \n        # Create layers with advanced optimizations\n        self.layers = nn.ModuleList([\n            AdvancedTransformerLayer(config, layer_idx) \n            for layer_idx in range(config.num_hidden_layers)\n        ])\n        \n        self._use_gradient_checkpointing = config.use_gradient_checkpointing\n        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> torch.Tensor:\n        # Embed tokens\n        if inputs_embeds is None:\n            if input_ids is None:\n                raise ValueError("input_ids cannot be None if inputs_embeds is None")\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        hidden_states = inputs_embeds\n\n        # Create attention mask if not provided\n        if attention_mask is None:\n            if input_ids is not None:\n                attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n            else:\n                # If input_ids is None, use inputs_embeds shape to create attention mask\n                attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.bool, device=inputs_embeds.device)\n\n        # Apply causal mask\n        causal_mask = self._prepare_decoder_attention_mask(\n            attention_mask, hidden_states.shape[:2], hidden_states.dtype, past_key_values\n        )\n\n        # Apply transformer layers\n        for decoder_layer in self.layers:\n            if self._use_gradient_checkpointing and self.training:\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n\n            hidden_states = layer_outputs[0]\n\n        hidden_states = self.norm(hidden_states)\n\n        return hidden_states\n\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, dtype, past_key_values):\n        """\n        Create causal attention mask for decoder.\n        """\n        # Create causal mask\n        batch_size, tgt_len = input_shape\n        causal_mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=attention_mask.device)\n        causal_mask = torch.triu(causal_mask, diagonal=1)\n\n        # Expand attention mask\n        expanded_attn_mask = attention_mask[:, None, None, :].expand(batch_size, 1, tgt_len, tgt_len).to(dtype)\n        expanded_attn_mask.masked_fill_(expanded_attn_mask == 0, torch.finfo(dtype).min)\n\n        # Combine masks\n        combined_mask = causal_mask.unsqueeze(0) + expanded_attn_mask\n        return combined_mask