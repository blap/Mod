"""\nIntegrated Memory Management System for Qwen3-VL model\nIntegrates all memory optimization components for unified management.\n"""\nimport numpy as np\nimport torch\nfrom typing import Any, Dict, List, Optional, Union\nfrom dataclasses import dataclass\nimport time\nimport threading\nimport logging\nimport os\n\n# Import the other components\nfrom memory_compression_system import MemoryCompressionManager\nfrom enhanced_predictive_tensor_lifecycle_manager import EnhancedPredictiveGarbageCollector\nfrom advanced_memory_management_vl import VisionLanguageMemoryOptimizer\nfrom advanced_memory_swapping_system import AdvancedMemorySwapper\nfrom advanced_memory_tiering_system import AdvancedMemoryTieringSystem\n\nfrom qwen3_vl.utils.debug_utils import conditional_debug
from qwen3_vl.utils.general_utils import is_debug_mode\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass TensorMetadata:\n    """Metadata for tensors managed by the integrated system"""\n    tensor_id: str\n    original_shape: tuple\n    dtype: torch.dtype\n    size_bytes: int\n    creation_time: float\n    tensor_type: str\n    compression_status: str = "uncompressed"  # uncompressed, compressed, swapped\n    tier: str = "cpu"  # gpu, cpu, disk\n    priority: float = 0.5\n\n\nclass IntegratedMemoryManagementSystem:\n    """\n    Integrated memory management system that coordinates all memory optimization components.\n    """\n    \n    def __init__(self, \n                 memory_optimizer: Optional[VisionLanguageMemoryOptimizer] = None,\n                 swapping_system: Optional[AdvancedMemorySwapper] = None,\n                 tiering_system: Optional[AdvancedMemoryTieringSystem] = None,\n                 compression_system: Optional[MemoryCompressionManager] = None,\n                 lifecycle_manager: Optional[EnhancedPredictiveGarbageCollector] = None):\n        """\n        Initialize the integrated memory management system\n        \n        Args:\n            memory_optimizer: VisionLanguageMemoryOptimizer instance\n            swapping_system: AdvancedMemorySwapper instance\n            tiering_system: AdvancedMemoryTieringSystem instance\n            compression_system: MemoryCompressionManager instance\n            lifecycle_manager: EnhancedPredictiveGarbageCollector instance\n        """\n        self.memory_optimizer = memory_optimizer or VisionLanguageMemoryOptimizer()\n        self.swapping_system = swapping_system\n        self.tiering_system = tiering_system\n        self.compression_system = compression_system or MemoryCompressionManager()\n        self.lifecycle_manager = lifecycle_manager or EnhancedPredictiveGarbageCollector()\n        \n        # Internal tracking\n        self.tensor_registry: Dict[str, TensorMetadata] = {}\n        self.tensor_data: Dict[str, Union[torch.Tensor, bytes]] = {}  # Either tensor or compressed bytes\n        self.system_stats = {\n            'tensors_managed': 0,\n            'total_bytes_managed': 0,\n            'bytes_compressed': 0,\n            'bytes_swapped': 0,\n            'compression_ratio': 1.0,\n            'swap_operations': 0,\n            'tiering_operations': 0,\n            'avg_compression_ratio': 0.0\n        }\n        self.lock = threading.Lock()\n        \n        logger.info("Integrated Memory Management System initialized")\n\n    def allocate_tensor(self, shape: tuple, dtype: torch.dtype, tensor_type: str = "general") -> torch.Tensor:\n        """\n        Allocate a tensor with integrated memory optimization\n        \n        Args:\n            shape: Shape of the tensor\n            dtype: Data type of the tensor\n            tensor_type: Type of tensor (e.g., "kv_cache", "activation", "gradient")\n            \n        Returns:\n            Allocated tensor\n        """\n        with self.lock:\n            # Use the memory optimizer to allocate with cache-aware optimizations\n            tensor = self.memory_optimizer.allocate_tensor_memory(shape, dtype, tensor_type)\n            \n            # Register the tensor with the lifecycle manager\n            tensor_id = f"tensor_{id(tensor)}_{int(time.time())}"\n            size_bytes = tensor.element_size() * tensor.nelement()\n            \n            tensor_metadata = TensorMetadata(\n                tensor_id=tensor_id,\n                original_shape=shape,\n                dtype=dtype,\n                size_bytes=size_bytes,\n                creation_time=time.time(),\n                tensor_type=tensor_type\n            )\n            \n            self.tensor_registry[tensor_id] = tensor_metadata\n            self.tensor_data[tensor_id] = tensor\n            self.system_stats['tensors_managed'] += 1\n            self.system_stats['total_bytes_managed'] += size_bytes\n            \n            # Register with lifecycle manager\n            self.lifecycle_manager.register_tensor(tensor_id, tensor, tensor_type)\n\n            conditional_debug(logger, f"Allocated tensor {tensor_id} with shape {shape}")\n            return tensor\n\n    def free_tensor(self, tensor: Union[torch.Tensor, str], tensor_type: str = "general") -> bool:\n        """\n        Free a tensor managed by the system\n        \n        Args:\n            tensor: Either the tensor itself or its ID\n            tensor_type: Type of tensor to free\n            \n        Returns:\n            True if successfully freed\n        """\n        with self.lock:\n            if isinstance(tensor, torch.Tensor):\n                # Find tensor ID by looking up the tensor reference\n                tensor_id = None\n                for tid, t_data in self.tensor_data.items():\n                    if torch.is_tensor(t_data) and t_data.data_ptr() == tensor.data_ptr():\n                        tensor_id = tid\n                        break\n            else:\n                tensor_id = tensor  # Already a string ID\n            \n            if tensor_id and tensor_id in self.tensor_registry:\n                # Update lifecycle manager\n                self.lifecycle_manager.update_access_pattern(tensor_id)\n                \n                # Remove from registry and data\n                del self.tensor_registry[tensor_id]\n                if tensor_id in self.tensor_data:\n                    tensor_obj = self.tensor_data[tensor_id]\n                    if torch.is_tensor(tensor_obj):\n                        size = tensor_obj.element_size() * tensor_obj.nelement()\n                        self.system_stats['total_bytes_managed'] -= size\n                    del self.tensor_data[tensor_id]\n                \n                # Force cleanup in lifecycle manager\n                self.lifecycle_manager.force_cleanup_tensor(tensor_id)\n                \n                # Free from memory optimizer\n                self.memory_optimizer.free_tensor_memory(tensor_obj if torch.is_tensor(tensor_obj) else None, tensor_type)\n\n                conditional_debug(logger, f"Freed tensor {tensor_id}")\n                return True\n            else:\n                logger.warning(f"Attempted to free unmanaged tensor: {tensor_id}")\n                return False\n\n    def compress_tensor(self, tensor: Union[torch.Tensor, str]) -> Union[torch.Tensor, bytes]:\n        """\n        Compress a tensor to save memory\n        \n        Args:\n            tensor: Either the tensor itself or its ID\n            \n        Returns:\n            Compressed tensor data (bytes)\n        """\n        with self.lock:\n            if isinstance(tensor, torch.Tensor):\n                # Find tensor ID by looking up the tensor reference\n                tensor_id = None\n                for tid, t_data in self.tensor_data.items():\n                    if torch.is_tensor(t_data) and t_data.data_ptr() == tensor.data_ptr():\n                        tensor_id = tid\n                        break\n                tensor_obj = tensor\n            else:\n                tensor_id = tensor\n                tensor_obj = self.tensor_data[tensor_id]\n            \n            if tensor_id and torch.is_tensor(tensor_obj):\n                # Compress the tensor\n                compressed_data = self.compression_system.compress(tensor_obj)\n                \n                # Update metadata\n                metadata = self.tensor_registry[tensor_id]\n                metadata.compression_status = "compressed"\n                self.system_stats['bytes_compressed'] += len(compressed_data)\n                \n                # Replace tensor with compressed data in our registry\n                self.tensor_data[tensor_id] = compressed_data\n\n                conditional_debug(logger, f"Compressed tensor {tensor_id}, size reduced from {tensor_obj.nelement() * tensor_obj.element_size()} to {len(compressed_data)} bytes")\n                return compressed_data\n            else:\n                logger.warning(f"Cannot compress unmanaged tensor: {tensor_id}")\n                return tensor_obj if not isinstance(tensor, str) else None\n\n    def decompress_tensor(self, tensor_id: str) -> torch.Tensor:\n        """\n        Decompress a tensor\n        \n        Args:\n            tensor_id: ID of the compressed tensor\n            \n        Returns:\n            Decompressed tensor\n        """\n        with self.lock:\n            if tensor_id not in self.tensor_registry:\n                raise ValueError(f"Tensor {tensor_id} not found in registry")\n            \n            metadata = self.tensor_registry[tensor_id]\n            if metadata.compression_status != "compressed":\n                logger.warning(f"Tensor {tensor_id} is not compressed")\n                return self.tensor_data[tensor_id] if torch.is_tensor(self.tensor_data[tensor_id]) else None\n            \n            compressed_data = self.tensor_data[tensor_id]\n            if not isinstance(compressed_data, bytes):\n                raise ValueError(f"Tensor {tensor_id} is not in compressed format")\n            \n            # Decompress\n            decompressed_tensor = self.compression_system.decompress(compressed_data, metadata.dtype)\n            \n            # Update metadata\n            metadata.compression_status = "uncompressed"\n            self.system_stats['bytes_compressed'] -= len(compressed_data)\n            \n            # Replace compressed data with decompressed tensor\n            self.tensor_data[tensor_id] = decompressed_tensor\n\n            conditional_debug(logger, f"Decompressed tensor {tensor_id}")\n            return decompressed_tensor\n\n    def tier_tensor(self, tensor: Union[torch.Tensor, str], priority: str = "normal") -> str:\n        """\n        Tier a tensor to appropriate memory level based on priority and usage\n        \n        Args:\n            tensor: Either the tensor itself or its ID\n            priority: Priority level ("high", "normal", "low")\n            \n        Returns:\n            Tier assigned to the tensor\n        """\n        # For now, this is a simplified implementation\n        # In a real implementation, this would interface with the tiering system\n        with self.lock:\n            if isinstance(tensor, torch.Tensor):\n                # Find tensor ID by looking up the tensor reference\n                tensor_id = None\n                for tid, t_data in self.tensor_data.items():\n                    if torch.is_tensor(t_data) and t_data.data_ptr() == tensor.data_ptr():\n                        tensor_id = tid\n                        break\n                tensor_obj = tensor\n            else:\n                tensor_id = tensor\n                tensor_obj = self.tensor_data[tensor_id]\n            \n            if not tensor_id:\n                raise ValueError("Tensor not found in system")\n            \n            # Determine tier based on priority and tensor type\n            if priority == "high":\n                tier = "gpu"\n            elif priority == "normal":\n                tier = "cpu"\n            else:\n                tier = "disk"\n            \n            # Update metadata\n            metadata = self.tensor_registry[tensor_id]\n            metadata.tier = tier\n            \n            self.system_stats['tiering_operations'] += 1\n\n            conditional_debug(logger, f"Tiered tensor {tensor_id} to {tier} tier")\n            return tier\n\n    def update_tensor_lifecycle(self, tensor: Union[torch.Tensor, str], access_pattern: str) -> None:\n        """\n        Update the lifecycle of a tensor based on access pattern\n        \n        Args:\n            tensor: Either the tensor itself or its ID\n            access_pattern: Pattern of access ("sequential", "random", "sparse")\n        """\n        with self.lock:\n            if isinstance(tensor, torch.Tensor):\n                # Find tensor ID by looking up the tensor reference\n                tensor_id = None\n                for tid, t_data in self.tensor_data.items():\n                    if torch.is_tensor(t_data) and t_data.data_ptr() == tensor.data_ptr():\n                        tensor_id = tid\n                        break\n            else:\n                tensor_id = tensor\n            \n            if tensor_id:\n                # Update in the lifecycle manager\n                tensor_obj = self.tensor_data[tensor_id] if torch.is_tensor(self.tensor_data[tensor_id]) else torch.randn(1)\n                self.lifecycle_manager.update_access_pattern(tensor_id, tensor_obj)\n\n    def process_cross_modal_attention(self, image_tensors: torch.Tensor, text_tensors: torch.Tensor) -> torch.Tensor:\n        """\n        Process cross-modal attention with memory-optimized tensors\n        \n        Args:\n            image_tensors: Image feature tensors\n            text_tensors: Text embedding tensors\n            \n        Returns:\n            Attention result tensor\n        """\n        # This would implement cross-modal attention with memory optimization\n        # For now, return a mock result\n        \n        # Determine output dimensions\n        batch_size = min(image_tensors.shape[0], text_tensors.shape[0])\n        output_dim = min(image_tensors.shape[-1], text_tensors.shape[-1])\n        \n        # Create result tensor with our allocation system\n        result_shape = (batch_size, image_tensors.shape[1], output_dim)\n        result = self.allocate_tensor(result_shape, torch.float32, "attention_result")\n        \n        # Simulate cross-modal attention computation\n        # (In a real implementation, this would be the actual attention mechanism)\n        if image_tensors.device != result.device:\n            image_tensors = image_tensors.to(result.device)\n        if text_tensors.device != result.device:\n            text_tensors = text_tensors.to(result.device)\n            \n        # Simple attention simulation\n        result = torch.matmul(\n            torch.mean(image_tensors, dim=1, keepdim=True),  # Mean pool spatial dims\n            text_tensors.transpose(-2, -1)\n        )\n        \n        # Update lifecycle for this result\n        self.update_tensor_lifecycle(result, "sequential")\n        \n        return result\n\n    def get_system_stats(self) -> Dict[str, Any]:\n        """\n        Get comprehensive system statistics\n        \n        Returns:\n            Dictionary with system statistics\n        """\n        with self.lock:\n            # Calculate compression ratio\n            total_original_size = sum(meta.size_bytes for meta in self.tensor_registry.values())\n            if total_original_size > 0:\n                compression_ratio = self.system_stats['bytes_compressed'] / total_original_size\n                self.system_stats['compression_ratio'] = compression_ratio\n                self.system_stats['avg_compression_ratio'] = compression_ratio\n            else:\n                self.system_stats['compression_ratio'] = 0.0\n                self.system_stats['avg_compression_ratio'] = 0.0\n\n            # Add stats from other components\n            lifecycle_stats = self.lifecycle_manager.get_lifecycle_stats()\n            \n            return {\n                **self.system_stats,\n                'lifecycle_stats': lifecycle_stats,\n                'active_tensors': len([tid for tid in self.tensor_registry.keys() \n                                     if self.lifecycle_manager.should_retain_tensor(tid)])\n            }\n\n    def cleanup(self) -> None:\n        """Cleanup all managed resources"""\n        with self.lock:\n            # Clean up unused tensors\n            tensors_to_remove = self.lifecycle_manager.get_cleanup_candidates()\n            for tensor_id in tensors_to_remove:\n                self.free_tensor(tensor_id)\n            \n            # Cleanup individual components if they have cleanup methods\n            if hasattr(self.memory_optimizer, 'cleanup'):\n                self.memory_optimizer.cleanup()\n            \n            if hasattr(self.lifecycle_manager, 'cleanup_unused_tensors'):\n                removed_tensors = self.lifecycle_manager.cleanup_unused_tensors()\n                self.system_stats['tensors_managed'] -= len(removed_tensors)\n\n    def get_tensor_info(self, tensor_id: str) -> Optional[TensorMetadata]:\n        """\n        Get metadata for a specific tensor\n        \n        Args:\n            tensor_id: ID of the tensor\n            \n        Returns:\n            Tensor metadata or None if not found\n        """\n        return self.tensor_registry.get(tensor_id)\n\n    def predict_tensor_lifecycle(self, tensor_id: str) -> Dict[str, Any]:\n        """\n        Predict the lifecycle of a specific tensor\n        \n        Args:\n            tensor_id: ID of the tensor to predict for\n            \n        Returns:\n            Lifecycle prediction dictionary\n        """\n        return self.lifecycle_manager.predict_tensor_lifecycle(tensor_id)