"""\nComprehensive KV Cache Optimization System for Qwen3-VL Model\nImplements Phase 2.85: KV Cache Optimization Strategies\nFeatures:\n1. Low-rank approximation techniques for KV cache compression\n2. Sliding window attention to limit cache size\n3. Efficient KV cache allocation for vision-language tasks\n4. Integration with existing memory management system\n"""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict, Any, Union\nfrom dataclasses import dataclass\nimport math\nimport numpy as np\nfrom memory_management.memory_manager import MemoryManager, MemoryConfig\n\n\n@dataclass\nclass KVCacheConfig:\n    """\n    Configuration for KV cache optimization.\n    """\n    # Low-rank approximation settings\n    use_low_rank: bool = True\n    low_rank_dimension: int = 64  # Rank for low-rank approximation\n    low_rank_method: str = "svd"  # Options: "svd", "random", "learned"\n    \n    # Sliding window settings\n    use_sliding_window: bool = True\n    sliding_window_size: int = 1024\n    \n    # Hybrid approach settings\n    use_hybrid: bool = True  # Combine low-rank and sliding window\n    \n    # Memory management settings\n    memory_efficient_allocation: bool = True\n    cache_compression_threshold: float = 0.1  # Threshold for compression activation\n    \n    # Vision-language specific settings\n    vision_language_optimized: bool = True\n    vision_seq_limit: int = 576  # Max sequence length for vision tokens\n    language_seq_limit: int = 2048  # Max sequence length for language tokens\n\n\nclass LowRankKVCompressor:\n    """\n    Low-rank approximation for KV cache compression using SVD or other methods.\n    """\n    def __init__(self, config: KVCacheConfig, memory_manager: MemoryManager):\n        self.config = config\n        self.memory_manager = memory_manager\n        self.method = config.low_rank_method\n        self.rank = config.low_rank_dimension\n\n    def compress(self, tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Compress a tensor using low-rank approximation.\n        Returns (left_matrix, right_matrix) such that tensor ≈ left @ right\n        """\n        if tensor.dim() != 3:  # [batch, seq_len, features]\n            raise ValueError(f"Expected 3D tensor, got {tensor.dim()}D")\n\n        batch_size, seq_len, feature_dim = tensor.shape\n        \n        # For very small tensors, return original to avoid numerical issues\n        if min(seq_len, feature_dim) <= self.rank:\n            # Use memory manager to allocate for the original tensor\n            result = tensor.clone()\n            return result, torch.eye(feature_dim, dtype=tensor.dtype, device=tensor.device)\n        \n        # Use different compression methods based on config\n        if self.method == "svd":\n            return self._svd_compress(tensor)\n        elif self.method == "random":\n            return self._random_compress(tensor)\n        elif self.method == "learned":\n            return self._learned_compress(tensor)\n        else:\n            raise ValueError(f"Unknown compression method: {self.method}")\n\n    def _svd_compress(self, tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """Compress using SVD decomposition."""\n        batch_size, seq_len, feature_dim = tensor.shape\n        \n        # Process each batch item separately to handle different sequences\n        left_matrices = []\n        right_matrices = []\n        \n        for i in range(batch_size):\n            matrix = tensor[i]  # [seq_len, feature_dim]\n            \n            try:\n                # Perform SVD\n                U, S, Vh = torch.linalg.svd(matrix, full_matrices=False)\n                \n                # Truncate to rank\n                U_trunc = U[:, :self.rank]\n                S_trunc = S[:self.rank]\n                Vh_trunc = Vh[:self.rank, :]\n                \n                # Create low-rank approximation: A ≈ U_trunc @ diag(S_trunc) @ Vh_trunc\n                # We'll return U_trunc @ sqrt(diag(S_trunc)) and sqrt(diag(S_trunc)) @ Vh_trunc\n                sqrt_S = torch.sqrt(S_trunc)\n                left = U_trunc * sqrt_S  # [seq_len, rank]\n                right = sqrt_S.unsqueeze(1) * Vh_trunc  # [rank, feature_dim]\n                \n                left_matrices.append(left)\n                right_matrices.append(right)\n            except:\n                # If SVD fails, use a simple approach\n                left = tensor[i, :, :self.rank] if feature_dim >= self.rank else F.pad(\n                    tensor[i], (0, self.rank - feature_dim)\n                )\n                right = torch.eye(self.rank, feature_dim, dtype=tensor.dtype, device=tensor.device)\n                \n                left_matrices.append(left)\n                right_matrices.append(right)\n        \n        left_stack = torch.stack(left_matrices, dim=0)  # [batch, seq_len, rank]\n        right_stack = torch.stack(right_matrices, dim=0)  # [batch, rank, feature_dim]\n        \n        return left_stack, right_stack\n\n    def _random_compress(self, tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """Compress using random projection."""\n        batch_size, seq_len, feature_dim = tensor.shape\n        \n        # Create random projection matrices\n        left = torch.randn(batch_size, seq_len, self.rank, dtype=tensor.dtype, device=tensor.device)\n        right = torch.randn(batch_size, self.rank, feature_dim, dtype=tensor.dtype, device=tensor.device)\n        \n        return left, right\n\n    def _learned_compress(self, tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """Placeholder for learned compression (would require training)."""\n        # For now, just use SVD as a default\n        return self._svd_compress(tensor)\n\n    def decompress(self, left: torch.Tensor, right: torch.Tensor) -> torch.Tensor:\n        """\n        Decompress tensors back to original shape.\n        """\n        return torch.matmul(left, right)  # [batch, seq_len, feature_dim]\n\n\nclass SlidingWindowKVCache:\n    """\n    KV cache with sliding window mechanism to limit cache size.\n    """\n    def __init__(self, config: KVCacheConfig, memory_manager: MemoryManager):\n        self.config = config\n        self.memory_manager = memory_manager\n        self.window_size = config.sliding_window_size\n        self.use_low_rank = config.use_low_rank\n\n        # If using low-rank compression, create compressor\n        if self.use_low_rank:\n            self.compressor = LowRankKVCompressor(config, memory_manager)\n\n        # Initialize cache storage\n        self.k_cache = None\n        self.v_cache = None\n        self.current_position = 0\n        self.cache_full = False\n\n    def update(self, key_states: torch.Tensor, value_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Update the sliding window KV cache with new states.\n        """\n        batch_size, num_heads, seq_len, head_dim = key_states.shape\n\n        # Calculate how many positions we need to store\n        total_new_tokens = seq_len\n\n        # If cache doesn't exist, initialize it\n        if self.k_cache is None:\n            max_seq_len = self.window_size\n            if self.use_low_rank:\n                # For low-rank, we store compressed representations\n                self.k_cache = torch.zeros(\n                    batch_size, num_heads, max_seq_len, self.config.low_rank_dimension,\n                    dtype=key_states.dtype, device=key_states.device\n                )\n                self.v_cache = torch.zeros(\n                    batch_size, num_heads, max_seq_len, self.config.low_rank_dimension,\n                    dtype=value_states.dtype, device=value_states.device\n                )\n            else:\n                self.k_cache = torch.zeros(\n                    batch_size, num_heads, max_seq_len, head_dim,\n                    dtype=key_states.dtype, device=key_states.device\n                )\n                self.v_cache = torch.zeros(\n                    batch_size, num_heads, max_seq_len, head_dim,\n                    dtype=value_states.dtype, device=value_states.device\n                )\n\n        # Determine where to store new values in the sliding window\n        if self.current_position + total_new_tokens <= self.window_size:\n            # No wraparound needed\n            start_pos = self.current_position\n            end_pos = start_pos + total_new_tokens\n\n            if self.use_low_rank:\n                # Compress the new states before storing\n                for head_idx in range(num_heads):\n                    k_head = key_states[:, head_idx, :, :]  # [batch, seq_len, head_dim]\n                    v_head = value_states[:, head_idx, :, :]  # [batch, seq_len, head_dim]\n\n                    k_left, k_right = self.compressor.compress(k_head)\n                    v_left, v_right = self.compressor.compress(v_head)\n\n                    # Store the left part of the decomposition, ensuring shape compatibility\n                    actual_rank = min(k_left.shape[-1], self.config.low_rank_dimension)\n                    self.k_cache[:, head_idx, start_pos:end_pos, :actual_rank] = k_left[:, :, :actual_rank]\n                    actual_rank_v = min(v_left.shape[-1], self.config.low_rank_dimension)\n                    self.v_cache[:, head_idx, start_pos:end_pos, :actual_rank_v] = v_left[:, :, :actual_rank_v]\n            else:\n                self.k_cache[:, :, start_pos:end_pos, :] = key_states\n                self.v_cache[:, :, start_pos:end_pos, :] = value_states\n\n            self.current_position = end_pos\n        else:\n            # Wraparound needed - store in multiple parts\n            remaining_space = self.window_size - self.current_position\n\n            if self.use_low_rank:\n                # Handle low-rank compression with wraparound\n                for head_idx in range(num_heads):\n                    k_head = key_states[:, head_idx, :, :]  # [batch, seq_len, head_dim]\n                    v_head = value_states[:, head_idx, :, :]  # [batch, seq_len, head_dim]\n\n                    k_left, k_right = self.compressor.compress(k_head)\n                    v_left, v_right = self.compressor.compress(v_head)\n\n                    # Store first part\n                    first_part_size = min(remaining_space, total_new_tokens)\n                    actual_rank = min(k_left.shape[-1], self.config.low_rank_dimension)\n                    self.k_cache[:, head_idx, self.current_position:self.current_position + first_part_size, :actual_rank] = k_left[:, :first_part_size, :actual_rank]\n                    actual_rank_v = min(v_left.shape[-1], self.config.low_rank_dimension)\n                    self.v_cache[:, head_idx, self.current_position:self.current_position + first_part_size, :actual_rank_v] = v_left[:, :first_part_size, :actual_rank_v]\n\n                    # Store remaining part at the beginning if needed\n                    if total_new_tokens > remaining_space:\n                        remaining_k = k_left[:, first_part_size:, :]\n                        remaining_v = v_left[:, first_part_size:, :]\n                        remaining_tokens = remaining_k.shape[1]  # How many tokens remain\n                        actual_rank_rem = min(remaining_k.shape[-1], self.config.low_rank_dimension)\n                        # Make sure we don't exceed the window size\n                        tokens_to_store = min(remaining_tokens, self.window_size)\n                        self.k_cache[:, head_idx, :tokens_to_store, :actual_rank_rem] = remaining_k[:, :tokens_to_store, :actual_rank_rem]\n                        actual_rank_v_rem = min(remaining_v.shape[-1], self.config.low_rank_dimension)\n                        self.v_cache[:, head_idx, :tokens_to_store, :actual_rank_v_rem] = remaining_v[:, :tokens_to_store, :actual_rank_v_rem]\n            else:\n                # Store first part\n                first_part_size = min(remaining_space, total_new_tokens)\n                self.k_cache[:, :, self.current_position:self.current_position + first_part_size, :] = key_states[:, :, :first_part_size, :]\n                self.v_cache[:, :, self.current_position:self.current_position + first_part_size, :] = value_states[:, :, :first_part_size, :]\n\n                # Store remaining part at the beginning if needed\n                if total_new_tokens > remaining_space:\n                    remaining_k = key_states[:, :, first_part_size:, :]\n                    remaining_v = value_states[:, :, first_part_size:, :]\n                    self.k_cache[:, :, :remaining_k.shape[2], :] = remaining_k\n                    self.v_cache[:, :, :remaining_v.shape[2], :] = remaining_v\n\n            # Update position with wraparound\n            self.current_position = (self.current_position + total_new_tokens) % self.window_size\n            self.cache_full = True\n\n        # Return the full window content (or available content)\n        if self.cache_full:\n            # Return the complete window\n            k_out = self.k_cache\n            v_out = self.v_cache\n        else:\n            # Return up to current position\n            k_out = self.k_cache[:, :, :self.current_position, :]\n            v_out = self.v_cache[:, :, :self.current_position, :]\n\n        # If using low-rank, return the compressed representation\n        if self.use_low_rank:\n            return k_out, v_out\n        else:\n            return k_out, v_out\n\n    def get_current_length(self) -> int:\n        """Get the current effective sequence length in cache."""\n        return min(self.current_position, self.window_size) if not self.cache_full else self.window_size\n\n    def reset(self):\n        """Reset the cache."""\n        self.k_cache = None\n        self.v_cache = None\n        self.current_position = 0\n        self.cache_full = False\n\n\nclass HybridKVCache:\n    """\n    Hybrid KV cache combining low-rank approximation with sliding window attention.\n    """\n    def __init__(self, config: KVCacheConfig, memory_manager: MemoryManager):\n        self.config = config\n        self.memory_manager = memory_manager\n        \n        # Initialize both components\n        self.sliding_window = SlidingWindowKVCache(config, memory_manager)\n        \n        # Low-rank compressor for the hybrid approach\n        if config.use_low_rank:\n            self.compressor = LowRankKVCompressor(config, memory_manager)\n\n    def update(self, key_states: torch.Tensor, value_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Update the hybrid KV cache with new states.\n        First applies sliding window, then low-rank approximation.\n        """\n        # First apply sliding window to limit sequence length\n        k_windowed, v_windowed = self.sliding_window.update(key_states, value_states)\n        \n        # Then apply low-rank approximation if enabled\n        if self.config.use_low_rank:\n            batch_size, num_heads, seq_len, head_dim = k_windowed.shape\n            \n            # Compress the windowed results\n            k_compressed_list = []\n            v_compressed_list = []\n            \n            for head_idx in range(num_heads):\n                k_head = k_windowed[:, head_idx, :, :]  # [batch, seq_len, head_dim]\n                v_head = v_windowed[:, head_idx, :, :]  # [batch, seq_len, head_dim]\n                \n                k_left, k_right = self.compressor.compress(k_head)\n                v_left, v_right = self.compressor.compress(v_head)\n                \n                # Store the compressed representation\n                k_compressed_list.append(k_left.unsqueeze(1))  # [batch, 1, seq_len, rank]\n                v_compressed_list.append(v_left.unsqueeze(1))  # [batch, 1, seq_len, rank]\n            \n            k_compressed = torch.cat(k_compressed_list, dim=1)  # [batch, num_heads, seq_len, rank]\n            v_compressed = torch.cat(v_compressed_list, dim=1)  # [batch, num_heads, seq_len, rank]\n            \n            return k_compressed, v_compressed\n        else:\n            return k_windowed, v_windowed\n\n    def get_current_length(self) -> int:\n        """Get the current effective sequence length in cache."""\n        return self.sliding_window.get_current_length()\n\n    def reset(self):\n        """Reset the cache."""\n        self.sliding_window.reset()\n\n\nclass VisionLanguageKVCache:\n    """\n    Optimized KV cache for vision-language tasks with specialized handling.\n    """\n    def __init__(self, config: KVCacheConfig, memory_manager: MemoryManager, layer_idx: Optional[int] = None):\n        self.config = config\n        self.memory_manager = memory_manager\n        self.layer_idx = layer_idx\n        \n        # Create specialized caches for vision and language components\n        if config.use_hybrid:\n            # Use hybrid cache for both vision and language\n            vision_config = KVCacheConfig(\n                use_low_rank=config.use_low_rank,\n                low_rank_dimension=config.low_rank_dimension,\n                use_sliding_window=config.use_sliding_window,\n                sliding_window_size=min(config.sliding_window_size, config.vision_seq_limit),\n                use_hybrid=True,\n                memory_efficient_allocation=config.memory_efficient_allocation\n            )\n            self.vision_cache = HybridKVCache(vision_config, memory_manager)\n            \n            language_config = KVCacheConfig(\n                use_low_rank=config.use_low_rank,\n                low_rank_dimension=config.low_rank_dimension,\n                use_sliding_window=config.use_sliding_window,\n                sliding_window_size=min(config.sliding_window_size, config.language_seq_limit),\n                use_hybrid=True,\n                memory_efficient_allocation=config.memory_efficient_allocation\n            )\n            self.language_cache = HybridKVCache(language_config, memory_manager)\n        else:\n            self.vision_cache = SlidingWindowKVCache(\n                KVCacheConfig(\n                    use_low_rank=config.use_low_rank,\n                    low_rank_dimension=config.low_rank_dimension,\n                    use_sliding_window=config.use_sliding_window,\n                    sliding_window_size=min(config.sliding_window_size, config.vision_seq_limit),\n                    memory_efficient_allocation=config.memory_efficient_allocation\n                ),\n                memory_manager\n            )\n            self.language_cache = SlidingWindowKVCache(\n                KVCacheConfig(\n                    use_low_rank=config.use_low_rank,\n                    low_rank_dimension=config.low_rank_dimension,\n                    use_sliding_window=config.use_sliding_window,\n                    sliding_window_size=min(config.sliding_window_size, config.language_seq_limit),\n                    memory_efficient_allocation=config.memory_efficient_allocation\n                ),\n                memory_manager\n            )\n        \n        # Track which cache to use\n        self.current_mode = "language"  # Default to language\n\n    def update(self, key_states: torch.Tensor, value_states: torch.Tensor,\n               is_vision: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Update the vision-language KV cache with new states.\n        """\n        if is_vision:\n            if hasattr(self.vision_cache, 'update'):\n                return self.vision_cache.update(key_states, value_states)\n            else:\n                # If vision cache doesn't have update method, return original\n                return key_states, value_states\n        else:\n            if hasattr(self.language_cache, 'update'):\n                return self.language_cache.update(key_states, value_states)\n            else:\n                # If language cache doesn't have update method, return original\n                return key_states, value_states\n\n    def get_current_length(self, is_vision: bool = False) -> int:\n        """Get the current effective sequence length in cache."""\n        if is_vision:\n            return self.vision_cache.get_current_length()\n        else:\n            return self.language_cache.get_current_length()\n\n    def reset(self, is_vision: bool = False):\n        """Reset the cache."""\n        if is_vision:\n            self.vision_cache.reset()\n        else:\n            self.language_cache.reset()\n\n\nclass OptimizedKVCacheManager:\n    """\n    Main KV cache manager that integrates optimization techniques with memory management.\n    """\n    def __init__(self, config: KVCacheConfig, memory_manager: MemoryManager):\n        self.config = config\n        self.memory_manager = memory_manager\n\n        # Initialize the appropriate cache based on configuration\n        if config.vision_language_optimized:\n            self.cache = VisionLanguageKVCache(config, memory_manager)\n        elif config.use_hybrid:\n            self.cache = HybridKVCache(config, memory_manager)\n        elif config.use_sliding_window:\n            self.cache = SlidingWindowKVCache(config, memory_manager)\n        else:\n            # Fallback to standard caching\n            self.cache = None\n\n        # Track memory usage\n        self.original_memory_usage = 0\n        self.compressed_memory_usage = 0\n        self.compression_ratio = 1.0\n\n    def update(self, key_states: torch.Tensor, value_states: torch.Tensor,\n               is_vision: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n        """\n        Update the KV cache with new states using optimized techniques.\n        For compatibility with attention mechanisms, this should return tensors\n        with the same dimensions as input (except for sequence length).\n        """\n        original_seq_len = key_states.size(-2)  # sequence length dimension\n        original_head_dim = key_states.size(-1)  # head dimension\n\n        # Calculate original memory usage\n        original_elements = key_states.numel() + value_states.numel()\n        self.original_memory_usage += original_elements * key_states.element_size()\n\n        if self.cache is not None:\n            # Use optimized cache\n            k_out, v_out = self.cache.update(key_states, value_states, is_vision=is_vision)\n\n            # For memory efficiency, we should keep compressed representations\n        # and only expand when necessary for computation\n        else:\n            # Fallback to standard behavior\n            k_out, v_out = key_states, value_states\n\n        # Calculate compressed memory usage (based on internal compressed storage)\n        # This is an approximation since we expand the output for compatibility\n        # In a real implementation, we would track the actual compressed size internally\n        compressed_elements = k_out.numel() + v_out.numel()\n        self.compressed_memory_usage += compressed_elements * k_out.element_size()\n\n        # Update compression ratio\n        if self.original_memory_usage > 0:\n            self.compression_ratio = self.compressed_memory_usage / self.original_memory_usage\n\n        return k_out, v_out\n\n    def get_memory_stats(self) -> Dict[str, Any]:\n        """Get memory usage statistics."""\n        return {\n            'original_memory_usage': self.original_memory_usage,\n            'compressed_memory_usage': self.compressed_memory_usage,\n            'compression_ratio': self.compression_ratio,\n            'memory_saved_bytes': self.original_memory_usage - self.compressed_memory_usage,\n            'memory_saved_percentage': (1 - self.compression_ratio) * 100 if self.compression_ratio != 0 else 0\n        }\n\n    def reset(self, is_vision: bool = False):\n        """Reset the cache."""\n        if self.cache is not None:\n            self.cache.reset(is_vision=is_vision)\n            \n            # Reset statistics\n            self.original_memory_usage = 0\n            self.compressed_memory_usage = 0\n            self.compression_ratio = 1.0\n\n\nclass KVCacheOptimizedAttention(nn.Module):\n    """\n    Attention mechanism with optimized KV caching strategies.\n    """\n    def __init__(self, config: KVCacheConfig, memory_manager: MemoryManager, \n                 hidden_size: int, num_attention_heads: int, \n                 max_position_embeddings: int = 2048, rope_theta: float = 10000.0):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        self.num_attention_heads = num_attention_heads\n        self.head_dim = self.hidden_size // self.num_attention_heads\n        self.num_key_value_heads = self.num_attention_heads  # Assuming GQA is not used\n        self.num_key_value_groups = 1\n        self.max_position_embeddings = max_position_embeddings\n        self.rope_theta = rope_theta\n\n        if (self.head_dim * self.num_attention_heads) != self.hidden_size:\n            raise ValueError(\n                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"\n                f" and `num_heads`: {self.num_attention_heads})."\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_attention_heads * self.head_dim, bias=True)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n        self.o_proj = nn.Linear(self.num_attention_heads * self.head_dim, self.hidden_size, bias=True)\n\nfrom attention.rotary_embeddings import Qwen3VLRotaryEmbedding\n        self.rotary_emb = Qwen3VLRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n        # Initialize optimized KV cache\n        self.kv_cache_manager = OptimizedKVCacheManager(config, memory_manager)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        is_vision: bool = False,  # Flag to indicate if processing vision tokens\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        # Project queries, keys, and values\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Reshape to multi-head format\n        query_states = query_states.view(bsz, q_len, self.num_attention_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        # Apply rotary embeddings to current states before caching\n        if position_ids is None:\n            position_ids = torch.arange(q_len, dtype=torch.long, device=hidden_states.device).unsqueeze(0).expand(bsz, -1)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        # Handle KV caching with optimized strategies\n        if use_cache:\n            # Update optimized KV cache with already rotated states\n            key_states, value_states = self.kv_cache_manager.update(\n                key_states, value_states, is_vision=is_vision\n            )\n\n            # For compatibility, return cache as past_key_value\n            past_key_value = (key_states, value_states)\n\n            # Check if we now have compressed representations (low-rank)\n            if key_states.size(-1) < self.head_dim or value_states.size(-1) < self.head_dim:\n                # We have low-rank compressed states, need to handle attention differently\n                # Use the actual dimensions of the compressed states\n                actual_key_dim = key_states.size(-1)\n                actual_value_dim = value_states.size(-1)\n\n                # Repeat keys and values for GQA (Grouped Query Attention) if applicable\nfrom attention.attention_mechanisms import repeat_kv\n                key_states = repeat_kv(key_states, self.num_key_value_groups)\n                value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n                # Compute attention with compressed dimensions\n                # This is a simplified handling - a full implementation would need to\n                # handle the low-rank decomposition appropriately\n                attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(actual_key_dim)\n\n                if attention_mask is not None:  # no matter the length, we just slice it\n                    causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n                    attn_weights = attn_weights + causal_mask\n\n                # Upcast attention to fp32\n                attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n                attn_output = torch.matmul(attn_weights, value_states)\n\n                # If the output dimension is different, we need to project back\n                if actual_value_dim != self.head_dim:\n                    # Project the compressed attention output back to full dimension\n                    attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n                    # We need a projection layer to map from compressed to full dimension\n                    # This is a simplified approach - in a real implementation, this would be a parameterized projection\n                    if hasattr(self, 'compress_to_full_proj'):\n                        # Use existing projection if available\n                        pass\n                    else:\n                        # For now, we'll just pad the dimensions to match\n                        expanded_output = torch.zeros(bsz, q_len, self.hidden_size, dtype=attn_output.dtype, device=attn_output.device)\n                        head_dim_size = min(actual_value_dim, self.head_dim)\n                        for head_idx in range(self.num_attention_heads):\n                            start_idx = head_idx * self.head_dim\n                            end_idx = start_idx + head_dim_size\n                            expanded_output[:, :, start_idx:end_idx] = attn_output[:, :, start_idx:end_idx]\n                        attn_output = expanded_output\n                else:\n                    attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, self.hidden_size)\n            else:\n                # Standard attention computation when no compression is applied\nfrom attention.attention_mechanisms import repeat_kv\n                key_states = repeat_kv(key_states, self.num_key_value_groups)\n                value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n                # Compute attention\n                attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n                if attention_mask is not None:  # no matter the length, we just slice it\n                    causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n                    attn_weights = attn_weights + causal_mask\n\n                # Upcast attention to fp32\n                attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n                attn_output = torch.matmul(attn_weights, value_states)\n\n                attn_output = attn_output.transpose(1, 2).contiguous()\n                attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n        else:\n            # No caching, proceed with standard attention\nfrom attention.attention_mechanisms import repeat_kv\n            key_states = repeat_kv(key_states, self.num_key_value_groups)\n            value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n            # Compute attention\n            attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n            if attention_mask is not None:  # no matter the length, we just slice it\n                causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n                attn_weights = attn_weights + causal_mask\n\n            # Upcast attention to fp32\n            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n            attn_output = torch.matmul(attn_weights, value_states)\n\n            attn_output = attn_output.transpose(1, 2).contiguous()\n            attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def get_cache_stats(self) -> Dict[str, Any]:\n        """Get cache statistics."""\n        return self.kv_cache_manager.get_memory_stats()\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    """\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep).\n    The hidden states go from (batch, num_key_value_heads, seqlen, head_dim) to\n    (batch, num_attention_heads, seqlen, head_dim)\n    """\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\ndef rotate_half(x):\n    """Rotates half the hidden dims of the input."""\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n    """Applies Rotary Position Embedding to the query and key tensors."""\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass Qwen3VLRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n        self.register_buffer("inv_freq", inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != "mps" else "cpu"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\n# Helper function to create optimized attention with default configuration\ndef create_optimized_attention_with_cache(\n    hidden_size: int,\n    num_attention_heads: int,\n    memory_manager: Optional[MemoryManager] = None,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n    max_position_embeddings: int = 2048,\n    rope_theta: float = 10000.0\n) -> KVCacheOptimizedAttention:\n    """\n    Create an attention module with optimized KV caching.\n    """\n    if memory_manager is None:\n        memory_manager = MemoryManager()\n    \n    if kv_cache_config is None:\n        kv_cache_config = KVCacheConfig()\n    \n    return KVCacheOptimizedAttention(\n        kv_cache_config,\n        memory_manager,\n        hidden_size=hidden_size,\n        num_attention_heads=num_attention_heads,\n        max_position_embeddings=max_position_embeddings,\n        rope_theta=rope_theta\n    )