"""\nGradient Checkpointing Integration with Memory Pooling System\nPhase 2.9: Memory Pooling and Pre-allocation Techniques - Gradient Checkpointing Integration\nOptimized for Intel i5-10210U + NVIDIA SM61 + NVMe SSD Hardware Configuration\n"""\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom typing import Any, Callable, List, Tuple, Dict, Optional, Union\nimport threading\nimport time\nimport gc\nimport weakref\nfrom collections import defaultdict, deque\nimport logging\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass GradientCheckpointingConfig:\n    """\n    Configuration for gradient checkpointing with memory pooling integration.\n    """\n    # Checkpointing settings\n    use_gradient_checkpointing: bool = True\n    checkpoint_every_n_layers: int = 2  # Checkpoint every N transformer layers\n    checkpoint_attention_layers: bool = True\n    checkpoint_mlp_layers: bool = True\n    \n    # Memory pooling integration settings\n    use_memory_pooling_for_checkpoints: bool = True\n    memory_pool_size_for_checkpoints: int = 2**29  # 512MB for checkpointed tensors\n    enable_tensor_caching_for_checkpoints: bool = True\n    \n    # Hardware-specific optimizations\n    hardware_compute_capability: Tuple[int, int] = (6, 1)  # SM61\n    memory_bandwidth_gb_s: float = 192.0  # Estimated for GTX 1080 Ti\n    use_half_precision_for_checkpoints: bool = True  # Use FP16 to save memory\n    \n    # Performance settings\n    enable_async_checkpointing: bool = True  # Use async operations where possible\n    checkpoint_compaction_interval: int = 100  # Compact checkpoint cache every N checkpoints\n\n\nclass MemoryEfficientCheckpointFunction(torch.autograd.Function):\n    """\n    Custom autograd function that integrates with memory pooling for gradient checkpointing.\n    """\n    \n    @staticmethod\n    def forward(ctx, run_function: Callable, memory_pool, *args):\n        """\n        Forward pass with memory-efficient checkpointing.\n        """\n        ctx.run_function = run_function\n        ctx.memory_pool = memory_pool\n        \n        # Store tensor metadata for backward pass\n        ctx.tensor_metadata = []\n        ctx.tensor_shapes = []\n        ctx.tensor_dtypes = []\n        ctx.tensor_devices = []\n        \n        preserved_args = []\n        for arg in args:\n            if torch.is_tensor(arg) and arg.requires_grad:\n                # Store tensor metadata\n                ctx.tensor_metadata.append({\n                    'requires_grad': arg.requires_grad,\n                    'is_leaf': arg.is_leaf\n                })\n                ctx.tensor_shapes.append(arg.shape)\n                ctx.tensor_dtypes.append(arg.dtype)\n                ctx.tensor_devices.append(arg.device)\n                \n                # Preserve the tensor using memory pool if available\n                if memory_pool and ctx.memory_pool.use_memory_pooling:\n                    # Use memory pool to preserve tensor for backward pass\n                    preserved_tensor = memory_pool.allocate_tensor(arg.shape, arg.dtype, arg.device)\n                    preserved_tensor.copy_(arg)\n                    preserved_args.append(preserved_tensor)\n                else:\n                    # Fallback: preserve tensor normally\n                    preserved_args.append(arg.detach().requires_grad_(arg.requires_grad))\n            else:\n                preserved_args.append(arg)\n        \n        ctx.preserved_args = preserved_args\n        ctx.args_mask = [torch.is_tensor(arg) for arg in args]\n        \n        # Run the function\n        with torch.no_grad():\n            outputs = run_function(*args)\n        \n        return outputs\n\n    @staticmethod\n    def backward(ctx, *grad_outputs):\n        """\n        Backward pass with memory-efficient recomputation.\n        """\n        # Retrieve preserved tensors from memory pool or normal preservation\n        detached_inputs = []\n        for preserved_arg, mask in zip(ctx.preserved_args, ctx.args_mask):\n            if mask and torch.is_tensor(preserved_arg):\n                # This was a tensor that we preserved\n                detached_inputs.append(preserved_arg.requires_grad_(True))\n            else:\n                # This was not a tensor, just pass through\n                detached_inputs.append(preserved_arg)\n        \n        # Recompute forward pass\n        with torch.enable_grad():\n            outputs = ctx.run_function(*detached_inputs)\n        \n        # If output is a tensor, convert to tuple for consistent handling\n        if torch.is_tensor(outputs):\n            outputs = (outputs,)\n        \n        # Compute gradients\n        gradients = torch.autograd.grad(\n            outputs, \n            detached_inputs, \n            grad_outputs,\n            allow_unused=True,\n            retain_graph=True\n        )\n        \n        # Return gradients along with None for non-tensor inputs and memory pool\n        result = [None, None]  # run_function and memory_pool don't need gradients\n        for grad in gradients:\n            result.append(grad)\n        \n        # Clean up preserved tensors if using memory pool\n        if ctx.memory_pool and ctx.memory_pool.use_memory_pooling:\n            for preserved_arg in ctx.preserved_args:\n                if torch.is_tensor(preserved_arg):\n                    # Return tensor to memory pool\n                    ctx.memory_pool.deallocate_tensor(preserved_arg)\n        \n        return tuple(result)\n\n\nclass GradientCheckpointingMemoryManager:\n    """\n    Memory manager specifically for gradient checkpointing operations.\n    Integrates with memory pooling system to optimize checkpoint storage and retrieval.\n    """\n    \n    def __init__(self, config: GradientCheckpointingConfig = None):\n        self.config = config or GradientCheckpointingConfig()\n        self._lock = threading.Lock()\n        \n        # Initialize memory pool for checkpointed tensors if enabled\n        if self.config.use_memory_pooling_for_checkpoints:\n            self.checkpoint_memory_pool = self._initialize_checkpoint_memory_pool()\n        else:\n            self.checkpoint_memory_pool = None\n        \n        # Cache for frequently checkpointed tensors\n        self.checkpoint_tensor_cache = {}\n        self.checkpoint_cache_size_limit = 50  # Max number of cached checkpointed tensors\n        \n        # Statistics\n        self.stats = {\n            'checkpoints_created': 0,\n            'checkpoints_restored': 0,\n            'tensors_cached': 0,\n            'cache_hits': 0,\n            'memory_saved_by_pooling': 0,\n            'checkpoint_compactions': 0\n        }\n        \n        # Track checkpoint patterns for optimization\n        self.checkpoint_patterns = defaultdict(int)\n        self.checkpoint_history = deque(maxlen=1000)\n    \n    def _initialize_checkpoint_memory_pool(self):\n        """\n        Initialize memory pool specifically for checkpointed tensors.\n        """\nfrom memory_management.memory_pool import MemoryPool\n        return MemoryPool(\n            initial_size=self.config.memory_pool_size_for_checkpoints,\n            use_memory_pooling=True,\n            memory_efficient_allocation=True\n        )\n    \n    def prepare_checkpoint_tensors(self, tensors: Tuple[torch.Tensor, ...]) -> Tuple[torch.Tensor, ...]:\n        """\n        Prepare tensors for checkpointing with memory optimization.\n        """\n        with self._lock:\n            prepared_tensors = []\n            \n            for tensor in tensors:\n                if torch.is_tensor(tensor) and tensor.requires_grad:\n                    # Optimize tensor for checkpointing based on hardware\n                    optimized_tensor = self._optimize_tensor_for_checkpointing(tensor)\n                    \n                    # Store in memory pool if available\n                    if self.checkpoint_memory_pool and self.config.use_memory_pooling_for_checkpoints:\n                        pooled_tensor = self.checkpoint_memory_pool.allocate_tensor(\n                            optimized_tensor.shape, \n                            optimized_tensor.dtype, \n                            optimized_tensor.device\n                        )\n                        pooled_tensor.copy_(optimized_tensor)\n                        prepared_tensors.append(pooled_tensor)\n                    else:\n                        prepared_tensors.append(optimized_tensor)\n                else:\n                    prepared_tensors.append(tensor)\n            \n            self.stats['checkpoints_created'] += 1\n            \n            # Update checkpoint patterns\n            pattern_key = tuple(t.shape if torch.is_tensor(t) else 'non_tensor' for t in prepared_tensors)\n            self.checkpoint_patterns[pattern_key] += 1\n            \n            # Record in history\n            self.checkpoint_history.append({\n                'timestamp': time.time(),\n                'pattern': pattern_key,\n                'tensor_count': len([t for t in prepared_tensors if torch.is_tensor(t)])\n            })\n            \n            return tuple(prepared_tensors)\n    \n    def _optimize_tensor_for_checkpointing(self, tensor: torch.Tensor) -> torch.Tensor:\n        """\n        Optimize tensor for checkpointing based on hardware characteristics.\n        """\n        # For SM61, consider using half precision for checkpointed tensors to save memory\n        if (self.config.use_half_precision_for_checkpoints and \n            tensor.dtype == torch.float32 and\n            tensor.numel() > 1024):  # Only for reasonably large tensors\n            \n            # Check if tensor values are within FP16 range\n            if tensor.abs().max() < 65504:  # Max value for FP16\n                return tensor.to(torch.float16)\n        \n        return tensor\n    \n    def restore_checkpoint_tensors(self, prepared_tensors: Tuple[Any, ...]) -> Tuple[Any, ...]:\n        """\n        Restore checkpointed tensors with proper gradient tracking.\n        """\n        with self._lock:\n            restored_tensors = []\n            \n            for tensor in prepared_tensors:\n                if torch.is_tensor(tensor):\n                    # Ensure tensor has proper gradient tracking\n                    restored_tensor = tensor.detach().requires_grad_(True)\n                    \n                    # Convert back to full precision if it was converted for checkpointing\n                    if (self.config.use_half_precision_for_checkpoints and \n                        tensor.dtype == torch.float16):\n                        restored_tensor = restored_tensor.float()\n                    \n                    restored_tensors.append(restored_tensor)\n                    self.stats['checkpoints_restored'] += 1\n                else:\n                    restored_tensors.append(tensor)\n            \n            return tuple(restored_tensors)\n    \n    def cache_checkpoint_result(self, key: str, result: Any):\n        """\n        Cache a checkpoint result for potential reuse.\n        """\n        with self._lock:\n            if len(self.checkpoint_tensor_cache) >= self.checkpoint_cache_size_limit:\n                # Remove oldest entry to make space\n                oldest_key = next(iter(self.checkpoint_tensor_cache))\n                del self.checkpoint_tensor_cache[oldest_key]\n            \n            self.checkpoint_tensor_cache[key] = result\n            self.stats['tensors_cached'] += 1\n    \n    def get_cached_checkpoint_result(self, key: str) -> Optional[Any]:\n        """\n        Get a cached checkpoint result.\n        """\n        with self._lock:\n            if key in self.checkpoint_tensor_cache:\n                self.stats['cache_hits'] += 1\n                return self.checkpoint_tensor_cache[key]\n            return None\n    \n    def compact_checkpoint_cache(self):\n        """\n        Compact the checkpoint cache to remove unused entries.\n        """\n        with self._lock:\n            # For now, just clear the cache periodically\n            # In a more sophisticated implementation, we would have a more nuanced approach\n            self.checkpoint_tensor_cache.clear()\n            self.stats['checkpoint_compactions'] += 1\n    \n    def get_checkpoint_efficiency_stats(self) -> Dict[str, Any]:\n        """\n        Get statistics about checkpoint memory efficiency.\n        """\n        with self._lock:\n            stats = self.stats.copy()\n            \n            # Calculate efficiency metrics\n            if stats['checkpoints_created'] > 0:\n                stats['cache_hit_rate'] = stats['cache_hits'] / stats['checkpoints_created']\n            else:\n                stats['cache_hit_rate'] = 0.0\n            \n            # If using memory pool, get its statistics\n            if self.checkpoint_memory_pool:\n                pool_stats = self.checkpoint_memory_pool.get_memory_stats()\n                stats['memory_pool_stats'] = pool_stats\n            \n            # Get checkpoint pattern analysis\n            most_common_pattern = max(self.checkpoint_patterns.items(), key=lambda x: x[1]) if self.checkpoint_patterns else (None, 0)\n            stats['most_common_checkpoint_pattern'] = most_common_pattern\n            \n            return stats\n\n\nclass MemoryOptimizedCheckpointWrapper(nn.Module):\n    """\n    Wrapper for modules that applies memory-optimized gradient checkpointing.\n    """\n    \n    def __init__(self, module: nn.Module, memory_manager: GradientCheckpointingMemoryManager, \n                 checkpoint_condition: Optional[Callable] = None):\n        super().__init__()\n        self.module = module\n        self.memory_manager = memory_manager\n        self.checkpoint_condition = checkpoint_condition or self._default_checkpoint_condition\n        \n        # Track whether this module should use checkpointing\n        self.use_checkpointing = True\n    \n    def _default_checkpoint_condition(self, *args, **kwargs) -> bool:\n        """\n        Default condition for when to apply checkpointing.\n        """\n        # Apply checkpointing to large tensors or when memory pressure is high\n        total_input_size = sum(\n            arg.numel() * arg.element_size() if torch.is_tensor(arg) else 0 \n            for arg in args if torch.is_tensor(arg)\n        )\n        \n        # Checkpoint if input is larger than 1MB or if explicitly enabled\n        return total_input_size > (1024 * 1024)  # 1MB threshold\n    \n    def forward(self, *args, **kwargs):\n        """\n        Forward pass with optional memory-optimized checkpointing.\n        """\n        if self.use_checkpointing and self.checkpoint_condition(*args, **kwargs):\n            # Apply memory-optimized checkpointing\n            def run_function(*inputs):\n                return self.module(*inputs, **kwargs)\n            \n            # Prepare inputs for checkpointing\n            checkpointed_inputs = self.memory_manager.prepare_checkpoint_tensors(args)\n            \n            # Use the memory-efficient checkpoint function\n            if torch.is_grad_enabled():\n                outputs = MemoryEfficientCheckpointFunction.apply(\n                    run_function,\n                    self.memory_manager.checkpoint_memory_pool,\n                    *checkpointed_inputs\n                )\n            else:\n                # No need for checkpointing in eval mode\n                outputs = self.module(*args, **kwargs)\n        else:\n            # Normal forward pass without checkpointing\n            outputs = self.module(*args, **kwargs)\n        \n        return outputs\n\n\nclass Qwen3VLCheckpointIntegration:\n    """\n    Main integration class for gradient checkpointing with memory pooling\n    in the Qwen3-VL model architecture.\n    """\n    \n    def __init__(self, config: GradientCheckpointingConfig = None):\n        self.config = config or GradientCheckpointingConfig()\n        self.memory_manager = GradientCheckpointingMemoryManager(self.config)\n        self._lock = threading.Lock()\n        \n        # Track which layers/modules are checkpointed\n        self.checkpointed_modules = set()\n        \n        # Hardware-specific optimization parameters\n        self.warp_size = 32  # Standard for NVIDIA GPUs\n        self.shared_memory_per_block = self._get_shared_memory_per_block()\n    \n    def _get_shared_memory_per_block(self) -> int:\n        """\n        Get shared memory per block based on compute capability.\n        """\n        if self.config.hardware_compute_capability >= (6, 0) and self.config.hardware_compute_capability < (7, 0):\n            return 48 * 1024  # 48KB for SM61\n        elif self.config.hardware_compute_capability >= (7, 0) and self.config.hardware_compute_capability < (8, 0):\n            return 96 * 1024  # 96KB for SM7.x (configurable)\n        else:\n            return 48 * 1024  # Default fallback\n    \n    def apply_memory_optimized_checkpointing_to_model(self, model: nn.Module) -> nn.Module:\n        """\n        Apply memory-optimized checkpointing to the entire model.\n        """\n        with self._lock:\n            # Identify transformer layers that can be checkpointed\n            for name, module in model.named_modules():\n                if self._is_transformer_layer(module):\n                    # Wrap the layer with memory-optimized checkpointing\n                    wrapped_module = MemoryOptimizedCheckpointWrapper(\n                        module, \n                        self.memory_manager,\n                        self._get_checkpoint_condition_for_layer(name, module)\n                    )\n                    \n                    # Replace the module in the parent\n                    self._replace_module_in_parent(model, name, wrapped_module)\n                    self.checkpointed_modules.add(name)\n            \n            return model\n    \n    def _is_transformer_layer(self, module: nn.Module) -> bool:\n        """\n        Determine if a module is a transformer layer that can be checkpointed.\n        """\n        # Check if it's a transformer layer (attention + MLP)\n        module_str = str(type(module))\n        return any(keyword in module_str.lower() for keyword in \n                  ['transformer', 'attention', 'mlp', 'feedforward', 'encoder', 'decoder'])\n    \n    def _get_checkpoint_condition_for_layer(self, name: str, module: nn.Module) -> Callable:\n        """\n        Get checkpoint condition function for a specific layer.\n        """\n        def condition(*args, **kwargs):\n            # Default condition based on layer name and input size\n            if 'attention' in name.lower() or 'mlp' in name.lower():\n                # Checkpoint attention and MLP layers if they're large enough\n                total_input_size = sum(\n                    arg.numel() * arg.element_size() if torch.is_tensor(arg) else 0 \n                    for arg in args if torch.is_tensor(arg)\n                )\n                return total_input_size > (512 * 1024)  # 512KB threshold for transformer layers\n            return True  # Default: checkpoint if condition allows\n        \n        return condition\n    \n    def _replace_module_in_parent(self, model: nn.Module, module_name: str, new_module: nn.Module):\n        """\n        Replace a module in its parent with a new module.\n        """\n        # Split the module name to navigate to the parent\n        parts = module_name.split('.')\n        parent = model\n        \n        # Navigate to the parent module\n        for part in parts[:-1]:\n            parent = getattr(parent, part)\n        \n        # Replace the last part\n        setattr(parent, parts[-1], new_module)\n    \n    def checkpoint_function(self, function: Callable, *args, use_reentrant: bool = True, **kwargs):\n        """\n        Memory-optimized version of torch.utils.checkpoint.checkpoint.\n        """\n        if not self.config.use_gradient_checkpointing:\n            return function(*args, **kwargs)\n        \n        # Prepare tensors for checkpointing\n        checkpointed_args = self.memory_manager.prepare_checkpoint_tensors(args)\n        \n        if use_reentrant:\n            # Use PyTorch's checkpoint function with our memory management\n            def wrapper_function(*inputs):\n                # Restore tensors for function execution\n                restored_inputs = self.memory_manager.restore_checkpoint_tensors(inputs)\n                return function(*restored_inputs, **kwargs)\n            \n            return checkpoint.checkpoint(wrapper_function, *checkpointed_args, use_reentrant=True)\n        else:\n            # Use non-reentrant checkpointing (more memory efficient)\n            def wrapper_function(*inputs):\n                restored_inputs = self.memory_manager.restore_checkpoint_tensors(inputs)\n                return function(*restored_inputs, **kwargs)\n            \n            return checkpoint.checkpoint(wrapper_function, *checkpointed_args, use_reentrant=False)\n    \n    def checkpoint_sequential(self, functions: List[Callable], input_val: Any):\n        """\n        Memory-optimized sequential checkpointing for a list of functions.\n        """\n        # For sequential operations, we can optimize checkpointing differently\n        # by potentially checkpointing every Nth function rather than all\n        checkpoint_every = self.config.checkpoint_every_n_layers\n        \n        result = input_val\n        for i, func in enumerate(functions):\n            if i % checkpoint_every == 0 and i > 0:\n                # Apply checkpointing to this function\n                result = self.checkpoint_function(func, result)\n            else:\n                # Normal execution\n                result = func(result)\n        \n        return result\n    \n    def optimize_vision_encoder_checkpointing(self, vision_encoder: nn.Module) -> nn.Module:\n        """\n        Apply specialized checkpointing optimization for vision encoder.\n        """\n        with self._lock:\n            # For vision encoders, we often have:\n            # 1. Patch embedding layers\n            # 2. Transformer blocks\n            # 3. Final projection layers\n            \n            for name, module in vision_encoder.named_modules():\n                if self._is_vision_transformer_block(module):\n                    # Apply checkpointing to transformer blocks\n                    wrapped_module = MemoryOptimizedCheckpointWrapper(\n                        module,\n                        self.memory_manager,\n                        self._get_vision_checkpoint_condition(name, module)\n                    )\n                    self._replace_module_in_parent(vision_encoder, name, wrapped_module)\n                    self.checkpointed_modules.add(name)\n            \n            return vision_encoder\n    \n    def _is_vision_transformer_block(self, module: nn.Module) -> bool:\n        """\n        Check if a module is a vision transformer block.\n        """\n        module_str = str(type(module)).lower()\n        return 'transformer' in module_str or 'attention' in module_str or 'vit' in module_str\n    \n    def _get_vision_checkpoint_condition(self, name: str, module: nn.Module) -> Callable:\n        """\n        Get checkpoint condition for vision-specific modules.\n        """\n        def condition(*args, **kwargs):\n            # For vision transformers, checkpoint based on patch count and embedding dimension\n            if args and torch.is_tensor(args[0]):\n                tensor = args[0]\n                if len(tensor.shape) >= 3:\n                    # Check if it's a patch or sequence tensor (batch, seq_len, features)\n                    seq_len = tensor.shape[1] if len(tensor.shape) > 1 else 1\n                    feature_dim = tensor.shape[-1] if len(tensor.shape) > 0 else 1\n                    \n                    # Checkpoint if sequence is long enough or features are high-dimensional\n                    return seq_len > 100 or feature_dim > 512\n            \n            return True\n        \n        return condition\n    \n    def get_integration_statistics(self) -> Dict[str, Any]:\n        """\n        Get statistics about the gradient checkpointing integration.\n        """\n        with self._lock:\n            checkpoint_stats = self.memory_manager.get_checkpoint_efficiency_stats()\n            \n            return {\n                'checkpointed_modules_count': len(self.checkpointed_modules),\n                'checkpointed_modules': list(self.checkpointed_modules),\n                'checkpoint_memory_manager_stats': checkpoint_stats,\n                'integration_config': {\n                    'use_gradient_checkpointing': self.config.use_gradient_checkpointing,\n                    'checkpoint_every_n_layers': self.config.checkpoint_every_n_layers,\n                    'use_memory_pooling_for_checkpoints': self.config.use_memory_pooling_for_checkpoints,\n                    'use_half_precision_for_checkpoints': self.config.use_half_precision_for_checkpoints,\n                    'hardware_compute_capability': self.config.hardware_compute_capability\n                },\n                'hardware_optimization_params': {\n                    'warp_size': self.warp_size,\n                    'shared_memory_per_block_kb': self.shared_memory_per_block // 1024\n                }\n            }\n    \n    def compact_checkpoint_storage(self):\n        """\n        Compact checkpoint storage to reduce memory fragmentation.\n        """\n        with self._lock:\n            self.memory_manager.compact_checkpoint_cache()\n            \n            # If using memory pool, defragment it\n            if self.memory_manager.checkpoint_memory_pool:\n                self.memory_manager.checkpoint_memory_pool.defragment_memory()\n\n\nclass VisionLanguageCheckpointIntegrator:\n    """\n    Integrates gradient checkpointing with vision-language memory optimizations.\n    """\n    \n    def __init__(self, checkpoint_config: GradientCheckpointingConfig = None):\n        self.checkpoint_config = checkpoint_config or GradientCheckpointingConfig()\n        self.checkpoint_integration = Qwen3VLCheckpointIntegration(self.checkpoint_config)\n        self._lock = threading.Lock()\n        \n        # Memory optimization for vision-language interactions\n        self.vision_language_memory_efficiency = {\n            'cross_attention_checkpointed': 0,\n            'vision_encoder_checkpointed': 0,\n            'language_decoder_checkpointed': 0,\n            'total_memory_saved_mb': 0\n        }\n    \n    def optimize_vision_language_model(self, model: nn.Module) -> nn.Module:\n        """\n        Optimize checkpointing for vision-language model with memory efficiency.\n        """\n        with self._lock:\n            # Apply general checkpointing optimization\n            optimized_model = self.checkpoint_integration.apply_memory_optimized_checkpointing_to_model(model)\n            \n            # Identify and optimize vision-specific components\n            for name, module in optimized_model.named_modules():\n                if self._is_vision_component(module):\n                    # Apply specialized vision checkpointing\n                    self.checkpoint_integration.optimize_vision_encoder_checkpointing(module)\n                    self.vision_language_memory_efficiency['vision_encoder_checkpointed'] += 1\n                \n                elif self._is_language_component(module):\n                    # Apply language-specific checkpointing optimization\n                    wrapped_module = MemoryOptimizedCheckpointWrapper(\n                        module,\n                        self.checkpoint_integration.memory_manager,\n                        self._get_language_checkpoint_condition(name, module)\n                    )\n                    self.checkpoint_integration._replace_module_in_parent(optimized_model, name, wrapped_module)\n                    self.vision_language_memory_efficiency['language_decoder_checkpointed'] += 1\n                \n                elif self._is_cross_modal_component(module):\n                    # Apply cross-modal checkpointing optimization\n                    wrapped_module = MemoryOptimizedCheckpointWrapper(\n                        module,\n                        self.checkpoint_integration.memory_manager,\n                        self._get_cross_modal_checkpoint_condition(name, module)\n                    )\n                    self.checkpoint_integration._replace_module_in_parent(optimized_model, name, wrapped_module)\n                    self.vision_language_memory_efficiency['cross_attention_checkpointed'] += 1\n            \n            return optimized_model\n    \n    def _is_vision_component(self, module: nn.Module) -> bool:\n        """\n        Check if module is a vision component.\n        """\n        module_str = str(type(module)).lower()\n        return any(keyword in module_str for keyword in \n                  ['vision', 'image', 'patch', 'pixel', 'conv', 'vit', 'vision_encoder'])\n    \n    def _is_language_component(self, module: nn.Module) -> bool:\n        """\n        Check if module is a language component.\n        """\n        module_str = str(type(module)).lower()\n        return any(keyword in module_str for keyword in \n                  ['language', 'text', 'token', 'embedding', 'llm', 'lm_head', 'language_decoder'])\n    \n    def _is_cross_modal_component(self, module: nn.Module) -> bool:\n        """\n        Check if module is a cross-modal component.\n        """\n        module_str = str(type(module)).lower()\n        return any(keyword in module_str for keyword in \n                  ['cross', 'fusion', 'align', 'projector', 'multimodal'])\n    \n    def _get_language_checkpoint_condition(self, name: str, module: nn.Module) -> Callable:\n        """\n        Get checkpoint condition for language components.\n        """\n        def condition(*args, **kwargs):\n            # For language components, checkpoint based on sequence length\n            if args and torch.is_tensor(args[0]):\n                tensor = args[0]\n                if len(tensor.shape) >= 2:\n                    seq_len = tensor.shape[1] if len(tensor.shape) > 1 else 1\n                    # Checkpoint for longer sequences to save memory\n                    return seq_len > 512\n            return True\n        return condition\n    \n    def _get_cross_modal_checkpoint_condition(self, name: str, module: nn.Module) -> Callable:\n        """\n        Get checkpoint condition for cross-modal components.\n        """\n        def condition(*args, **kwargs):\n            # For cross-modal components, checkpoint based on combined vision-language dimensions\n            total_elements = 0\n            for arg in args:\n                if torch.is_tensor(arg):\n                    total_elements += arg.numel()\n            \n            # Checkpoint if the cross-modal interaction involves large tensors\n            return total_elements > (256 * 1024)  # 256K elements threshold\n        return condition\n    \n    def get_vision_language_checkpoint_stats(self) -> Dict[str, Any]:\n        """\n        Get statistics about vision-language checkpointing optimization.\n        """\n        with self._lock:\n            integration_stats = self.checkpoint_integration.get_integration_statistics()\n            \n            return {\n                'vision_language_specific_stats': self.vision_language_memory_efficiency,\n                'general_integration_stats': integration_stats,\n                'memory_efficiency_gains': self._calculate_memory_efficiency_gains()\n            }\n    \n    def _calculate_memory_efficiency_gains(self) -> Dict[str, float]:\n        """\n        Calculate estimated memory efficiency gains from checkpointing.\n        """\n        # Estimate memory savings based on checkpointed components\n        # This is a simplified calculation - in practice, actual savings would be measured\n        vision_savings = self.vision_language_memory_efficiency['vision_encoder_checkpointed'] * 50  # 50MB per encoder block\n        language_savings = self.vision_language_memory_efficiency['language_decoder_checkpointed'] * 30  # 30MB per decoder block\n        cross_modal_savings = self.vision_language_memory_efficiency['cross_attention_checkpointed'] * 20  # 20MB per cross-attention block\n        \n        total_estimated_savings_mb = vision_savings + language_savings + cross_modal_savings\n        \n        return {\n            'estimated_vision_savings_mb': vision_savings,\n            'estimated_language_savings_mb': language_savings,\n            'estimated_cross_modal_savings_mb': cross_modal_savings,\n            'total_estimated_savings_mb': total_estimated_savings_mb\n        }\n\n\n# Global checkpoint integration instance\n_global_checkpoint_integrator = None\n_checkpoint_lock = threading.Lock()\n\n\ndef get_checkpoint_integrator(config: GradientCheckpointingConfig = None) -> Qwen3VLCheckpointIntegration:\n    """\n    Get the global checkpoint integration instance.\n    """\n    global _global_checkpoint_integrator\n    if _global_checkpoint_integrator is None:\n        with _checkpoint_lock:\n            if _global_checkpoint_integrator is None:\n                _global_checkpoint_integrator = Qwen3VLCheckpointIntegration(config)\n    return _global_checkpoint_integrator\n\n\ndef apply_memory_optimized_checkpointing(model: nn.Module, \n                                       config: GradientCheckpointingConfig = None) -> nn.Module:\n    """\n    Apply memory-optimized gradient checkpointing to a model.\n    """\n    integrator = get_checkpoint_integrator(config)\n    return integrator.apply_memory_optimized_checkpointing_to_model(model)\n\n\ndef checkpoint_with_memory_optimization(function: Callable, *args, **kwargs):\n    """\n    Run a function with memory-optimized gradient checkpointing.\n    """\n    integrator = get_checkpoint_integrator()\n    return integrator.checkpoint_function(function, *args, **kwargs)\n\n\ndef get_checkpoint_integration_stats() -> Dict[str, Any]:\n    """\n    Get statistics about the checkpoint integration.\n    """\n    integrator = get_checkpoint_integrator()\n    return integrator.get_integration_statistics()\n\n\ndef compact_checkpoint_storage():\n    """\n    Compact checkpoint storage to reduce memory fragmentation.\n    """\n    integrator = get_checkpoint_integrator()\n    integrator.compact_checkpoint_storage()\n\n\ndef optimize_vision_language_model_for_checkpointing(model: nn.Module, \n                                                   config: GradientCheckpointingConfig = None) -> nn.Module:\n    """\n    Optimize a vision-language model for memory-efficient checkpointing.\n    """\n    vision_lang_integrator = VisionLanguageCheckpointIntegrator(config)\n    return vision_lang_integrator.optimize_vision_language_model(model)\n\n\ndef get_vision_language_checkpoint_stats() -> Dict[str, Any]:\n    """\n    Get statistics about vision-language checkpointing optimization.\n    """\n    vision_lang_integrator = VisionLanguageCheckpointIntegrator()\n    return vision_lang_integrator.get_vision_language_checkpoint_stats()\n\n\nif __name__ == "__main__":\n    print("Testing Gradient Checkpointing Integration with Memory Pooling...")\n\n    # Test configuration\n    config = GradientCheckpointingConfig(\n        use_gradient_checkpointing=True,\n        use_memory_pooling_for_checkpoints=True,\n        use_half_precision_for_checkpoints=True,\n        checkpoint_every_n_layers=2,\n        hardware_compute_capability=(6, 1)  # SM61\n    )\n\n    print("\n1. Testing Memory-Efficient Checkpoint Function...")\n\n    # Test basic checkpoint function\n    def simple_function(x):\n        return x * x + 2 * x + 1\n\n    # Create test tensor\n    x = torch.randn(10, 20, requires_grad=True)\n\n    # Test memory-optimized checkpointing\n    integrator = Qwen3VLCheckpointIntegration(config)\n    \n    # Test the custom checkpoint function\n    result = integrator.checkpoint_function(simple_function, x)\n    print(f"Checkpoint function result shape: {result.shape}")\n    \n    # Test with gradient computation\n    loss = result.sum()\n    loss.backward()\n    print(f"Gradient computed successfully, x.grad shape: {x.grad.shape if x.grad is not None else None}")\n\n    print("\n2. Testing Memory Manager...")\n\n    # Test memory manager directly\n    memory_manager = GradientCheckpointingMemoryManager(config)\n\n    # Test tensor preparation for checkpointing\n    test_tensors = (torch.randn(5, 10, 20), torch.randn(5, 10))\n    prepared = memory_manager.prepare_checkpoint_tensors(test_tensors)\n    print(f"Prepared {len(prepared)} tensors for checkpointing")\n    \n    # Test tensor restoration\n    restored = memory_manager.restore_checkpoint_tensors(prepared)\n    print(f"Restored {len(restored)} tensors from checkpointing")\n    \n    # Test caching functionality\n    memory_manager.cache_checkpoint_result("test_key", torch.randn(3, 4))\n    cached_result = memory_manager.get_cached_checkpoint_result("test_key")\n    print(f"Cached result retrieved: {cached_result is not None}")\n    \n    # Get memory manager statistics\n    manager_stats = memory_manager.get_checkpoint_efficiency_stats()\n    print(f"Memory manager stats - Checkpoints created: {manager_stats['checkpoints_created']}")\n    print(f"Cache hit rate: {manager_stats['cache_hit_rate']:.3f}")\n\n    print("\n3. Testing Memory-Optimized Checkpoint Wrapper...")\n\n    # Test the checkpoint wrapper\n    class SimpleModule(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.linear = nn.Linear(20, 10)\n        \n        def forward(self, x):\n            return torch.relu(self.linear(x))\n\n    module = SimpleModule()\n    wrapper = MemoryOptimizedCheckpointWrapper(module, memory_manager)\n\n    # Test forward pass with wrapper\n    input_tensor = torch.randn(5, 20, requires_grad=True)\n    output = wrapper(input_tensor)\n    print(f"Wrapped module output shape: {output.shape}")\n    \n    # Test backward pass\n    loss = output.sum()\n    loss.backward()\n    print(f"Backward pass completed, input grad shape: {input_tensor.grad.shape}")\n\n    print("\n4. Testing Model-Wide Integration...")\n\n    # Create a simple transformer-like model for testing\n    class SimpleTransformerLayer(nn.Module):\n        def __init__(self, dim: int):\n            super().__init__()\n            self.attention = nn.MultiheadAttention(dim, num_heads=4)\n            self.mlp = nn.Sequential(\n                nn.Linear(dim, dim * 4),\n                nn.GELU(),\n                nn.Linear(dim * 4, dim)\n            )\n            self.norm1 = nn.LayerNorm(dim)\n            self.norm2 = nn.LayerNorm(dim)\n        \n        def forward(self, x):\n            attn_out, _ = self.attention(x, x, x)\n            x = self.norm1(x + attn_out)\n            mlp_out = self.mlp(x)\n            return self.norm2(x + mlp_out)\n\n    # Create a small transformer model\n    transformer_model = nn.Sequential(*[\n        SimpleTransformerLayer(512) for _ in range(4)\n    ])\n\n    # Apply memory-optimized checkpointing to the model\n    optimized_model = apply_memory_optimized_checkpointing(transformer_model, config)\n    print(f"Applied checkpointing to model with {len(optimized_model)} layers")\n\n    # Test forward pass\n    test_input = torch.randn(2, 100, 512, requires_grad=True)  # Batch=2, Seq=100, Dim=512\n    output = optimized_model(test_input)\n    print(f"Model output shape: {output.shape}")\n    \n    # Test backward pass\n    loss = output.sum()\n    loss.backward()\n    print(f"Backward pass completed successfully")\n\n    print("\n5. Testing Vision-Language Integration...")\n\n    # Test vision-language checkpoint optimization\n    vision_lang_model = nn.Sequential(\n        SimpleTransformerLayer(768),  # Vision encoder layer\n        SimpleTransformerLayer(768),  # Vision encoder layer\n        SimpleTransformerLayer(512),  # Projection layer\n        SimpleTransformerLayer(4096), # Language model layer\n        SimpleTransformerLayer(4096)  # Language model layer\n    )\n\n    # Optimize for vision-language checkpointing\n    optimized_vl_model = optimize_vision_language_model_for_checkpointing(vision_lang_model, config)\n    print(f"Optimized vision-language model with {len(optimized_vl_model)} layers")\n\n    # Test with vision-like input\n    vision_input = torch.randn(1, 197, 768, requires_grad=True)  # [batch, seq, features] - similar to ViT\n    vl_output = optimized_vl_model(vision_input)\n    print(f"Vision-language model output shape: {vl_output.shape}")\n\n    # Test backward pass\n    vl_loss = vl_output.sum()\n    vl_loss.backward()\n    print(f"Vision-language backward pass completed")\n\n    print("\n6. Testing Statistics and Monitoring...")\n\n    # Get integration statistics\n    integration_stats = get_checkpoint_integration_stats()\n    print(f"Checkpointed modules: {integration_stats['checkpointed_modules_count']}")\n    print(f"Memory pooling enabled: {integration_stats['integration_config']['use_memory_pooling_for_checkpoints']}")\n    print(f"Half precision for checkpoints: {integration_stats['integration_config']['use_half_precision_for_checkpoints']}")\n\n    # Get vision-language specific stats\n    vl_stats = get_vision_language_checkpoint_stats()\n    print(f"Vision encoder blocks checkpointed: {vl_stats['vision_language_specific_stats']['vision_encoder_checkpointed']}")\n    print(f"Language decoder blocks checkpointed: {vl_stats['vision_language_specific_stats']['language_decoder_checkpointed']}")\n    print(f"Estimated total memory savings: {vl_stats['memory_efficiency_gains']['total_estimated_savings_mb']:.2f} MB")\n\n    print("\n7. Testing Storage Compaction...")\n\n    # Test storage compaction\n    compact_checkpoint_storage()\n    print("Checkpoint storage compacted successfully")\n\n    # Get final statistics after compaction\n    final_stats = get_checkpoint_integration_stats()\n    print(f"Checkpoint compactions performed: {final_stats['checkpoint_memory_manager_stats']['checkpoint_compactions']}")\n\n    print("\n8. Testing Sequential Checkpointing...")\n\n    # Test sequential checkpointing\n    functions = [lambda x: torch.relu(x @ torch.randn(10, 10)) for _ in range(6)]\n    input_val = torch.randn(5, 10, requires_grad=True)\n    \n    sequential_result = integrator.checkpoint_sequential(functions, input_val)\n    print(f"Sequential checkpointing result shape: {sequential_result.shape}")\n    \n    # Test backward pass\n    seq_loss = sequential_result.sum()\n    seq_loss.backward()\n    print(f"Sequential backward pass completed")\n\n    print("\nGradient Checkpointing Integration with Memory Pooling implementation completed!")\n    print("All tests passed for hardware-optimized gradient checkpointing system.")