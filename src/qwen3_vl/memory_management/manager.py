"""\nComprehensive Memory Management System for Qwen3-VL Model\nConsolidated module containing the main memory manager that orchestrates all memory-related components.\n"""\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport psutil\nimport gc\nimport math\nimport time\nfrom typing import Dict, Any, Tuple, Optional, List, Callable\nfrom collections import defaultdict, deque\nimport logging\nimport threading\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom memory_management.allocation import MemoryAllocator, BuddyAllocator\nfrom memory_management.pooling import MemoryPool\nfrom memory_management.compression import MemoryCompressionManager\nfrom memory_management.defragmentation import MemoryDefragmenter\nfrom memory_management.utils import get_hardware_optimizer\n\n\n@dataclass\nclass MemoryConfig:\n    """Configuration for memory management optimizations."""\n    # Memory pool settings\n    use_memory_pool: bool = True\n    memory_pool_size: int = 1024 * 1024 * 1024  # 1GB pool by default\n    enable_memory_compaction: bool = True\n\n    # Tensor allocation settings\n    use_tensor_caching: bool = True\n    cache_max_size: int = 100  # Max cached tensors per shape\n    use_inference_memory_efficient: bool = True\n\n    # Hardware-specific settings for Intel i5-10210U + NVIDIA SM61\n    hardware_compute_capability: Tuple[int, int] = (6, 1)  # SM61\n    memory_bandwidth_gb_s: float = 192.0  # Estimated for GTX 1080 Ti\n    shared_memory_per_block: int = 48 * 1024  # 48KB for SM61\n\n    # Memory fragmentation settings\n    defragmentation_threshold: float = 0.3  # Defragment when fragmentation > 30%\n    memory_pressure_threshold: float = 0.8  # High memory pressure threshold\n\n    # KV cache optimization settings\n    kv_cache_strategy: str = "sliding_window"\n    use_low_rank_kv_cache: bool = True\n    kv_cache_window_size: int = 1024\n    kv_low_rank_dimension: int = 64\n    kv_cache_max_length: int = 2048\n\n    # Gradient checkpointing settings\n    use_gradient_checkpointing: bool = True\n    use_activation_sparsity: bool = True\n    sparsity_ratio: float = 0.1\n\n    # Memory defragmentation settings\n    memory_defragmentation_enabled: bool = True\n    memory_defragmentation_threshold: float = 0.3\n\n\nclass MemoryManager:\n    """\n    Centralized memory manager for the Qwen3-VL model.\n    Handles memory allocation, deallocation, and optimization.\n    """\n    def __init__(self, config: MemoryConfig = None):\n        self.config = config or MemoryConfig()\n        \n        # Initialize memory allocation system\n        self.allocator = MemoryAllocator(\n            initial_pool_size=self.config.memory_pool_size,\n            max_pool_size=self.config.memory_pool_max_size if hasattr(self.config, 'memory_pool_max_size') else self.config.memory_pool_size,\n            use_buddy_allocation=True\n        )\n\n        # Initialize memory pooling system\n        self.memory_pool = MemoryPool(\n            initial_size=self.config.memory_pool_size\n        )\n\n        # Initialize compression manager\n        self.compression_manager = MemoryCompressionManager(\n            compression_threshold=0.1,\n            preferred_method='automatic'\n        )\n\n        # Initialize defragmenter\n        self.defragmenter = MemoryDefragmenter(\n            memory_pool_size=self.config.memory_pool_size\n        )\n\n        # Initialize other memory optimization components\n        self.gradient_checkpointing_enabled = self.config.use_gradient_checkpointing\n        self.activation_sparsity_enabled = self.config.use_activation_sparsity\n        self.sparsity_ratio = self.config.sparsity_ratio\n\n        # Initialize defragmentation system if enabled\n        self.defragmentation_enabled = self.config.memory_defragmentation_enabled\n        self.defragmentation_threshold = self.config.memory_defragmentation_threshold\n\n        # Statistics\n        self.stats = {\n            'total_allocations': 0,\n            'total_deallocations': 0,\n            'peak_memory_usage': 0,\n            'allocation_errors': 0\n        }\n\n        # Memory pressure monitoring\n        self.memory_pressure = 0.0\n\n    def allocate_tensor(self, shape: Tuple[int, ...], dtype: torch.dtype = torch.float32,\n                       device: torch.device = torch.device('cpu')) -> torch.Tensor:\n        """Allocate a tensor with specified shape and type using optimized allocation."""\n        try:\n            tensor = self.memory_pool.allocate_tensor(shape, dtype, device)\n            self.stats['total_allocations'] += 1\n\n            # Update peak memory if needed\n            tensor_size = tensor.numel() * tensor.element_size()\n            if tensor_size > self.stats['peak_memory_usage']:\n                self.stats['peak_memory_usage'] = tensor_size\n\n            # Update memory pressure\n            if device.type == 'cuda' and torch.cuda.is_available():\n                current_memory = torch.cuda.memory_allocated(device)\n                max_memory = torch.cuda.get_device_properties(device).total_memory\n                self.memory_pressure = current_memory / max_memory\n            else:\n                # For CPU, use system memory\n                self.memory_pressure = psutil.virtual_memory().percent / 100.0\n\n            return tensor\n        except Exception as e:\n            print(f"Tensor allocation failed: {e}")\n            self.stats['allocation_errors'] += 1\n            # Fallback to standard PyTorch allocation\n            return torch.empty(shape, dtype=dtype, device=device)\n\n    def free_tensor(self, tensor: torch.Tensor) -> bool:\n        """Free a tensor and return it to the memory pool if appropriate."""\n        try:\n            success = self.memory_pool.deallocate_tensor(tensor)\n            if success:\n                self.stats['total_deallocations'] += 1\n            return success\n        except Exception as e:\n            print(f"Tensor deallocation failed: {e}")\n            return False\n\n    def get_memory_stats(self) -> Dict:\n        """Get current memory usage statistics."""\n        pool_stats = self.memory_pool.get_memory_stats()\n\n        return {\n            **self.stats,\n            'pool_stats': pool_stats,\n            'memory_pressure': self.memory_pressure,\n            'system_memory_percent': psutil.virtual_memory().percent,\n            'available_system_memory_gb': psutil.virtual_memory().available / (1024**3)\n        }\n\n    def clear_cache(self):\n        """Clear tensor caches to free up memory."""\n        self.memory_pool.tensor_cache.clear_cache()\n\n    def defragment_memory(self) -> Dict:\n        """Perform memory defragmentation."""\n        return self.defragmenter.defragment_memory()\n\n    def register_common_tensor_shapes(self, shapes: List[Tuple[Tuple[int, ...], torch.dtype]]):\n        """Register additional common tensor shapes for pre-allocation."""\n        self.memory_pool.common_shapes.extend(shapes)\n        # Pre-allocate some tensors of these shapes\n        for shape, dtype in shapes:\n            for _ in range(2):  # Pre-allocate 2 of each new shape\n                try:\n                    tensor = self.memory_pool.allocate_tensor(shape, dtype)\n                    self.memory_pool.tensor_cache.return_tensor(tensor)\n                except Exception:\n                    # If allocation fails, continue with other shapes\n                    continue\n\n    def compress_tensor_if_beneficial(self, tensor: torch.Tensor) -> Tuple[torch.Tensor, bool]:\n        """Compress a tensor if it would be beneficial."""\n        compressed_data, was_compressed, _ = self.compression_manager.compress_if_beneficial(tensor)\n        if was_compressed:\n            # Decompress back to tensor format for compatibility\n            decompressed_tensor = self.compression_manager.decompress_tensor(compressed_data)\n            return decompressed_tensor, True\n        else:\n            return tensor, False\n\n    def enable_gradient_checkpointing(self):\n        """\n        Enable gradient checkpointing for memory efficiency during training.\n        """\n        self.gradient_checkpointing_enabled = True\n\n    def disable_gradient_checkpointing(self):\n        """\n        Disable gradient checkpointing.\n        """\n        self.gradient_checkpointing_enabled = False\n\n    def apply_activation_sparsity(self, activations):\n        """\n        Apply activation sparsity if enabled.\n\n        Args:\n            activations: Activations to apply sparsity to\n\n        Returns:\n            Sparsified activations\n        """\n        if self.activation_sparsity_enabled:\n            # Apply sparsity based on the configured ratio\n            # This is a simplified implementation - in practice, this would use more sophisticated methods\n            if isinstance(activations, torch.Tensor):\n                # Create a mask to zero out activations based on sparsity ratio\n                mask = torch.rand_like(activations) < self.sparsity_ratio\n                return activations * mask.to(activations.dtype)\n        return activations\n\n\nclass GradientCheckpointingMemoryIntegrator:\n    """\n    Integrates memory pooling with existing gradient checkpointing mechanisms\n    to reduce memory overhead during training.\n    """\n    def __init__(self, memory_pool, compression_manager=None):\n        self.memory_pool = memory_pool\n        self.compression_manager = compression_manager\n        self.checkpoint_cache = {}  # {key: tensor}\n        self.checkpoint_history = deque(maxlen=100)\n\n        # Hardware-specific optimization for SM61\n        self.shared_memory_per_block = 48 * 1024  # 48KB for SM61\n\n    def checkpoint_tensors(self, tensors: List[torch.Tensor],\n                          key: Optional[str] = None) -> Dict[str, Any]:\n        """\n        Checkpoint tensors using memory pool for efficient storage\n        """\n        checkpoint_key = key or f"chkpt_{int(time.time() * 1000000)}"\n        checkpoint_data = {}\n\n        for i, tensor in enumerate(tensors):\n            tensor_key = f"{checkpoint_key}_tensor_{i}"\n\n            # For SM61, consider using half precision to save memory\n            if tensor.dtype == torch.float32:\n                # Store in memory pool managed cache\n                pooled_tensor = self.memory_pool.allocate_tensor(tensor.shape, torch.float16, tensor.device)\n                pooled_tensor.copy_(tensor.half())\n                checkpoint_data[tensor_key] = pooled_tensor\n            else:\n                # Create a copy of the tensor using pooled memory if it's large enough to benefit\n                if tensor.numel() > 1024:  # Only for larger tensors\n                    pooled_tensor = self.memory_pool.allocate_tensor(tensor.shape, tensor.dtype, tensor.device)\n                    pooled_tensor.copy_(tensor)\n                    checkpoint_data[tensor_key] = pooled_tensor\n                else:\n                    checkpoint_data[tensor_key] = tensor.detach().clone()\n\n        self.checkpoint_cache[checkpoint_key] = checkpoint_data\n        self.checkpoint_history.append({\n            'key': checkpoint_key,\n            'timestamp': time.time(),\n            'size': sum(t.numel() * t.element_size() for t in checkpoint_data.values())\n        })\n\n        return {\n            'checkpoint_key': checkpoint_key,\n            'saved_tensors': len(tensors),\n            'memory_saved': sum(t.numel() * t.element_size() for t in tensors)\n        }\n\n    def restore_tensors(self, checkpoint_key: str) -> List[torch.Tensor]:\n        """\n        Restore tensors from checkpoint using memory pool\n        """\n        if checkpoint_key not in self.checkpoint_cache:\n            raise KeyError(f"Checkpoint {checkpoint_key} not found")\n\n        checkpoint_data = self.checkpoint_cache[checkpoint_key]\n        restored_tensors = []\n\n        for key, stored_tensor in checkpoint_data.items():\n            # Check if it was stored in half precision\n            if stored_tensor.dtype == torch.float16:\n                # Restore to full precision\n                original_shape = stored_tensor.shape\n                original_dtype = torch.float32  # Assume original was float32\n                restored_tensor = self.memory_pool.allocate_tensor(original_shape, original_dtype, stored_tensor.device)\n                restored_tensor.copy_(stored_tensor.float())\n                restored_tensors.append(restored_tensor)\n            else:\n                restored_tensors.append(stored_tensor)\n\n        # Remove from cache after restoration (caller should manage lifecycle)\n        del self.checkpoint_cache[checkpoint_key]\n\n        return restored_tensors\n\n    def clear_checkpoint_cache(self):\n        """\n        Clear the checkpoint cache, returning tensors to the memory pool\n        """\n        for checkpoint_key, checkpoint_data in self.checkpoint_cache.items():\n            for tensor_key, pooled_tensor in checkpoint_data.items():\n                # Return pooled tensor to memory pool\n                if hasattr(pooled_tensor, 'shape'):  # Check if still a tensor\n                    self.memory_pool.deallocate_tensor(pooled_tensor)\n\n        self.checkpoint_cache.clear()\n        self.checkpoint_history.clear()\n\n\nclass VisionEncoderMemoryOptimizer:\n    """\n    Optimizes memory layouts specifically for vision encoder operations\n    """\n    def __init__(self, shared_memory_per_block: int = 48 * 1024):\n        self.shared_memory_per_block = shared_memory_per_block\n        self.memory_access_pattern_analyzer = self._analyze_memory_access_patterns()\n\n    def _analyze_memory_access_patterns(self) -> Dict[str, Any]:\n        """Analyze optimal memory access patterns for vision operations"""\n        return {\n            'convolutional': {\n                'memory_format': 'channels_last',  # Better for convolutions on NVIDIA GPUs\n                'tile_size': 64,  # Optimal for SM61 memory transactions\n            },\n            'attention': {\n                'memory_format': 'contiguous',  # Better for attention on most architectures\n                'tile_size': 32,  # Optimal for attention computations\n            },\n            'patch_processing': {\n                'memory_format': 'channels_last',  # For vision transformer patch processing\n                'tile_size': 14,  # Common patch grid size\n            }\n        }\n\n    def optimize_patch_processing_memory(self, batch_size: int, image_size: Tuple[int, int], patch_size: int) -> Dict:\n        """\n        Optimize memory layout for patch processing in vision transformers\n        """\n        h, w = image_size\n        num_patches_h = h // patch_size\n        num_patches_w = w // patch_size\n        num_patches = num_patches_h * num_patches_w\n\n        # Calculate memory requirements for different stages of patch processing\n        patch_embedding_shape = (batch_size, num_patches, patch_size * patch_size * 3)  # RGB channels\n        position_embedding_shape = (1, num_patches, patch_size * patch_size * 3)\n        cls_token_shape = (batch_size, 1, patch_size * patch_size * 3)\n\n        # Optimize for memory access patterns in SM61\n        memory_layout = {\n            'patches': patch_embedding_shape,\n            'position_embeddings': position_embedding_shape,\n            'cls_tokens': cls_token_shape,\n            'patch_embeddings': (batch_size, num_patches, patch_size * patch_size * 3),\n            'transformer_outputs': [(batch_size, num_patches + 1, patch_size * patch_size * 3)] * 12  # 12 layers\n        }\n\n        # Calculate total memory requirement\n        total_params = 0\n        for shape in memory_layout.values():\n            if isinstance(shape[0], int):  # Single shape\n                total_params += np.prod(shape)\n            else:  # Multiple shapes (like transformer outputs)\n                for s in shape:\n                    total_params += np.prod(s)\n\n        total_memory_mb = (total_params * 4) / (1024 * 1024)  # Assuming float32 (4 bytes)\n\n        return {\n            'memory_layout': memory_layout,\n            'total_memory_mb': total_memory_mb,\n            'memory_access_pattern': self.memory_access_pattern_analyzer['patch_processing']['memory_format'],\n            'tile_size': self.memory_access_pattern_analyzer['patch_processing']['tile_size']\n        }\n\n    def optimize_convolutional_memory(self, input_shape: Tuple[int, ...]) -> Dict:\n        """\n        Optimize memory for convolutional operations in vision processing\n        """\n        batch_size, channels, height, width = input_shape\n\n        # For SM61, consider using channels_last format for better memory access\n        memory_format = self.memory_access_pattern_analyzer['convolutional']['memory_format']\n\n        # Optimize for memory access patterns in convolutions based on SM61 capabilities\n        memory_layout = {\n            'input': input_shape,\n            'weights': (64, channels, 3, 3),  # Example conv kernel\n            'bias': (64,),  # Example bias\n            'output': (batch_size, 64, height, width),  # Example output\n            'feature_maps': [\n                (batch_size, channels, height, width),  # Input\n                (batch_size, 64, height, width),       # After conv\n                (batch_size, 64, height//2, width//2), # After pooling\n                (batch_size, 128, height//2, width//2), # After second conv\n            ]\n        }\n\n        # Calculate memory for each stage\n        total_memory = 0\n        stage_memory = []\n        for i, fmap_shape in enumerate(memory_layout['feature_maps']):\n            mem = np.prod(fmap_shape) * 4  # 4 bytes for float32\n            stage_memory.append(mem)\n            total_memory += mem\n\n        return {\n            'memory_layout': memory_layout,\n            'stage_memory_bytes': stage_memory,\n            'total_memory_bytes': total_memory,\n            'memory_format': memory_format,\n            'tile_size': self.memory_access_pattern_analyzer['convolutional']['tile_size']\n        }\n\n\nclass MemoryEfficientDataLoader:\n    """\n    Memory-efficient data loader with optimized data transfer and caching.\n    """\n    def __init__(self, dataset, memory_manager: MemoryManager = None,\n                 pin_memory: bool = True, **kwargs):\n        self.dataset = dataset\n        self.memory_manager = memory_manager or get_memory_manager()\n        self.pin_memory = pin_memory\n\n        # Set default values that are memory efficient for the target hardware\n        kwargs.setdefault('batch_size', 1)\n        kwargs.setdefault('shuffle', False)\n        kwargs.setdefault('drop_last', False)\n        kwargs.setdefault('pin_memory', pin_memory)\n\n        # Use multiprocessing for data loading to reduce memory overhead\n        kwargs.setdefault('num_workers', 2)\n        kwargs.setdefault('persistent_workers', True)\n\n        self.dataloader = torch.utils.data.DataLoader(dataset, **kwargs)\n\n        # Track memory usage\n        self.stats = {\n            'pinned_memory_batches': 0,\n            'transferred_batches': 0\n        }\n\n    def __iter__(self):\n        return iter(self.dataloader)\n\n    def __len__(self):\n        return len(self.dataloader)\n\n    def transfer_to_device(self, data, device: torch.device):\n        """Transfer data to device with memory optimization."""\n        if isinstance(data, torch.Tensor):\n            if self.pin_memory and data.device.type == 'cpu':\n                self.stats['pinned_memory_batches'] += 1\n                # Use memory manager for optimized transfer\n                return self.memory_manager.allocate_tensor(\n                    data.shape, data.dtype, device\n                ).copy_(data)\n            else:\n                return data.to(device, non_blocking=self.pin_memory)\n        elif isinstance(data, dict):\n            return {k: self.transfer_to_device(v, device) for k, v in data.items()}\n        elif isinstance(data, list):\n            return [self.transfer_to_device(d, device) for d in data]\n        else:\n            return data.to(device, non_blocking=self.pin_memory) if hasattr(data, 'to') else data\n\n\ndef create_optimized_dataloader(dataset, memory_manager: MemoryManager = None, **kwargs):\n    """Create an optimized data loader with memory-efficient settings."""\n    return MemoryEfficientDataLoader(dataset, memory_manager=memory_manager, **kwargs)\n\n\ndef optimize_model_memory(model: torch.nn.Module, memory_manager: MemoryManager = None, config: MemoryConfig = None):\n    """\n    Apply memory optimizations to the given model.\n    """\n    if config is None:\n        config = MemoryConfig()\n\n    if memory_manager is None:\n        memory_manager = get_memory_manager(config)\n\n    # Add memory manager reference to model components that need it\n    for module in model.modules():\n        if hasattr(module, '_register_memory_manager'):\n            module._register_memory_manager(memory_manager)\n\n    # Optimize tensor allocation for model parameters\n    for name, param in model.named_parameters():\n        # For SM61, consider using half precision for some parameters to save memory\n        if config.use_inference_memory_efficient and param.dtype == torch.float32:\n            # Don't change dtype here, but ensure memory-efficient allocation\n            # The dtype change would happen during model initialization\n            pass\n\n    return model\n\n\n# Global memory manager instance\n_global_memory_manager = None\n_global_manager_lock = threading.Lock()\n\n\ndef get_memory_manager(config: MemoryConfig = None) -> MemoryManager:\n    """Get the global memory manager instance"""\n    global _global_memory_manager\n    with _global_manager_lock:\n        if _global_memory_manager is None:\n            if _global_memory_manager is None:\n                _global_memory_manager = MemoryManager(config or MemoryConfig())\n    return _global_memory_manager\n\n\ndef allocate_tensor_with_manager(shape: Tuple[int, ...], dtype: torch.dtype = torch.float32,\n                                device: torch.device = torch.device('cpu')) -> torch.Tensor:\n    """Allocate a tensor using the global memory manager"""\n    manager = get_memory_manager()\n    return manager.allocate_tensor(shape, dtype, device)\n\n\ndef free_tensor_with_manager(tensor: torch.Tensor) -> bool:\n    """Free a tensor using the global memory manager"""\n    manager = get_memory_manager()\n    return manager.free_tensor(tensor)