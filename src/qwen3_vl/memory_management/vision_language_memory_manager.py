"""\nRefactored Advanced Memory Management System for Vision-Language Models (Qwen3-VL)\nFollowing the new modular architecture with dependency injection and proper separation of concerns.\n"""\n\nimport ctypes\nimport mmap\nimport os\nimport sys\nfrom typing import Dict, List, Tuple, Optional, Any, Union\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport numpy as np\nimport threading\nimport time\nfrom collections import defaultdict, deque\nimport gc\nimport weakref\nfrom concurrent.futures import ThreadPoolExecutor\nimport logging\nimport math\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    import psutil\nexcept ImportError:\n    logger.warning("psutil not available, some memory monitoring features will be limited")\n    psutil = None\n\nfrom config.config import Qwen3VLConfig\nfrom system.interfaces import MemoryManager\n# from src.qwen3_vl.system.di_container import get_container  # Commented out to avoid circular import\n\n\nclass MemoryPoolType(Enum):\n    """Types of memory pools for different allocation patterns"""\n    TENSOR_DATA = "tensor_data"           # For tensor storage\n    ACTIVATION_BUFFER = "activation_buffer"  # For intermediate activations\n    KV_CACHE = "kv_cache"                 # For attention mechanisms\n    TEMPORARY = "temporary"               # For temporary computations\n    FIXED_SIZE = "fixed_size"             # For fixed-size allocations\n\n\n@dataclass\nclass MemoryBlock:\n    """Represents a memory block in the pool"""\n    ptr: int                    # Memory address\n    size: int                   # Size in bytes\n    pool_type: MemoryPoolType   # Type of pool this block belongs to\n    allocated: bool             # Allocation status\n    timestamp: float            # Time of allocation\n    ref_count: int              # Reference count for smart deallocation\n    alignment: int              # Memory alignment boundary\n\n\nclass MemoryDefragmenter:\n    """Handles memory defragmentation for optimal allocation"""\n\n    def __init__(self, memory_pool):\n        self.memory_pool = memory_pool\n        self.defrag_threshold = 0.3  # Defragment when fragmentation > 30%\n        self.defrag_lock = threading.Lock()\n\n    def calculate_fragmentation(self) -> float:\n        """Calculate current memory fragmentation level"""\n        free_blocks = [block for block in self.memory_pool.blocks if not block.allocated]\n        if not free_blocks:\n            return 0.0\n\n        total_free = sum(block.size for block in free_blocks)\n        largest_free = max((block.size for block in free_blocks), default=0)\n\n        if total_free == 0:\n            return 0.0\n\n        return 1.0 - (largest_free / total_free) if total_free > 0 else 0.0\n\n    def compact_memory(self):\n        """Compact memory blocks to reduce fragmentation"""\n        with self.defrag_lock:\n            # Sort free blocks by address to identify contiguous regions\n            free_blocks = [block for block in self.memory_pool.blocks if not block.allocated]\n            free_blocks.sort(key=lambda x: x.ptr)\n\n            # Merge contiguous free blocks\n            i = 0\n            while i < len(free_blocks) - 1:\n                current = free_blocks[i]\n                next_block = free_blocks[i + 1]\n\n                # Check if blocks are contiguous\n                if current.ptr + current.size == next_block.ptr:\n                    # Merge blocks\n                    current.size += next_block.size\n                    self.memory_pool.blocks.remove(next_block)\n                    # Remove from free_blocks list too\n                    free_blocks.pop(i + 1)\n                    continue\n\n                i += 1\n\n    def should_defragment(self) -> bool:\n        """Check if defragmentation is needed"""\n        fragmentation = self.calculate_fragmentation()\n        return fragmentation > self.defrag_threshold\n\n\nclass AdvancedMemoryPool:\n    """Advanced memory pool with NUMA awareness and cache optimization"""\n\n    def __init__(self, initial_size: int = 1024 * 1024 * 1024,  # 1GB default\n                 page_size: int = 4096,  # Standard 4KB page\n                 enable_defragmentation: bool = True):\n        """\n        Initialize advanced memory pool\n\n        Args:\n            initial_size: Initial size of the memory pool in bytes\n            page_size: Memory page size for alignment\n            enable_defragmentation: Whether to enable automatic defragmentation\n        """\n        # Input validation\n        if not isinstance(initial_size, int) or initial_size <= 0:\n            raise ValueError(f"initial_size must be a positive integer, got {initial_size}")\n        if not isinstance(page_size, int) or page_size <= 0:\n            raise ValueError(f"page_size must be a positive integer, got {page_size}")\n        if not isinstance(enable_defragmentation, bool):\n            raise ValueError(f"enable_defragmentation must be a boolean, got {enable_defragmentation}")\n\n        self.initial_size = initial_size\n        self.page_size = page_size\n        self.enable_defragmentation = enable_defragmentation\n\n        # Create memory pool using mmap for better control\n        # On Windows, we need to use different approach as MAP_PRIVATE/MAP_ANONYMOUS don't exist\n        try:\n            # Try to use mmap with anonymous mapping (Unix/Linux)\n            # Check if the required flags exist\n            if hasattr(mmap, 'MAP_PRIVATE') and hasattr(mmap, 'MAP_ANONYMOUS'):\n                self.pool_ptr = mmap.mmap(-1, initial_size, mmap.MAP_PRIVATE | mmap.MAP_ANONYMOUS)\n            else:\n                raise AttributeError("Required mmap flags not available")\n        except (AttributeError, OSError):\n            # On Windows, create a temporary file-backed mapping\n            import tempfile\n            try:\n                self.temp_file = tempfile.TemporaryFile()\n                self.temp_file.truncate(initial_size)\n                self.pool_ptr = mmap.mmap(self.temp_file.fileno(), initial_size)\n            except Exception as e:\n                logger.error(f"Failed to create memory pool: {e}")\n                raise RuntimeError(f"Failed to create memory pool: {e}") from e\n\n        try:\n            self.pool_base = ctypes.addressof(ctypes.c_char.from_buffer(self.pool_ptr))\n        except Exception as e:\n            logger.error(f"Failed to get memory pool base address: {e}")\n            raise RuntimeError(f"Failed to get memory pool base address: {e}") from e\n\n        # Track memory blocks\n        self.blocks: List[MemoryBlock] = []\n        self.block_map: Dict[int, MemoryBlock] = {}  # Address -> Block mapping\n        self.pool_lock = threading.RLock()\n\n        # Pool statistics\n        self.stats = {\n            'total_allocated': 0,\n            'total_freed': 0,\n            'current_usage': 0,\n            'peak_usage': 0,\n            'allocation_count': 0,\n            'deallocation_count': 0\n        }\n\n        # Initialize with one large free block\n        try:\n            initial_block = MemoryBlock(\n                ptr=self.pool_base,\n                size=initial_size,\n                pool_type=MemoryPoolType.TEMPORARY,\n                allocated=False,\n                timestamp=time.time(),\n                ref_count=0,\n                alignment=page_size\n            )\n            self.blocks.append(initial_block)\n            self.block_map[self.pool_base] = initial_block\n        except Exception as e:\n            logger.error(f"Failed to initialize memory blocks: {e}")\n            raise RuntimeError(f"Failed to initialize memory blocks: {e}") from e\n\n        # Defragmenter\n        try:\n            self.defragmenter = MemoryDefragmenter(self) if enable_defragmentation else None\n        except Exception as e:\n            logger.error(f"Failed to initialize defragmenter: {e}")\n            raise RuntimeError(f"Failed to initialize defragmenter: {e}") from e\n\n        # Thread-local storage for per-thread pools\n        self.thread_local = threading.local()\n\n        logger.info(f"AdvancedMemoryPool initialized with {initial_size / (1024**3):.2f} GB")\n\n    def _align_size(self, size: int, alignment: int) -> int:\n        """Align size to the specified boundary"""\n        if not isinstance(size, int) or size < 0:\n            raise ValueError(f"size must be a non-negative integer, got {size}")\n        if not isinstance(alignment, int) or alignment <= 0:\n            raise ValueError(f"alignment must be a positive integer, got {alignment}")\n        return ((size + alignment - 1) // alignment) * alignment\n\n    def _find_suitable_block(self, size: int, alignment: int, pool_type: MemoryPoolType) -> Optional[MemoryBlock]:\n        """Find a suitable free block for allocation"""\n        # Input validation\n        if not isinstance(size, int) or size <= 0:\n            raise ValueError(f"size must be a positive integer, got {size}")\n        if not isinstance(alignment, int) or alignment <= 0:\n            raise ValueError(f"alignment must be a positive integer, got {alignment}")\n        if not isinstance(pool_type, MemoryPoolType):\n            raise ValueError(f"pool_type must be a MemoryPoolType, got {pool_type}")\n\n        try:\n            aligned_size = self._align_size(size, alignment)\n        except ValueError as e:\n            logger.error(f"Invalid alignment parameters: {e}")\n            raise\n\n        # Look for best fit (smallest block that fits)\n        try:\n            suitable_blocks = [\n                block for block in self.blocks\n                if not block.allocated and block.size >= aligned_size and block.alignment >= alignment\n            ]\n        except Exception as e:\n            logger.error(f"Error filtering suitable blocks: {e}")\n            raise RuntimeError(f"Error filtering suitable blocks: {e}") from e\n\n        if not suitable_blocks:\n            return None\n\n        # Return best fit (smallest block that still fits)\n        try:\n            return min(suitable_blocks, key=lambda b: b.size)\n        except ValueError:\n            # This happens if suitable_blocks is empty, which we already handled\n            return None\n\n    def allocate(self, size: int, pool_type: MemoryPoolType = MemoryPoolType.TEMPORARY,\n                 alignment: int = None) -> Optional[Tuple[int, int]]:\n        """\n        Allocate memory from the pool\n\n        Args:\n            size: Size to allocate in bytes\n            pool_type: Type of memory pool\n            alignment: Memory alignment requirement (defaults to page_size)\n\n        Returns:\n            Tuple of (memory_address, actual_allocated_size) or None if allocation fails\n        """\n        # Input validation\n        if not isinstance(size, int) or size <= 0:\n            raise ValueError(f"size must be a positive integer, got {size}")\n        if not isinstance(pool_type, MemoryPoolType):\n            raise ValueError(f"pool_type must be a MemoryPoolType, got {pool_type}")\n        if alignment is not None and (not isinstance(alignment, int) or alignment <= 0):\n            raise ValueError(f"alignment must be a positive integer or None, got {alignment}")\n\n        if alignment is None:\n            alignment = self.page_size\n\n        try:\n            aligned_size = self._align_size(size, alignment)\n        except ValueError as e:\n            logger.error(f"Invalid alignment parameters: {e}")\n            raise\n\n        with self.pool_lock:\n            try:\n                # Check if defragmentation is needed\n                if self.defragmenter and self.defragmenter.should_defragment():\n                    logger.debug("Performing memory defragmentation")\n                    self.defragmenter.compact_memory()\n\n                block = self._find_suitable_block(aligned_size, alignment, pool_type)\n\n                if block is None:\n                    # Try to expand pool if possible\n                    if self._expand_pool(aligned_size):\n                        block = self._find_suitable_block(aligned_size, alignment, pool_type)\n                        if block is None:\n                            logger.warning(f"Failed to allocate {aligned_size} bytes after expanding pool")\n                            return None\n                    else:\n                        logger.warning(f"Failed to allocate {aligned_size} bytes - no suitable block found and pool expansion failed")\n                        return None\n\n                # Split block if it's much larger than needed\n                if block.size > aligned_size * 2:  # If block is significantly larger\n                    new_block = MemoryBlock(\n                        ptr=block.ptr + aligned_size,\n                        size=block.size - aligned_size,\n                        pool_type=MemoryPoolType.TEMPORARY,\n                        allocated=False,\n                        timestamp=time.time(),\n                        ref_count=0,\n                        alignment=alignment\n                    )\n\n                    block.size = aligned_size\n                    self.blocks.append(new_block)\n                    self.block_map[new_block.ptr] = new_block\n\n                # Mark block as allocated\n                block.allocated = True\n                block.pool_type = pool_type\n                block.timestamp = time.time()\n                block.ref_count = 1\n\n                # Update statistics\n                self.stats['total_allocated'] += aligned_size\n                self.stats['current_usage'] += aligned_size\n                self.stats['allocation_count'] += 1\n                if self.stats['current_usage'] > self.stats['peak_usage']:\n                    self.stats['peak_usage'] = self.stats['current_usage']\n\n                logger.debug(f"Allocated {aligned_size} bytes at {hex(block.ptr)} for {pool_type.value}")\n                return block.ptr, aligned_size\n            except Exception as e:\n                logger.error(f"Error during allocation: {e}")\n                raise RuntimeError(f"Error during allocation: {e}") from e\n\n    def _expand_pool(self, additional_size: int) -> bool:\n        """Attempt to expand the memory pool"""\n        # Input validation\n        if not isinstance(additional_size, int) or additional_size <= 0:\n            logger.error(f"additional_size must be a positive integer, got {additional_size}")\n            return False\n\n        try:\n            # Calculate new size (double the current size or add enough for the request)\n            current_size = len(self.pool_ptr)\n            new_size = max(current_size * 2, current_size + additional_size * 2)\n\n            # Resize the memory mapping\n            self.pool_ptr.resize(new_size)\n\n            # Add the new region as a free block\n            new_base = ctypes.addressof(ctypes.c_char.from_buffer(self.pool_ptr)) + current_size\n            new_block = MemoryBlock(\n                ptr=new_base,\n                size=new_size - current_size,\n                pool_type=MemoryPoolType.TEMPORARY,\n                allocated=False,\n                timestamp=time.time(),\n                ref_count=0,\n                alignment=self.page_size\n            )\n\n            self.blocks.append(new_block)\n            self.block_map[new_block.ptr] = new_block\n\n            logger.debug(f"Expanded memory pool to {new_size / (1024**3):.2f} GB")\n            return True\n        except OSError as e:\n            logger.error(f"OS error during memory pool expansion: {e}")\n            return False\n        except Exception as e:\n            logger.error(f"Failed to expand memory pool: {e}")\n            return False\n\n    def deallocate(self, ptr: int) -> bool:\n        """\n        Deallocate memory back to the pool\n\n        Args:\n            ptr: Pointer to deallocate\n\n        Returns:\n            True if successful, False otherwise\n        """\n        # Input validation\n        if not isinstance(ptr, int) or ptr <= 0:\n            logger.error(f"ptr must be a positive integer, got {ptr}")\n            return False\n\n        with self.pool_lock:\n            try:\n                if ptr not in self.block_map:\n                    logger.warning(f"Attempted to deallocate unknown pointer: {hex(ptr)}")\n                    return False\n\n                block = self.block_map[ptr]\n\n                if not block.allocated:\n                    logger.warning(f"Attempted to deallocate already freed block: {hex(ptr)}")\n                    return False\n\n                # Decrement reference count\n                block.ref_count -= 1\n\n                if block.ref_count <= 0:\n                    block.allocated = False\n                    block.ref_count = 0\n\n                    # Update statistics\n                    self.stats['total_freed'] += block.size\n                    self.stats['current_usage'] -= block.size\n                    self.stats['deallocation_count'] += 1\n\n                    logger.debug(f"Deallocated {block.size} bytes at {hex(ptr)}")\n                    return True\n                else:\n                    logger.debug(f"Reduced ref count for {hex(ptr)}, now {block.ref_count}")\n                    return True\n            except KeyError:\n                logger.warning(f"Block not found in block_map for pointer: {hex(ptr)}")\n                return False\n            except Exception as e:\n                logger.error(f"Error during deallocation: {e}")\n                return False\n\n    def get_stats(self) -> Dict[str, Any]:\n        """Get memory pool statistics"""\n        with self.pool_lock:\n            stats = self.stats.copy()\n            stats['fragmentation'] = self.defragmenter.calculate_fragmentation() if self.defragmenter else 0.0\n            stats['pool_utilization'] = (stats['current_usage'] / self.initial_size) if self.initial_size > 0 else 0.0\n            return stats\n\n    def cleanup(self):\n        """Clean up the memory pool"""\n        with self.pool_lock:\n            try:\n                if hasattr(self, 'pool_ptr') and self.pool_ptr:\n                    try:\n                        self.pool_ptr.close()\n                    except Exception as e:\n                        logger.warning(f"Error closing memory pool: {e}")\n            except Exception as e:\n                logger.error(f"Error accessing pool_ptr during cleanup: {e}")\n\n            if hasattr(self, 'temp_file'):\n                try:\n                    self.temp_file.close()\n                    # On Windows, explicitly delete the temporary file to ensure cleanup\n                    import tempfile\n                    import os\n                    temp_filename = getattr(self.temp_file, 'name', None)\n                    if temp_filename and os.path.exists(temp_filename):\n                        try:\n                            os.unlink(temp_filename)\n                        except Exception as e:\n                            # If we can't delete the file, it might be in use by another process\n                            logger.warning(f"Could not delete temporary file {temp_filename}: {e}")\n                except Exception as e:\n                    # Handle case where file is already closed\n                    logger.warning(f"Error closing temporary file: {e}")\n\n\nclass CacheAwareMemoryManager:\n    """Cache-aware memory manager optimizing for CPU cache performance"""\n\n    def __init__(self, cache_line_size: int = 64, l1_size: int = 32 * 1024,\n                 l2_size: int = 256 * 1024, l3_size: int = 6 * 1024 * 1024):\n        """\n        Initialize cache-aware memory manager\n\n        Args:\n            cache_line_size: CPU cache line size in bytes\n            l1_size: L1 cache size in bytes\n            l2_size: L2 cache size in bytes\n            l3_size: L3 cache size in bytes\n        """\n        self.cache_line_size = cache_line_size\n        self.l1_size = l1_size\n        self.l2_size = l2_size\n        self.l3_size = l3_size\n\n        # Memory layout optimization flags\n        self.use_cache_blocking = True\n        self.optimize_for_temporal_locality = True\n        self.use_prefetching = True\n\n        # Cache performance metrics\n        self.cache_hits = 0\n        self.cache_misses = 0\n\n    def optimize_memory_layout(self, data: np.ndarray, layout_type: str = "cache_friendly") -> np.ndarray:\n        """\n        Optimize memory layout for cache performance\n\n        Args:\n            data: Input numpy array\n            layout_type: Type of optimization ('cache_friendly', 'row_major', 'col_major', 'blocked')\n\n        Returns:\n            Optimized array\n        """\n        if layout_type == "cache_friendly":\n            # For matrices, optimize for row-major access patterns\n            if data.ndim == 2:\n                # Ensure contiguous memory layout\n                return np.ascontiguousarray(data)\n            elif data.ndim == 3:\n                # For tensors, optimize for the most frequently accessed dimension first\n                return np.ascontiguousarray(data)\n        elif layout_type == "blocked":\n            # Apply cache blocking/tiling for matrix operations\n            return self._apply_cache_blocking(data)\n        elif layout_type == "row_major":\n            return np.asarray(data, order='C')\n        elif layout_type == "col_major":\n            return np.asarray(data, order='F')\n\n        return data\n\n    def _apply_cache_blocking(self, data: np.ndarray, block_size: int = 64) -> np.ndarray:\n        """Apply cache blocking to optimize memory access patterns"""\n        if data.ndim != 2:\n            return data  # Only implement for 2D arrays for now\n\n        rows, cols = data.shape\n        blocked_array = np.empty_like(data)\n\n        # Apply blocking in both dimensions\n        for i in range(0, rows, block_size):\n            for j in range(0, cols, block_size):\n                end_i = min(i + block_size, rows)\n                end_j = min(j + block_size, cols)\n                blocked_array[i:end_i, j:end_j] = data[i:end_i, j:end_j]\n\n        return blocked_array\n\n    def prefetch_data(self, data_ptr: int, size: int, offset: int = 0):\n        """\n        Prefetch data into CPU cache\n\n        Args:\n            data_ptr: Memory address to prefetch\n            size: Size of data to prefetch\n            offset: Offset from pointer\n        """\n        if not self.use_prefetching:\n            return\n\n        # Use low-level prefetch instruction if available\n        # Note: This is a simplified simulation - real implementation would use platform-specific instructions\n        try:\n            # In practice, this would use intrinsics like _mm_prefetch on x86\n            # For simulation, we'll just touch the memory to hint to the OS\n            ctypes.memmove(data_ptr + offset, data_ptr + offset, min(64, size))  # Touch cache line\n        except:\n            pass  # Silently handle if low-level access fails\n\n\nclass GPUCPUMemoryOptimizer:\n    """Optimizes memory transfers between GPU and CPU for vision-language models"""\n\n    def __init__(self, device_memory_limit: int = 2 * 1024 * 1024 * 1024):  # 2GB default\n        self.device_memory_limit = device_memory_limit\n        self.host_page_locked_pool = None\n        self.pinned_memory_enabled = True\n        self.unified_memory_enabled = False  # For CUDA unified memory systems\n\n        # Initialize pinned memory pool if available\n        self._init_pinned_memory_pool()\n\n    def _init_pinned_memory_pool(self):\n        """Initialize pinned memory pool for faster GPU transfers"""\n        try:\n            import torch\n            if torch.cuda.is_available():\n                # Enable pinned memory for faster host-device transfers\n                self.pinned_memory_enabled = True\n                logger.info("Pinned memory enabled for GPU transfers")\n            else:\n                logger.info("CUDA not available, using standard memory transfers")\n        except ImportError:\n            logger.info("PyTorch not available, using standard memory transfers")\n\n    def optimize_tensor_placement(self, tensor: Any, target_device: str = "auto") -> Any:\n        """\n        Optimize tensor placement between CPU and GPU based on memory constraints\n\n        Args:\n            tensor: Input tensor\n            target_device: Target device ('cpu', 'cuda', 'auto')\n\n        Returns:\n            Tensor placed optimally\n        """\n        try:\n            import torch\n            if not isinstance(tensor, torch.Tensor):\n                # Convert numpy arrays to torch tensors for optimization\n                if isinstance(tensor, np.ndarray):\n                    tensor = torch.from_numpy(tensor)\n                else:\n                    tensor = torch.tensor(tensor)\n\n            if target_device == "auto":\n                # Estimate memory usage and place accordingly\n                tensor_size = tensor.element_size() * tensor.nelement()\n                available_gpu_memory = self._get_available_gpu_memory()\n\n                if tensor_size < available_gpu_memory * 0.8:  # Use 80% threshold\n                    target_device = "cuda" if torch.cuda.is_available() else "cpu"\n                else:\n                    target_device = "cpu"\n\n            if target_device == "cuda" and torch.cuda.is_available():\n                # Use pinned memory for faster transfer if available\n                if self.pinned_memory_enabled:\n                    return tensor.pin_memory().to('cuda', non_blocking=True)\n                else:\n                    return tensor.to('cuda')\n            else:\n                return tensor.to('cpu')\n\n        except ImportError:\n            # Fallback if PyTorch is not available\n            return tensor\n\n    def _get_available_gpu_memory(self) -> int:\n        """Get available GPU memory in bytes"""\n        try:\n            import torch\n            if torch.cuda.is_available():\n                total_memory = torch.cuda.get_device_properties(0).total_memory\n                reserved_memory = torch.cuda.memory_reserved(0)\n                allocated_memory = torch.cuda.memory_allocated(0)\n                return total_memory - reserved_memory\n            else:\n                return 0\n        except ImportError:\n            return 0\n\n    def batch_memory_transfer(self, tensors: List[Any], target_device: str = "cuda") -> List[Any]:\n        """Optimize batch memory transfers"""\n        optimized_tensors = []\n\n        for tensor in tensors:\n            optimized_tensor = self.optimize_tensor_placement(tensor, target_device)\n            optimized_tensors.append(optimized_tensor)\n\n        return optimized_tensors\n\n\nclass VisionLanguageMemoryManager(MemoryManager):\n    """\n    Vision-Language Memory Manager implementing the MemoryManager interface.\n    This is the main entry point for memory management in the refactored architecture.\n    """\n    \n    def __init__(self, config: Qwen3VLConfig):\n        """\n        Initialize the vision-language memory manager with dependency injection.\n\n        Args:\n            config: Qwen3VLConfig instance with memory management settings\n        """\n        self.config = config\n\n        # Validate inputs\n        if not isinstance(config, Qwen3VLConfig):\n            raise TypeError(f"config must be a Qwen3VLConfig instance, got {type(config)}")\n\n        # Initialize memory pools based on configuration\n        self.enable_memory_pool = config.use_sparsity or config.use_moe or config.kv_cache_strategy != "standard"\n        self.enable_cache_optimization = True\n        self.enable_gpu_optimization = True\n\n        # Initialize components\n        memory_pool_size = getattr(config, 'memory_pool_size', 2 * 1024 * 1024 * 1024)  # 2GB default\n        try:\n            self.memory_pool = AdvancedMemoryPool(memory_pool_size) if self.enable_memory_pool else None\n        except Exception as e:\n            logger.error(f"Error initializing memory pool: {e}")\n            self.memory_pool = None\n            self.enable_memory_pool = False\n\n        try:\n            self.cache_manager = CacheAwareMemoryManager() if self.enable_cache_optimization else None\n        except Exception as e:\n            logger.error(f"Error initializing cache manager: {e}")\n            self.cache_manager = None\n            self.enable_cache_optimization = False\n\n        try:\n            self.gpu_optimizer = GPUCPUMemoryOptimizer() if self.enable_gpu_optimization else None\n        except Exception as e:\n            logger.error(f"Error initializing GPU optimizer: {e}")\n            self.gpu_optimizer = None\n            self.enable_gpu_optimization = False\n\n        # Specialized pools for different types of data in vision-language models\n        self.kv_cache_pool = None\n        self.image_feature_pool = None\n        self.text_embedding_pool = None\n\n        if self.enable_memory_pool:\n            try:\n                # Create specialized pools based on configuration\n                kv_cache_size = getattr(config, 'kv_cache_pool_size', 512 * 1024 * 1024)  # 512MB for KV cache\n                image_feature_size = getattr(config, 'image_feature_pool_size', 1024 * 1024 * 1024)  # 1GB for image features\n                text_embedding_size = getattr(config, 'text_embedding_pool_size', 512 * 1024 * 1024)  # 512MB for text embeddings\n                \n                self.kv_cache_pool = AdvancedMemoryPool(kv_cache_size)\n                self.image_feature_pool = AdvancedMemoryPool(image_feature_size)\n                self.text_embedding_pool = AdvancedMemoryPool(text_embedding_size)\n            except Exception as e:\n                logger.error(f"Error initializing specialized memory pools: {e}")\n                self.kv_cache_pool = None\n                self.image_feature_pool = None\n                self.text_embedding_pool = None\n\n        # Track tensor allocations using a regular dictionary with array IDs as keys\n        # This avoids issues with numpy arrays not being hashable\n        self.tensor_allocation_map = {}\n        self.tensor_allocation_lock = threading.Lock()\n\n        logger.info("VisionLanguageMemoryManager initialized")\n\n    def allocate(self, size: int, pool_type: str = "general") -> Optional[Tuple[int, int]]:\n        """\n        Allocate memory of specified size using the MemoryManager interface.\n\n        Args:\n            size: Size in bytes to allocate\n            pool_type: Type of memory pool to use\n\n        Returns:\n            Tuple of (memory_address, actual_allocated_size) or None if allocation fails\n        """\n        # Map string pool type to enum\n        pool_type_enum = self._map_pool_type(pool_type)\n        \n        # Allocate using the appropriate pool based on type\n        if pool_type_enum == MemoryPoolType.KV_CACHE and self.kv_cache_pool:\n            return self.kv_cache_pool.allocate(size, pool_type_enum)\n        elif pool_type_enum == MemoryPoolType.ACTIVATION_BUFFER and self.memory_pool:\n            # For activation buffers, use the main pool\n            return self.memory_pool.allocate(size, pool_type_enum)\n        elif pool_type_enum == MemoryPoolType.TENSOR_DATA and self.memory_pool:\n            return self.memory_pool.allocate(size, pool_type_enum)\n        elif pool_type_enum == MemoryPoolType.TEMPORARY and self.memory_pool:\n            return self.memory_pool.allocate(size, pool_type_enum)\n        elif pool_type_enum == MemoryPoolType.FIXED_SIZE and self.memory_pool:\n            return self.memory_pool.allocate(size, pool_type_enum)\n        else:\n            # Use general pool if no specific pool matches\n            if self.memory_pool:\n                return self.memory_pool.allocate(size, MemoryPoolType.TENSOR_DATA)\n            else:\n                # If no pools are enabled, return a simulated allocation\n                import time\n                ptr = int(time.time() * 1000000) % 1000000000  # Simulated pointer\n                return ptr, size\n\n    def deallocate(self, ptr: int) -> bool:\n        """\n        Deallocate memory at the specified pointer using the MemoryManager interface.\n\n        Args:\n            ptr: Pointer to deallocate\n\n        Returns:\n            True if successful, False otherwise\n        """\n        # Try to deallocate from all pools\n        success = False\n        if self.memory_pool:\n            success |= self.memory_pool.deallocate(ptr)\n        if self.kv_cache_pool:\n            success |= self.kv_cache_pool.deallocate(ptr)\n        if self.image_feature_pool:\n            success |= self.image_feature_pool.deallocate(ptr)\n        if self.text_embedding_pool:\n            success |= self.text_embedding_pool.deallocate(ptr)\n        \n        return success\n\n    def get_stats(self) -> Dict[str, Any]:\n        """\n        Get memory usage statistics using the MemoryManager interface.\n\n        Returns:\n            Dictionary with memory statistics\n        """\n        stats = {}\n\n        if self.memory_pool:\n            stats['general_pool'] = self.memory_pool.get_stats()\n\n        if self.kv_cache_pool:\n            stats['kv_cache_pool'] = self.kv_cache_pool.get_stats()\n\n        if self.image_feature_pool:\n            stats['image_feature_pool'] = self.image_feature_pool.get_stats()\n\n        if self.text_embedding_pool:\n            stats['text_embedding_pool'] = self.text_embedding_pool.get_stats()\n\n        # System memory info if psutil is available\n        if psutil:\n            stats['system_memory'] = {\n                'virtual_memory_percent': psutil.virtual_memory().percent,\n                'available_gb': psutil.virtual_memory().available / (1024**3),\n                'used_gb': psutil.virtual_memory().used / (1024**3)\n            }\n\n        return stats\n\n    def _map_pool_type(self, pool_type: str) -> MemoryPoolType:\n        """\n        Map string pool type to MemoryPoolType enum.\n\n        Args:\n            pool_type: String representation of pool type\n\n        Returns:\n            MemoryPoolType enum value\n        """\n        mapping = {\n            'tensor_data': MemoryPoolType.TENSOR_DATA,\n            'activation': MemoryPoolType.ACTIVATION_BUFFER,\n            'kv_cache': MemoryPoolType.KV_CACHE,\n            'temporary': MemoryPoolType.TEMPORARY,\n            'fixed_size': MemoryPoolType.FIXED_SIZE,\n            'general': MemoryPoolType.TENSOR_DATA\n        }\n        return mapping.get(pool_type.lower(), MemoryPoolType.TENSOR_DATA)\n\n    def allocate_tensor_memory(self, shape: Tuple[int, ...], dtype=np.float32,\n                              tensor_type: str = "general") -> Optional[np.ndarray]:\n        """\n        Allocate memory for tensors with optimization.\n\n        Args:\n            shape: Shape of the tensor\n            dtype: Data type\n            tensor_type: Type of tensor ('kv_cache', 'image_features', 'text_embeddings', 'general')\n\n        Returns:\n            Allocated numpy array or None if allocation fails\n        """\n        # Validate inputs\n        if not isinstance(shape, (tuple, list)):\n            raise TypeError(f"shape must be a tuple or list, got {type(shape)}")\n\n        if not all(isinstance(dim, int) and dim > 0 for dim in shape):\n            raise ValueError(f"shape must contain positive integers, got {shape}")\n\n        if not isinstance(tensor_type, str):\n            raise TypeError(f"tensor_type must be a string, got {type(tensor_type)}")\n\n        valid_tensor_types = {'general', 'kv_cache', 'image_features', 'text_embeddings'}\n        if tensor_type not in valid_tensor_types:\n            raise ValueError(f"tensor_type must be one of {valid_tensor_types}, got {tensor_type}")\n\n        # Handle both numpy and torch dtypes with proper validation\n        original_dtype = dtype\n        try:\n            if isinstance(dtype, type) and hasattr(dtype, 'dtype'):  # If it's a numpy generic type\n                # This is a numpy scalar type, get the corresponding dtype\n                dtype = np.dtype(dtype)\n            elif hasattr(dtype, 'dtype'):  # If it's a torch tensor with dtype attribute\n                dtype = dtype.dtype\n            elif hasattr(dtype, 'name'):  # If it's already a numpy dtype\n                dtype = np.dtype(dtype)\n            elif isinstance(dtype, str):  # If it's a string\n                dtype = np.dtype(dtype)  # Convert string to numpy dtype\n            else:  # If it's a torch dtype directly or other format\n                try:\n                    import torch\n                    if isinstance(dtype, torch.dtype):\n                        # Convert torch dtype to numpy dtype\n                        torch_to_numpy_dtype = {\n                            torch.float32: np.float32,\n                            torch.float64: np.float64,\n                            torch.float16: np.float16,\n                            torch.bfloat16: np.float32,  # bfloat16 maps to float32 in numpy\n                            torch.int32: np.int32,\n                            torch.int64: np.int64,\n                            torch.int16: np.int16,\n                            torch.int8: np.int8,\n                            torch.uint8: np.uint8,\n                            torch.bool: np.bool_,\n                            torch.complex64: np.complex64,\n                            torch.complex128: np.complex128,\n                        }\n                        dtype = torch_to_numpy_dtype.get(dtype, np.float32)\n                    else:\n                        # If it's not a torch dtype, try to use it directly as numpy dtype\n                        try:\n                            dtype = np.dtype(dtype)\n                        except:\n                            # Default fallback\n                            dtype = np.float32\n                except ImportError:\n                    # If torch is not available, try to use directly as numpy dtype\n                    try:\n                        dtype = np.dtype(dtype)\n                    except:\n                        # Default fallback\n                        dtype = np.float32\n\n            element_size = np.dtype(dtype).itemsize\n            total_elements = np.prod(shape)\n            if not isinstance(total_elements, (int, np.integer)) or total_elements <= 0:\n                raise ValueError(f"Invalid total number of elements: {total_elements}")\n\n            size_bytes = int(element_size * total_elements)  # Convert to Python int to avoid numpy int64 issues\n\n            # Check for potential overflow\n            if size_bytes < 0:\n                raise OverflowError(f"Size calculation overflowed: {element_size} * {total_elements}")\n\n            # For now, use the memory pool to track allocations but create arrays separately\n            # to avoid mmap buffer export issues. This still provides memory management benefits.\n            if self.memory_pool and tensor_type == "general":\n                if self.memory_pool:\n                    ptr, actual_size = self.allocate(size_bytes, "tensor_data")\n                    if ptr:\n                        # Create array with standard allocation but track with our pool\n                        array = np.zeros(shape, dtype=dtype)\n\n                        if self.cache_manager:\n                            array = self.cache_manager.optimize_memory_layout(array)\n\n                        # Store reference to track when it should be "freed" using array id as key\n                        with self.tensor_allocation_lock:\n                            self.tensor_allocation_map[id(array)] = (self.memory_pool, ptr, size_bytes)\n                        return array\n            elif tensor_type == "kv_cache" and self.kv_cache_pool:\n                ptr, actual_size = self.allocate(size_bytes, "kv_cache")\n                if ptr:\n                    array = np.zeros(shape, dtype=dtype)\n                    with self.tensor_allocation_lock:\n                        self.tensor_allocation_map[id(array)] = (self.kv_cache_pool, ptr, size_bytes)\n                    return array\n            elif tensor_type == "image_features" and self.image_feature_pool:\n                ptr, actual_size = self.allocate(size_bytes, "tensor_data")\n                if ptr:\n                    array = np.zeros(shape, dtype=dtype)\n                    with self.tensor_allocation_lock:\n                        self.tensor_allocation_map[id(array)] = (self.image_feature_pool, ptr, size_bytes)\n                    return array\n            elif tensor_type == "text_embeddings" and self.text_embedding_pool:\n                ptr, actual_size = self.allocate(size_bytes, "tensor_data")\n                if ptr:\n                    array = np.zeros(shape, dtype=dtype)\n                    with self.tensor_allocation_lock:\n                        self.tensor_allocation_map[id(array)] = (self.text_embedding_pool, ptr, size_bytes)\n                    return array\n\n            # Fallback to standard allocation\n            array = np.zeros(shape, dtype=dtype)\n            if self.cache_manager:\n                array = self.cache_manager.optimize_memory_layout(array)\n            return array\n        except ValueError as e:\n            logger.error(f"ValueError in allocate_tensor_memory: {e}")\n            raise\n        except MemoryError as e:\n            logger.error(f"MemoryError in allocate_tensor_memory: {e}")\n            return None\n        except Exception as e:\n            logger.error(f"Unexpected error in allocate_tensor_memory: {e}")\n            return None\n\n    def free_tensor_memory(self, tensor: np.ndarray, tensor_type: str = "general"):\n        """Free tensor memory back to appropriate pool"""\n        # Validate inputs\n        if not isinstance(tensor, np.ndarray):\n            logger.warning(f"tensor must be a numpy array, got {type(tensor)}")\n            return\n\n        if not isinstance(tensor_type, str):\n            logger.warning(f"tensor_type must be a string, got {type(tensor_type)}")\n            return\n\n        if not self.memory_pool:\n            return  # Standard GC will handle it\n\n        # Use the stored reference to deallocate from the appropriate pool\n        tensor_id = id(tensor)\n        try:\n            with self.tensor_allocation_lock:\n                if tensor_id in self.tensor_allocation_map:\n                    mem_pool, ptr, size = self.tensor_allocation_map[tensor_id]\n                    if mem_pool and hasattr(mem_pool, 'deallocate'):\n                        mem_pool.deallocate(ptr)\n                    del self.tensor_allocation_map[tensor_id]\n        except KeyError:\n            # The tensor was not in the map, which is fine\n            pass\n        except Exception as e:\n            logger.error(f"Error freeing tensor memory: {e}")\n\n    def optimize_image_processing_memory(self, image_batch: np.ndarray) -> np.ndarray:\n        """\n        Optimize memory for image processing pipeline\n\n        Args:\n            image_batch: Batch of images as numpy array\n\n        Returns:\n            Memory-optimized image batch\n        """\n        if self.cache_manager:\n            # Optimize for cache-friendly access patterns\n            optimized_batch = self.cache_manager.optimize_memory_layout(\n                image_batch, layout_type="cache_friendly"\n            )\n        else:\n            optimized_batch = image_batch\n\n        # Prefetch next batch if possible\n        if self.cache_manager and hasattr(image_batch, '__array_interface__'):\n            ptr = image_batch.__array_interface__['data'][0]\n            size = image_batch.nbytes\n            self.cache_manager.prefetch_data(ptr, size)\n\n        return optimized_batch\n\n    def optimize_attention_memory(self, batch_size: int, seq_len: int, hidden_dim: int,\n                                 num_heads: int) -> Dict[str, Any]:\n        """\n        Optimize memory for attention mechanism\n\n        Args:\n            batch_size: Batch size\n            seq_len: Sequence length\n            hidden_dim: Hidden dimension\n            num_heads: Number of attention heads\n\n        Returns:\n            Dictionary with optimized memory allocations for Q, K, V, and attention scores\n        """\n        head_dim = hidden_dim // num_heads\n\n        # Calculate sizes for Q, K, V matrices\n        qkv_size = (batch_size, seq_len, hidden_dim)\n\n        # Allocate with specialized KV cache pool\n        q = self.allocate_tensor_memory(qkv_size, dtype=np.float32, tensor_type="kv_cache")\n        k = self.allocate_tensor_memory(qkv_size, dtype=np.float32, tensor_type="kv_cache")\n        v = self.allocate_tensor_memory(qkv_size, dtype=np.float32, tensor_type="kv_cache")\n\n        # Attention scores matrix\n        attn_scores_size = (batch_size, num_heads, seq_len, seq_len)\n        attn_scores = self.allocate_tensor_memory(attn_scores_size, dtype=np.float32,\n                                                 tensor_type="kv_cache")\n\n        return {\n            'query': q,\n            'key': k,\n            'value': v,\n            'attention_scores': attn_scores,\n            'head_dim': head_dim\n        }\n\n    def optimize_tensor_placement(self, tensor: Any, target_device: str = "auto") -> Any:\n        """\n        Optimize tensor placement between CPU and GPU based on memory constraints.\n        Wrapper for the GPU optimizer's method.\n\n        Args:\n            tensor: Input tensor\n            target_device: Target device ('cpu', 'cuda', 'auto')\n\n        Returns:\n            Tensor placed optimally\n        """\n        if self.gpu_optimizer:\n            return self.gpu_optimizer.optimize_tensor_placement(tensor, target_device)\n        else:\n            # If GPU optimizer is not enabled, return tensor as is\n            return tensor\n\n    def cleanup(self):\n        """Clean up all memory pools"""\n        if self.memory_pool:\n            self.memory_pool.cleanup()\n        if self.kv_cache_pool:\n            self.kv_cache_pool.cleanup()\n        if self.image_feature_pool:\n            self.image_feature_pool.cleanup()\n        if self.text_embedding_pool:\n            self.text_embedding_pool.cleanup()\n\n\ndef create_memory_manager_from_config(config: Qwen3VLConfig) -> VisionLanguageMemoryManager:\n    """\n    Factory function to create a memory manager from configuration.\n    \n    Args:\n        config: Qwen3VLConfig instance\n        \n    Returns:\n        VisionLanguageMemoryManager instance\n    """\n    return VisionLanguageMemoryManager(config)\n\n\ndef get_memory_manager(config: Qwen3VLConfig = None) -> VisionLanguageMemoryManager:\n    """\n    Get the memory manager instance.\n\n    Args:\n        config: Qwen3VLConfig instance (if None, uses default config)\n\n    Returns:\n        VisionLanguageMemoryManager instance\n    """\n    if config is None:\n        config = Qwen3VLConfig()\n    return create_memory_manager_from_config(config)