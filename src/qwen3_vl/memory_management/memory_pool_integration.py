"""Integration utilities for Qwen3-VL memory pooling system."""\n\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Tuple, Dict, Any, Union\nfrom memory_management.tensor_memory_pool import OptimizedMemoryPoolManager, create_memory_pool_manager\nfrom memory_management.specialized_tensor_pools import SpecializedPoolManager\nfrom memory_management.cache_alignment import IntelCacheAlignedPoolManager, create_intel_optimized_pool_manager\nimport logging\n\n\nclass MemoryPoolIntegration:\n    """\n    Integration utilities to connect the custom memory pooling system with existing Qwen3-VL code.\n    Provides wrappers and adapters to seamlessly integrate memory pooling into the existing architecture.\n    """\n    \n    def __init__(self, config):\n        """\n        Initialize memory pool integration.\n        \n        Args:\n            config: Configuration object containing memory pool parameters\n        """\n        self.config = config\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize memory pool managers\n        self.memory_pool_manager = create_memory_pool_manager(config)\n        self.specialized_pool_manager = SpecializedPoolManager(\n            base_capacity=getattr(config, 'memory_pool_base_capacity', 2 * 1024 * 1024 * 1024),\n            dtype=getattr(config, 'memory_pool_dtype', torch.float16),\n            device=getattr(config, 'memory_pool_device', None)\n        )\n        self.intel_optimized_manager = create_intel_optimized_pool_manager(config)\n        \n        # Track tensors allocated through the pool\n        self.allocated_tensors = {}\n        \n        self.logger.info("Memory pool integration initialized")\n\n    def get_attention_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """\n        Get an attention tensor from the specialized pool.\n        \n        Args:\n            shape: Shape of the attention tensor\n            dtype: Data type of the tensor (optional)\n            \n        Returns:\n            Attention tensor from the pool\n        """\n        dtype = dtype or getattr(self.config, 'memory_pool_dtype', torch.float16)\n        \n        # Try to get from specialized pool first\n        try:\n            tensor = self.specialized_pool_manager.get_tensor(shape, 'attention', dtype)\n        except:\n            # Fallback to general pool\n            tensor = self.memory_pool_manager.get_tensor(shape, 'attention', dtype)\n        \n        # Track tensor for proper cleanup\n        tensor_id = id(tensor)\n        self.allocated_tensors[tensor_id] = {\n            'tensor': tensor,\n            'pool_type': 'attention',\n            'shape': shape,\n            'allocation_time': torch.cuda.Event() if torch.cuda.is_available() else None\n        }\n        \n        if torch.cuda.is_available() and self.allocated_tensors[tensor_id]['allocation_time']:\n            self.allocated_tensors[tensor_id]['allocation_time'].record()\n        \n        return tensor\n\n    def get_kv_cache_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """\n        Get a KV cache tensor from the specialized pool.\n        \n        Args:\n            shape: Shape of the KV cache tensor\n            dtype: Data type of the tensor (optional)\n            \n        Returns:\n            KV cache tensor from the pool\n        """\n        dtype = dtype or getattr(self.config, 'memory_pool_dtype', torch.float16)\n        \n        # Try to get from specialized pool first\n        try:\n            tensor = self.specialized_pool_manager.get_tensor(shape, 'kv_cache', dtype)\n        except:\n            # Fallback to general pool\n            tensor = self.memory_pool_manager.get_tensor(shape, 'kv_cache', dtype)\n        \n        # Track tensor for proper cleanup\n        tensor_id = id(tensor)\n        self.allocated_tensors[tensor_id] = {\n            'tensor': tensor,\n            'pool_type': 'kv_cache',\n            'shape': shape,\n            'allocation_time': torch.cuda.Event() if torch.cuda.is_available() else None\n        }\n        \n        if torch.cuda.is_available() and self.allocated_tensors[tensor_id]['allocation_time']:\n            self.allocated_tensors[tensor_id]['allocation_time'].record()\n        \n        return tensor\n\n    def get_image_embedding_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """\n        Get an image embedding tensor from the specialized pool.\n        \n        Args:\n            shape: Shape of the image embedding tensor\n            dtype: Data type of the tensor (optional)\n            \n        Returns:\n            Image embedding tensor from the pool\n        """\n        dtype = dtype or getattr(self.config, 'memory_pool_dtype', torch.float16)\n        \n        # Try to get from specialized pool first\n        try:\n            tensor = self.specialized_pool_manager.get_tensor(shape, 'image_embeddings', dtype)\n        except:\n            # Fallback to general pool\n            tensor = self.memory_pool_manager.get_tensor(shape, 'image_embeddings', dtype)\n        \n        # Track tensor for proper cleanup\n        tensor_id = id(tensor)\n        self.allocated_tensors[tensor_id] = {\n            'tensor': tensor,\n            'pool_type': 'image_embeddings',\n            'shape': shape,\n            'allocation_time': torch.cuda.Event() if torch.cuda.is_available() else None\n        }\n        \n        if torch.cuda.is_available() and self.allocated_tensors[tensor_id]['allocation_time']:\n            self.allocated_tensors[tensor_id]['allocation_time'].record()\n        \n        return tensor\n\n    def get_text_embedding_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """\n        Get a text embedding tensor from the specialized pool.\n        \n        Args:\n            shape: Shape of the text embedding tensor\n            dtype: Data type of the tensor (optional)\n            \n        Returns:\n            Text embedding tensor from the pool\n        """\n        dtype = dtype or getattr(self.config, 'memory_pool_dtype', torch.float16)\n        \n        # Try to get from specialized pool first\n        try:\n            tensor = self.specialized_pool_manager.get_tensor(shape, 'text_embeddings', dtype)\n        except:\n            # Fallback to general pool\n            tensor = self.memory_pool_manager.get_tensor(shape, 'text_embeddings', dtype)\n        \n        # Track tensor for proper cleanup\n        tensor_id = id(tensor)\n        self.allocated_tensors[tensor_id] = {\n            'tensor': tensor,\n            'pool_type': 'text_embeddings',\n            'shape': shape,\n            'allocation_time': torch.cuda.Event() if torch.cuda.is_available() else None\n        }\n        \n        if torch.cuda.is_available() and self.allocated_tensors[tensor_id]['allocation_time']:\n            self.allocated_tensors[tensor_id]['allocation_time'].record()\n        \n        return tensor\n\n    def get_intermediate_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None) -> torch.Tensor:\n        """\n        Get an intermediate activation tensor from the specialized pool.\n        \n        Args:\n            shape: Shape of the intermediate tensor\n            dtype: Data type of the tensor (optional)\n            \n        Returns:\n            Intermediate tensor from the pool\n        """\n        dtype = dtype or getattr(self.config, 'memory_pool_dtype', torch.float16)\n        \n        # Try to get from specialized pool first\n        try:\n            tensor = self.specialized_pool_manager.get_tensor(shape, 'intermediate', dtype)\n        except:\n            # Fallback to general pool\n            tensor = self.memory_pool_manager.get_tensor(shape, 'intermediate', dtype)\n        \n        # Track tensor for proper cleanup\n        tensor_id = id(tensor)\n        self.allocated_tensors[tensor_id] = {\n            'tensor': tensor,\n            'pool_type': 'intermediate',\n            'shape': shape,\n            'allocation_time': torch.cuda.Event() if torch.cuda.is_available() else None\n        }\n        \n        if torch.cuda.is_available() and self.allocated_tensors[tensor_id]['allocation_time']:\n            self.allocated_tensors[tensor_id]['allocation_time'].record()\n        \n        return tensor\n\n    def return_tensor(self, tensor: torch.Tensor, tensor_type: Optional[str] = None):\n        """\n        Return a tensor to the appropriate pool.\n        \n        Args:\n            tensor: Tensor to return to pool\n            tensor_type: Type of tensor (optional, will be inferred if not provided)\n        """\n        tensor_id = id(tensor)\n        \n        if tensor_id in self.allocated_tensors:\n            if tensor_type is None:\n                tensor_type = self.allocated_tensors[tensor_id]['pool_type']\n            \n            # Return to appropriate pool\n            if tensor_type in ['attention', 'kv_cache', 'image_embeddings', 'text_embeddings', 'intermediate']:\n                try:\n                    self.specialized_pool_manager.return_tensor(tensor, tensor_type)\n                except:\n                    self.memory_pool_manager.return_tensor(tensor, tensor_type)\n            else:\n                self.memory_pool_manager.return_tensor(tensor, tensor_type)\n            \n            # Remove from tracking\n            del self.allocated_tensors[tensor_id]\n        else:\n            # If not tracked, return to general pool\n            self.memory_pool_manager.return_tensor(tensor, tensor_type or 'general')\n\n    def wrap_model_for_memory_pooling(self, model: nn.Module) -> nn.Module:\n        """\n        Wrap a model to use memory pooling for tensor allocations.\n        \n        Args:\n            model: PyTorch model to wrap\n            \n        Returns:\n            Wrapped model with memory pooling integration\n        """\n        # This is a simplified implementation - in practice, you might need to \n        # override specific model methods or use hooks\n        \n        # Add memory pool reference to the model\n        model.memory_pool_integration = self\n        \n        # Wrap forward method to use pooled tensors where appropriate\n        original_forward = model.forward\n        \n        def pooled_forward(*args, **kwargs):\n            # This is a simplified wrapper - in a real implementation, you'd\n            # need to identify where tensors are allocated in the model\n            result = original_forward(*args, **kwargs)\n            return result\n        \n        model.forward = pooled_forward\n        return model\n\n    def integrate_with_attention_mechanism(self, attention_module: nn.Module) -> nn.Module:\n        """\n        Integrate memory pooling with attention mechanisms.\n        \n        Args:\n            attention_module: Attention module to integrate\n            \n        Returns:\n            Integrated attention module\n        """\n        # Store reference to memory pool\n        attention_module.memory_pool_integration = self\n        \n        # Wrap the attention computation to use pooled tensors\n        original_forward = attention_module.forward\n        \n        def pooled_attention_forward(*args, **kwargs):\n            # In a real implementation, this would intercept tensor allocations\n            # and replace them with pooled tensors\n            return original_forward(*args, **kwargs)\n        \n        attention_module.forward = pooled_attention_forward\n        return attention_module\n\n    def integrate_with_kv_cache(self, kv_cache_module: nn.Module) -> nn.Module:\n        """\n        Integrate memory pooling with KV cache mechanisms.\n        \n        Args:\n            kv_cache_module: KV cache module to integrate\n            \n        Returns:\n            Integrated KV cache module\n        """\n        # Store reference to memory pool\n        kv_cache_module.memory_pool_integration = self\n        \n        # Add methods to use pooled tensors\n        def get_pooled_cache_tensor(self, shape: Tuple[int, ...], dtype: Optional[torch.dtype] = None):\n            return self.memory_pool_integration.get_kv_cache_tensor(shape, dtype)\n        \n        def return_pooled_cache_tensor(self, tensor: torch.Tensor):\n            self.memory_pool_integration.return_tensor(tensor, 'kv_cache')\n        \n        kv_cache_module.get_pooled_cache_tensor = get_pooled_cache_tensor.__get__(kv_cache_module)\n        kv_cache_module.return_pooled_cache_tensor = return_pooled_cache_tensor.__get__(kv_cache_module)\n        \n        return kv_cache_module\n\n    def get_memory_efficiency_stats(self) -> Dict[str, Any]:\n        """\n        Get comprehensive memory efficiency statistics.\n        \n        Returns:\n            Dictionary with memory efficiency statistics\n        """\n        return {\n            'general_pool_stats': self.memory_pool_manager.get_pool_stats(),\n            'specialized_pool_stats': self.specialized_pool_manager.get_pool_stats(),\n            'intel_optimized_stats': self.intel_optimized_manager.get_stats(),\n            'tracked_tensors': len(self.allocated_tensors),\n            'integration_status': 'active'\n        }\n\n    def cleanup(self):\n        """Clean up all tracked tensors and return them to pools."""\n        for tensor_id, tensor_info in list(self.allocated_tensors.items()):\n            tensor = tensor_info['tensor']\n            tensor_type = tensor_info['pool_type']\n            self.return_tensor(tensor, tensor_type)\n        \n        # Clear tracking\n        self.allocated_tensors.clear()\n\n\nclass Qwen3VLMemoryPoolAdapter:\n    """\n    Adapter class to integrate memory pooling with existing Qwen3-VL components.\n    """\n    \n    def __init__(self, config):\n        self.integration = MemoryPoolIntegration(config)\n        \n    def allocate_attention_weights(self, batch_size: int, num_heads: int, \n                                  seq_len: int, head_dim: int) -> torch.Tensor:\n        """Allocate attention weights tensor using memory pooling."""\n        shape = (batch_size, num_heads, seq_len, seq_len if seq_len <= 2048 else 2048)  # Limit for pooling\n        return self.integration.get_attention_tensor(shape)\n    \n    def allocate_kv_cache(self, batch_size: int, num_heads: int, \n                         seq_len: int, head_dim: int, is_key: bool = True) -> torch.Tensor:\n        """Allocate KV cache tensor using memory pooling."""\n        shape = (batch_size, num_heads, seq_len, head_dim)\n        return self.integration.get_kv_cache_tensor(shape)\n    \n    def allocate_image_features(self, batch_size: int, num_patches: int, \n                               feature_dim: int) -> torch.Tensor:\n        """Allocate image feature tensor using memory pooling."""\n        shape = (batch_size, num_patches, feature_dim)\n        return self.integration.get_image_embedding_tensor(shape)\n    \n    def allocate_text_embeddings(self, batch_size: int, seq_len: int, \n                                embed_dim: int) -> torch.Tensor:\n        """Allocate text embedding tensor using memory pooling."""\n        shape = (batch_size, seq_len, embed_dim)\n        return self.integration.get_text_embedding_tensor(shape)\n    \n    def allocate_intermediate_states(self, batch_size: int, seq_len: int, \n                                    intermediate_dim: int) -> torch.Tensor:\n        """Allocate intermediate state tensor using memory pooling."""\n        shape = (batch_size, seq_len, intermediate_dim)\n        return self.integration.get_intermediate_tensor(shape)\n    \n    def get_pool_statistics(self) -> Dict[str, Any]:\n        """Get memory pool statistics."""\n        return self.integration.get_memory_efficiency_stats()\n\n\ndef create_memory_pool_adapter(config) -> Qwen3VLMemoryPoolAdapter:\n    """\n    Factory function to create a memory pool adapter.\n    \n    Args:\n        config: Configuration object with memory pool parameters\n        \n    Returns:\n        Qwen3VLMemoryPoolAdapter instance\n    """\n    return Qwen3VLMemoryPoolAdapter(config)\n\n\n# Backward compatibility wrapper for existing code\nclass LegacyMemoryPoolAdapter:\n    """\n    Legacy adapter to maintain compatibility with existing resource management code.\n    """\n    \n    def __init__(self, config):\n        self.adapter = create_memory_pool_adapter(config)\n        \n    def allocate_tensor_memory(self, shape: Tuple[int, ...], \n                              dtype: torch.dtype = torch.float16, \n                              tensor_type: str = "general") -> torch.Tensor:\n        """\n        Legacy method to allocate tensor memory with pooling.\n        """\n        if tensor_type == "attention_weights":\n            # Use attention pool for attention-related tensors\n            return self.adapter.allocate_attention_weights(\n                shape[0], shape[1], shape[2], shape[3] if len(shape) > 3 else 64\n            )\n        elif tensor_type == "kv_cache":\n            # Use KV cache pool\n            batch_size = shape[0] if len(shape) > 0 else 1\n            num_heads = shape[1] if len(shape) > 1 else 8\n            seq_len = shape[2] if len(shape) > 2 else 512\n            head_dim = shape[3] if len(shape) > 3 else 64\n            return self.adapter.allocate_kv_cache(batch_size, num_heads, seq_len, head_dim)\n        elif tensor_type == "image_features":\n            # Use image embeddings pool\n            batch_size = shape[0] if len(shape) > 0 else 1\n            num_patches = shape[1] if len(shape) > 1 else 576\n            feature_dim = shape[2] if len(shape) > 2 else 1152\n            return self.adapter.allocate_image_features(batch_size, num_patches, feature_dim)\n        elif tensor_type == "text_embeddings":\n            # Use text embeddings pool\n            batch_size = shape[0] if len(shape) > 0 else 1\n            seq_len = shape[1] if len(shape) > 1 else 512\n            embed_dim = shape[2] if len(shape) > 2 else 4096\n            return self.adapter.allocate_text_embeddings(batch_size, seq_len, embed_dim)\n        elif tensor_type == "mlp_intermediate":\n            # Use intermediate pool\n            batch_size = shape[0] if len(shape) > 0 else 1\n            seq_len = shape[1] if len(shape) > 1 else 512\n            intermediate_dim = shape[2] if len(shape) > 2 else 11008\n            return self.adapter.allocate_intermediate_states(batch_size, seq_len, intermediate_dim)\n        else:\n            # Use appropriate pool based on shape characteristics\n            if len(shape) >= 4 and shape[1] >= 8:  # Likely attention\n                return self.adapter.allocate_attention_weights(\n                    shape[0], shape[1], shape[2], shape[3] if len(shape) > 3 else 64\n                )\n            elif len(shape) >= 3 and shape[2] >= 512:  # Likely cache or embedding\n                if shape[2] <= 2048:  # KV cache size\n                    return self.adapter.allocate_kv_cache(\n                        shape[0], shape[1], shape[2], shape[3] if len(shape) > 3 else 64\n                    )\n                else:  # Embedding\n                    return self.adapter.allocate_text_embeddings(\n                        shape[0], shape[1], shape[2]\n                    )\n            else:\n                # Default to general allocation\n                # For now, return a regular tensor, but in a full implementation \n                # we'd have a general pool\n                return torch.empty(shape, dtype=dtype, device=self.adapter.adapter.integration.memory_pool_manager.device)\n\n\n# Example usage and integration test\nif __name__ == "__main__":\n    print("Testing Memory Pool Integration...")\n    \n    # Create mock config\n    class MockConfig:\n        def __init__(self):\n            self.memory_pool_base_capacity = 1024 * 1024 * 512  # 512MB\n            self.memory_pool_dtype = torch.float16\n            self.memory_pool_device = 'cpu'\n            self.memory_pool_cache_line_size = 64\n            self.memory_pool_l3_cache_size = 6 * 1024 * 1024  # 6MB\n    \n    config = MockConfig()\n    \n    # Test the integration\n    adapter = create_memory_pool_adapter(config)\n    \n    print("\n1. Testing attention tensor allocation...")\n    attn_tensor = adapter.allocate_attention_weights(2, 16, 512, 64)\n    print(f"Allocated attention tensor: {attn_tensor.shape}, {attn_tensor.dtype}")\n    \n    print("\n2. Testing KV cache tensor allocation...")\n    kv_tensor = adapter.allocate_kv_cache(1, 32, 1024, 128)\n    print(f"Allocated KV cache tensor: {kv_tensor.shape}, {kv_tensor.dtype}")\n    \n    print("\n3. Testing image features tensor allocation...")\n    img_tensor = adapter.allocate_image_features(1, 576, 1152)\n    print(f"Allocated image features tensor: {img_tensor.shape}, {img_tensor.dtype}")\n    \n    print("\n4. Testing text embeddings tensor allocation...")\n    text_tensor = adapter.allocate_text_embeddings(1, 512, 4096)\n    print(f"Allocated text embeddings tensor: {text_tensor.shape}, {text_tensor.dtype}")\n    \n    print("\n5. Testing intermediate states tensor allocation...")\n    int_tensor = adapter.allocate_intermediate_states(1, 512, 11008)\n    print(f"Allocated intermediate states tensor: {int_tensor.shape}, {int_tensor.dtype}")\n    \n    # Get statistics\n    print("\n6. Memory pool statistics:")\n    stats = adapter.get_pool_statistics()\n    print(f"  Tracked tensors: {stats['tracked_tensors']}")\n    print(f"  General pool utilization: {stats['general_pool_stats']['aggregate']['total_utilization_percent']:.2f}%")\n    print(f"  Specialized pool utilization: {stats['specialized_pool_stats']['aggregate']['total_utilization_percent']:.2f}%")\n    \n    # Test legacy adapter\n    print("\n7. Testing legacy adapter compatibility...")\n    legacy_adapter = LegacyMemoryPoolAdapter(config)\n    \n    legacy_tensor = legacy_adapter.allocate_tensor_memory((8, 1024, 1024), tensor_type="attention_weights")\n    print(f"Allocated via legacy adapter: {legacy_tensor.shape}, {legacy_tensor.dtype}")\n    \n    print("\nMemory pool integration test completed successfully!")