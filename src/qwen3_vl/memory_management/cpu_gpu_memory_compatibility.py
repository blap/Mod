"""\nCPU-GPU Memory Management Compatibility System\nOptimized for Intel i5-10210U + NVIDIA SM61 + NVMe SSD Hardware Configuration\nImplements Phase 2.9: Memory Pooling and Pre-allocation Techniques - CPU/GPU Memory Compatibility\n"""\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional, Any, Union, Callable\nfrom collections import defaultdict, OrderedDict\nimport threading\nimport time\nimport gc\nimport psutil\nfrom dataclasses import dataclass\nimport logging\nimport queue\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n\n@dataclass\nclass MemoryCompatibilityConfig:\n    """\n    Configuration for CPU-GPU memory compatibility optimizations.\n    """\n    # Memory management settings\n    use_unified_memory: bool = False  # Use unified memory (not available on all hardware)\n    enable_pinned_memory: bool = True  # Use pinned memory for faster CPU-GPU transfers\n    enable_memory_pooling: bool = True  # Use memory pooling for both CPU and GPU\n    memory_pool_size: int = 2**30  # 1GB default for memory pool\n    \n    # Hardware-specific settings\n    hardware_compute_capability: Tuple[int, int] = (6, 1)  # SM61\n    cpu_memory_bandwidth_gb_s: float = 25.6  # Estimated for Intel i5-10210U with DDR4-2666\n    gpu_memory_bandwidth_gb_s: float = 192.0  # Estimated for GTX 1080 Ti\n    nvme_ssd_available: bool = True  # NVMe SSD available for memory overflow\n    \n    # Transfer optimization settings\n    use_async_transfers: bool = True  # Use asynchronous memory transfers\n    transfer_overlap_compute: bool = True  # Overlap transfers with computation\n    memory_copy_chunk_size: int = 2**20  # 1MB chunks for large transfers\n    \n    # Memory allocation settings\n    prefer_cpu_for_small_tensors: bool = True  # Keep small tensors on CPU to save GPU memory\n    tensor_size_threshold_cpu_gpu: int = 1024 * 1024  # 1MB threshold\n\n\nclass UnifiedMemoryManager:\n    """\n    Unified memory management system that seamlessly handles both CPU and GPU memory\n    with optimized allocation and transfer strategies.\n    """\n    \n    def __init__(self, config: MemoryCompatibilityConfig = None):\n        self.config = config or MemoryCompatibilityConfig()\n        self._lock = threading.Lock()\n        \n        # Initialize memory pools for CPU and GPU\n        self.cpu_memory_pool = self._create_cpu_memory_pool()\n        self.gpu_memory_pool = self._create_gpu_memory_pool() if torch.cuda.is_available() else None\n        \n        # Memory transfer optimization\n        self.transfer_executor = ThreadPoolExecutor(max_workers=2)  # For async transfers\n        self.pinned_memory_pool = self._create_pinned_memory_pool() if self.config.enable_pinned_memory else None\n        \n        # Track tensor placement decisions\n        self.tensor_placement_history = defaultdict(list)\n        self.placement_stats = {\n            'cpu_allocations': 0,\n            'gpu_allocations': 0,\n            'transfers_initiated': 0,\n            'async_transfers': 0,\n            'pinned_memory_used': 0\n        }\n        \n        # Memory usage tracking\n        self.cpu_memory_usage = 0\n        self.gpu_memory_usage = 0\n        self.max_cpu_memory_usage = 0\n        self.max_gpu_memory_usage = 0\n        \n        # Async transfer queues\n        self.async_transfer_queue = queue.Queue()\n        self.transfer_active = True\n        self.transfer_thread = threading.Thread(target=self._process_async_transfers, daemon=True)\n        self.transfer_thread.start()\n    \n    def _create_cpu_memory_pool(self):\n        """\n        Create a memory pool for CPU allocations.\n        """\n        # For CPU, we'll use a simple tensor cache approach\n        # since CPU memory allocation is generally fast\n        return TensorCache(max_cache_size=50, max_cache_size_per_key=10)\n    \n    def _create_gpu_memory_pool(self):\n        """\n        Create a memory pool for GPU allocations using the existing implementation.\n        """\n        try:\nfrom memory_management.memory_pool import MemoryPool\n            return MemoryPool(initial_size=self.config.memory_pool_size)\n        except ImportError:\n            # Fallback if memory pool module not available\n            return None\n    \n    def _create_pinned_memory_pool(self):\n        """\n        Create a pinned memory pool for efficient CPU-GPU transfers.\n        """\n        try:\n            # For pinned memory, we'll maintain a cache of pinned tensors\n            return TensorCache(max_cache_size=20, max_cache_size_per_key=5)\n        except:\n            return None\n    \n    def allocate_tensor(self, shape: Tuple[int, ...], dtype: torch.dtype = torch.float32,\n                       preferred_device: str = "auto") -> torch.Tensor:\n        """\n        Allocate a tensor with optimized placement based on size and hardware characteristics.\n        """\n        with self._lock:\n            # Determine optimal device based on tensor size and config\n            optimal_device = self._determine_optimal_device(shape, dtype, preferred_device)\n            \n            if optimal_device == "cpu":\n                tensor = self._allocate_cpu_tensor(shape, dtype)\n                self.placement_stats['cpu_allocations'] += 1\n                self._update_cpu_memory_usage(tensor.element_size() * tensor.numel(), "allocate")\n            elif optimal_device == "gpu" and self.gpu_memory_pool:\n                tensor = self.gpu_memory_pool.allocate_tensor(shape, dtype, torch.device("cuda"))\n                self.placement_stats['gpu_allocations'] += 1\n                self._update_gpu_memory_usage(tensor.element_size() * tensor.numel(), "allocate")\n            else:\n                # Fallback to standard allocation\n                device = torch.device("cuda" if optimal_device == "gpu" and torch.cuda.is_available() else "cpu")\n                tensor = torch.empty(shape, dtype=dtype, device=device)\n                if device.type == "cuda":\n                    self.placement_stats['gpu_allocations'] += 1\n                    self._update_gpu_memory_usage(tensor.element_size() * tensor.numel(), "allocate")\n                else:\n                    self.placement_stats['cpu_allocations'] += 1\n                    self._update_cpu_memory_usage(tensor.element_size() * tensor.numel(), "allocate")\n            \n            # Track placement decision\n            tensor_size = np.prod(shape) * torch.tensor([], dtype=dtype).element_size()\n            self.tensor_placement_history[optimal_device].append({\n                'shape': shape,\n                'size_bytes': tensor_size,\n                'timestamp': time.time()\n            })\n            \n            return tensor\n    \n    def _determine_optimal_device(self, shape: Tuple[int, ...], dtype: torch.dtype, \n                                 preferred_device: str) -> str:\n        """\n        Determine the optimal device for tensor allocation based on size and hardware.\n        """\n        if preferred_device != "auto":\n            return preferred_device\n        \n        # Calculate tensor size\n        tensor_size = np.prod(shape) * torch.tensor([], dtype=dtype).element_size()\n        \n        # For small tensors, prefer CPU to save GPU memory if configured\n        if (self.config.prefer_cpu_for_small_tensors and \n            tensor_size < self.config.tensor_size_threshold_cpu_gpu and\n            torch.cuda.is_available()):\n            return "cpu"\n        \n        # For larger tensors, prefer GPU for computation if available\n        if torch.cuda.is_available():\n            # Check current GPU memory pressure\n            gpu_memory_allocated = torch.cuda.memory_allocated()\n            gpu_memory_reserved = torch.cuda.memory_reserved()\n            gpu_memory_capacity = torch.cuda.get_device_properties(0).total_memory\n            \n            # If GPU memory is under pressure, consider keeping on CPU for some tensors\n            memory_pressure = gpu_memory_allocated / gpu_memory_capacity\n            if memory_pressure > 0.8 and tensor_size < 10 * 1024 * 1024:  # Less than 10MB\n                return "cpu"\n            else:\n                return "gpu"\n        else:\n            return "cpu"\n    \n    def _allocate_cpu_tensor(self, shape: Tuple[int, ...], dtype: torch.dtype) -> torch.Tensor:\n        """\n        Allocate a tensor on CPU with potential pinned memory optimization.\n        """\n        if (self.config.enable_pinned_memory and \n            self.pinned_memory_pool and \n            np.prod(shape) * torch.tensor([], dtype=dtype).element_size() > 64 * 1024):  # Larger than 64KB\n            # Try to get from pinned memory pool\n            try:\n                tensor = self.pinned_memory_pool.get_tensor(shape, dtype, torch.device("cpu"))\n                if tensor.is_pinned():\n                    self.placement_stats['pinned_memory_used'] += 1\n                    return tensor\n            except:\n                pass  # Fall back to regular allocation\n        \n        # Regular CPU allocation\n        return torch.empty(shape, dtype=dtype, device="cpu")\n    \n    def transfer_tensor(self, tensor: torch.Tensor, target_device: Union[str, torch.device],\n                       async_transfer: bool = None) -> torch.Tensor:\n        """\n        Transfer a tensor between CPU and GPU with optimization.\n        """\n        target_device = torch.device(target_device) if isinstance(target_device, str) else target_device\n        \n        if tensor.device == target_device:\n            return tensor  # Already on target device\n        \n        with self._lock:\n            self.placement_stats['transfers_initiated'] += 1\n            \n            # Determine if async transfer should be used\n            if async_transfer is None:\n                async_transfer = (self.config.use_async_transfers and \n                                tensor.numel() * tensor.element_size() > 1024 * 1024)  # Larger than 1MB\n            \n            if async_transfer:\n                self.placement_stats['async_transfers'] += 1\n                # For async transfer, we'll put it in the queue to be processed\n                future = self.transfer_executor.submit(\n                    lambda: tensor.to(target_device, non_blocking=True)\n                )\n                return future.result()  # In real implementation, this would be handled differently\n            else:\n                # Synchronous transfer\n                transferred_tensor = tensor.to(target_device, non_blocking=self.config.enable_pinned_memory)\n                return transferred_tensor\n    \n    def _process_async_transfers(self):\n        """\n        Process async transfers in a separate thread.\n        """\n        while self.transfer_active:\n            try:\n                transfer_request = self.async_transfer_queue.get(timeout=1.0)\n                # Process the transfer\n                tensor, target_device = transfer_request\n                transferred = tensor.to(target_device, non_blocking=True)\n                # In a real implementation, we would return the result somewhere\n                self.async_transfer_queue.task_done()\n            except queue.Empty:\n                continue\n            except Exception as e:\n                logging.error(f"Error in async transfer: {e}")\n    \n    def _update_cpu_memory_usage(self, size_bytes: int, operation: str):\n        """\n        Update CPU memory usage statistics.\n        """\n        if operation == "allocate":\n            self.cpu_memory_usage += size_bytes\n            self.max_cpu_memory_usage = max(self.max_cpu_memory_usage, self.cpu_memory_usage)\n        elif operation == "deallocate":\n            self.cpu_memory_usage = max(0, self.cpu_memory_usage - size_bytes)\n    \n    def _update_gpu_memory_usage(self, size_bytes: int, operation: str):\n        """\n        Update GPU memory usage statistics.\n        """\n        if operation == "allocate":\n            self.gpu_memory_usage += size_bytes\n            self.max_gpu_memory_usage = max(self.max_gpu_memory_usage, self.gpu_memory_usage)\n        elif operation == "deallocate":\n            self.gpu_memory_usage = max(0, self.gpu_memory_usage - size_bytes)\n    \n    def get_memory_stats(self) -> Dict[str, Any]:\n        """\n        Get comprehensive memory statistics for both CPU and GPU.\n        """\n        with self._lock:\n            stats = {\n                'placement_stats': self.placement_stats.copy(),\n                'current_memory_usage': {\n                    'cpu_current_mb': self.cpu_memory_usage / (1024 * 1024),\n                    'cpu_peak_mb': self.max_cpu_memory_usage / (1024 * 1024),\n                    'gpu_current_mb': self.gpu_memory_usage / (1024 * 1024),\n                    'gpu_peak_mb': self.max_gpu_memory_usage / (1024 * 1024)\n                },\n                'system_memory': {\n                    'total_system_memory_gb': psutil.virtual_memory().total / (1024**3),\n                    'available_system_memory_gb': psutil.virtual_memory().available / (1024**3),\n                    'system_memory_percent_used': psutil.virtual_memory().percent\n                }\n            }\n            \n            if torch.cuda.is_available():\n                stats['gpu_memory_details'] = {\n                    'allocated_mb': torch.cuda.memory_allocated() / (1024 * 1024),\n                    'reserved_mb': torch.cuda.memory_reserved() / (1024 * 1024),\n                    'max_allocated_mb': torch.cuda.max_memory_allocated() / (1024 * 1024),\n                    'max_reserved_mb': torch.cuda.max_memory_reserved() / (1024 * 1024),\n                    'gpu_total_memory_mb': torch.cuda.get_device_properties(0).total_memory / (1024 * 1024)\n                }\n            \n            return stats\n    \n    def defragment_memory(self):\n        """\n        Defragment both CPU and GPU memory pools.\n        """\n        with self._lock:\n            # Defragment CPU memory pool\n            if hasattr(self.cpu_memory_pool, 'defragment'):\n                self.cpu_memory_pool.defragment()\n            \n            # Defragment GPU memory pool if available\n            if self.gpu_memory_pool and hasattr(self.gpu_memory_pool, 'defragment'):\n                self.gpu_memory_pool.defragment()\n            \n            # Clear PyTorch cache\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n    \n    def shutdown(self):\n        """\n        Shutdown the unified memory manager and clean up resources.\n        """\n        self.transfer_active = False\n        self.transfer_executor.shutdown(wait=True)\n\n\nclass CrossDeviceTensorOptimizer:\n    """\n    Optimizes tensor operations across CPU and GPU devices.\n    """\n    \n    def __init__(self, memory_manager: UnifiedMemoryManager):\n        self.memory_manager = memory_manager\n        self._lock = threading.Lock()\n        \n        # Track cross-device operation patterns\n        self.operation_patterns = defaultdict(lambda: {'count': 0, 'avg_time': 0.0, 'total_time': 0.0})\n        \n        # For hardware-specific optimizations\n        self.warp_size = 32  # Standard for NVIDIA GPUs\n        self.shared_memory_per_block = self._get_shared_memory_per_block()\n    \n    def _get_shared_memory_per_block(self) -> int:\n        """\n        Get shared memory per block based on compute capability.\n        """\n        if self.memory_manager.config.hardware_compute_capability >= (6, 0):\n            return 48 * 1024  # 48KB for SM61\n        else:\n            return 32 * 1024  # Default fallback\n    \n    def optimize_tensor_operation_placement(self, operation_type: str, input_shapes: List[Tuple[int, ...]], \n                                          compute_intensity: float = 1.0) -> str:\n        """\n        Determine optimal device placement for a tensor operation based on compute intensity and memory requirements.\n        """\n        with self._lock:\n            # Calculate total memory requirement\n            total_memory = sum(\n                np.prod(shape) * 4 for shape in input_shapes  # Assuming float32\n            )\n            \n            # For compute-intensive operations, prefer GPU\n            if compute_intensity > 10.0 and total_memory < 100 * 1024 * 1024:  # Less than 100MB\n                if torch.cuda.is_available():\n                    return "gpu"\n            \n            # For memory-intensive operations with large tensors, consider available memory\n            if total_memory > 50 * 1024 * 1024:  # Larger than 50MB\n                if torch.cuda.is_available():\n                    gpu_memory_free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n                    if total_memory < gpu_memory_free * 0.8:  # Use only 80% of free GPU memory\n                        return "gpu"\n                    else:\n                        return "cpu"\n                else:\n                    return "cpu"\n            \n            # For small operations, prefer CPU to save GPU resources\n            if total_memory < 1 * 1024 * 1024:  # Less than 1MB\n                return "cpu"\n            \n            # Default: use GPU if available\n            return "gpu" if torch.cuda.is_available() else "cpu"\n    \n    def execute_cross_device_operation(self, operation_fn: Callable, *inputs: torch.Tensor, \n                                     preferred_device: str = "auto") -> torch.Tensor:\n        """\n        Execute an operation with optimized device placement for inputs and output.\n        """\n        start_time = time.time()\n        \n        # Determine optimal device for this operation\n        input_shapes = [inp.shape for inp in inputs if torch.is_tensor(inp)]\n        compute_intensity = self._estimate_compute_intensity(operation_fn, input_shapes)\n        \n        optimal_device = self.optimize_tensor_operation_placement(\n            operation_fn.__name__ if hasattr(operation_fn, '__name__') else 'unknown',\n            input_shapes,\n            compute_intensity\n        )\n        \n        # Transfer inputs to optimal device\n        transferred_inputs = []\n        for inp in inputs:\n            if torch.is_tensor(inp) and inp.device.type != optimal_device:\n                transferred_inp = self.memory_manager.transfer_tensor(inp, optimal_device)\n                transferred_inputs.append(transferred_inp)\n            else:\n                transferred_inputs.append(inp)\n        \n        # Execute operation on optimal device\n        with torch.device(optimal_device):\n            result = operation_fn(*transferred_inputs)\n        \n        # Update operation pattern statistics\n        operation_key = f"{operation_fn.__name__ if hasattr(operation_fn, '__name__') else 'unknown'}_{optimal_device}"\n        execution_time = time.time() - start_time\n        \n        with self._lock:\n            self.operation_patterns[operation_key]['count'] += 1\n            self.operation_patterns[operation_key]['total_time'] += execution_time\n            count = self.operation_patterns[operation_key]['count']\n            self.operation_patterns[operation_key]['avg_time'] = (\n                self.operation_patterns[operation_key]['total_time'] / count\n            )\n        \n        return result\n    \n    def _estimate_compute_intensity(self, operation_fn: Callable, input_shapes: List[Tuple[int, ...]]) -> float:\n        """\n        Estimate compute intensity (operations per byte) for an operation.\n        """\n        # This is a simplified estimation - in practice, this would be more complex\n        total_elements = sum(np.prod(shape) for shape in input_shapes)\n        \n        if "matmul" in str(operation_fn).lower() or "linear" in str(operation_fn).lower():\n            # Matrix multiplication: O(n^3) operations for n*n matrices\n            if len(input_shapes) >= 2:\n                # Estimate based on the largest dimension\n                max_dim = max(max(shape) for shape in input_shapes if len(shape) >= 2)\n                ops = max_dim ** 3  # Simplified estimation\n                return ops / (total_elements * 4)  # Operations per byte (assuming float32)\n        elif "conv" in str(operation_fn).lower():\n            # Convolution: depends on kernel size, input/output size\n            if len(input_shapes) >= 2:\n                # Simplified estimation for convolution\n                ops = np.prod(input_shapes[0]) * input_shapes[1][0]  # input_size * output_channels\n                return ops / (total_elements * 4)\n        else:\n            # For other operations, assume 1:1 ratio\n            return 1.0\n    \n    def get_optimization_recommendations(self) -> Dict[str, Any]:\n        """\n        Get recommendations for cross-device operation optimization.\n        """\n        with self._lock:\n            # Analyze operation patterns to provide recommendations\n            recommendations = {\n                'operation_patterns_analysis': dict(self.operation_patterns),\n                'preferred_device_for_operations': {},\n                'memory_bandwidth_utilization': {\n                    'cpu_memory_bandwidth_gb_s': self.memory_manager.config.cpu_memory_bandwidth_gb_s,\n                    'gpu_memory_bandwidth_gb_s': self.memory_manager.config.gpu_memory_bandwidth_gb_s,\n                    'estimated_utilization': 0.0  # Would be calculated based on actual usage\n                }\n            }\n            \n            # Identify operations that consistently perform better on specific devices\n            for op_pattern, stats in self.operation_patterns.items():\n                if stats['count'] >= 5:  # Only consider patterns with sufficient samples\n                    op_name, device = op_pattern.rsplit('_', 1)\n                    if op_name not in recommendations['preferred_device_for_operations']:\n                        recommendations['preferred_device_for_operations'][op_name] = {}\n                    recommendations['preferred_device_for_operations'][op_name][device] = stats['avg_time']\n            \n            return recommendations\n\n\nclass MemoryEfficientDataLoader:\n    """\n    Memory-efficient data loader that optimizes CPU-GPU data transfer.\n    """\n    \n    def __init__(self, dataset, memory_manager: UnifiedMemoryManager, \n                 batch_size: int = 1, shuffle: bool = False, \n                 pin_memory: bool = None, num_workers: int = 0,\n                 **kwargs):\n        self.dataset = dataset\n        self.memory_manager = memory_manager\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.pin_memory = pin_memory if pin_memory is not None else memory_manager.config.enable_pinned_memory\n        self.num_workers = num_workers\n        \n        # Create base data loader\n        self.base_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            pin_memory=self.pin_memory,\n            num_workers=num_workers,\n            **kwargs\n        )\n        \n        # Track transfer efficiency\n        self.transfer_stats = {\n            'total_batches': 0,\n            'pinned_memory_batches': 0,\n            'average_transfer_time': 0.0\n        }\n        self._lock = threading.Lock()\n    \n    def __iter__(self):\n        for batch in self.base_loader:\n            start_time = time.time()\n            \n            # Move batch to GPU if available and beneficial\n            if torch.cuda.is_available():\n                batch = self._transfer_batch_optimized(batch)\n            \n            transfer_time = time.time() - start_time\n            \n            with self._lock:\n                self.transfer_stats['total_batches'] += 1\n                if self.pin_memory:\n                    self.transfer_stats['pinned_memory_batches'] += 1\n                # Update average transfer time\n                total_time = self.transfer_stats['average_transfer_time'] * (self.transfer_stats['total_batches'] - 1) + transfer_time\n                self.transfer_stats['average_transfer_time'] = total_time / self.transfer_stats['total_batches']\n            \n            yield batch\n    \n    def _transfer_batch_optimized(self, batch: Any) -> Any:\n        """\n        Transfer a batch to GPU with optimization.\n        """\n        def transfer_tensor_recursive(obj):\n            if torch.is_tensor(obj):\n                # Use memory manager for optimized transfer\n                return self.memory_manager.transfer_tensor(obj, "cuda", async_transfer=True)\n            elif isinstance(obj, dict):\n                return {key: transfer_tensor_recursive(value) for key, value in obj.items()}\n            elif isinstance(obj, list):\n                return [transfer_tensor_recursive(item) for item in obj]\n            elif isinstance(obj, tuple):\n                return tuple(transfer_tensor_recursive(item) for item in obj)\n            else:\n                return obj\n        \n        return transfer_tensor_recursive(batch)\n    \n    def get_transfer_efficiency_stats(self) -> Dict[str, Any]:\n        """\n        Get statistics about data transfer efficiency.\n        """\n        with self._lock:\n            return self.transfer_stats.copy()\n    \n    def __len__(self):\n        return len(self.base_loader)\n\n\nclass HardwareAwareMemoryOptimizer:\n    """\n    Hardware-aware memory optimization for the specific Intel i5-10210U + NVIDIA SM61 configuration.\n    """\n    \n    def __init__(self, memory_manager: UnifiedMemoryManager):\n        self.memory_manager = memory_manager\n        self._lock = threading.Lock()\n        \n        # Hardware-specific parameters\n        self.cpu_core_count = 4  # Intel i5-10210U has 4 cores, 8 threads\n        self.cpu_thread_count = 8\n        self.gpu_compute_units = self._get_gpu_compute_units()\n        self.memory_bus_width = 320  # GTX 1080 Ti has 320-bit bus (assumed similar for SM61)\n        \n        # Cache for hardware-specific optimizations\n        self.optimization_cache = {}\n    \n    def _get_gpu_compute_units(self) -> int:\n        """\n        Get estimated number of compute units based on compute capability.\n        """\n        # For SM61 (GP104/GTX 1080 Ti), there are 20 SMs with 128 CUDA cores each = 2560 cores\n        if self.memory_manager.config.hardware_compute_capability == (6, 1):\n            return 20  # Number of SMs\n        else:\n            return 16  # Default fallback\n    \n    def optimize_tensor_for_hardware(self, tensor: torch.Tensor, operation_type: str = "general") -> torch.Tensor:\n        """\n        Optimize tensor memory layout for the specific hardware architecture.\n        """\n        with self._lock:\n            original_device = tensor.device\n            original_shape = tensor.shape\n            \n            # For SM61 architecture, optimize memory access patterns\n            if original_device.type == "cuda" and operation_type in ["attention", "convolution", "matmul"]:\n                # For operations that benefit from specific memory layouts on GPU\n                \n                if operation_type == "convolution" and len(tensor.shape) == 4:\n                    # For convolution, channels_last format can be more efficient on some GPUs\n                    if self._should_use_channels_last_for_convolution(tensor.shape):\n                        return tensor.to(memory_format=torch.channels_last)\n                \n                elif operation_type == "attention" and len(tensor.shape) >= 3:\n                    # For attention operations, ensure dimensions align with warp size for efficient access\n                    return self._optimize_attention_tensor(tensor)\n                \n                elif operation_type == "matmul" and len(tensor.shape) == 2:\n                    # For matrix operations, consider memory alignment\n                    return self._optimize_matmul_tensor(tensor)\n            \n            # For CPU operations, consider different optimizations\n            elif original_device.type == "cpu":\n                if operation_type == "convolution" and len(tensor.shape) == 4:\n                    # On CPU, channels_first might be more efficient\n                    return tensor.to(memory_format=torch.contiguous_format)\n            \n            # Return original tensor if no specific optimization applies\n            return tensor\n    \n    def _should_use_channels_last_for_convolution(self, shape: Tuple[int, ...]) -> bool:\n        """\n        Determine if channels_last format is beneficial for this convolution shape on SM61.\n        """\n        # For SM61, channels_last can be beneficial for larger spatial dimensions\n        if len(shape) == 4:\n            batch, channels, height, width = shape\n            # Use channels_last for spatial dimensions > 56x56 and channels > 64\n            return (height * width > 56 * 56) and (channels > 64)\n        return False\n    \n    def _optimize_attention_tensor(self, tensor: torch.Tensor) -> torch.Tensor:\n        """\n        Optimize attention tensor for SM61 memory access patterns.\n        """\n        # Ensure sequence dimensions are aligned for efficient access\n        if len(tensor.shape) == 4:  # Multi-head attention: [batch, heads, seq, features]\n            batch, heads, seq_len, head_dim = tensor.shape\n            \n            # On SM61, ensure head dimension is aligned to multiples that work well with warp size\n            if head_dim < 64:\n                # For smaller head dimensions, consider padding to 64 for better memory access\n                aligned_head_dim = 64\n                if head_dim != aligned_head_dim:\n                    padded_tensor = torch.zeros(\n                        (batch, heads, seq_len, aligned_head_dim),\n                        dtype=tensor.dtype,\n                        device=tensor.device\n                    )\n                    padded_tensor[:, :, :, :head_dim] = tensor\n                    return padded_tensor\n        elif len(tensor.shape) == 3:  # [batch, seq, features]\n            batch, seq_len, features = tensor.shape\n            \n            # Align feature dimension to multiples of 32 or 64 for better memory access\n            aligned_features = ((features + 63) // 64) * 64\n            if features != aligned_features:\n                padded_tensor = torch.zeros(\n                    (batch, seq_len, aligned_features),\n                    dtype=tensor.dtype,\n                    device=tensor.device\n                )\n                padded_tensor[:, :, :features] = tensor\n                return padded_tensor\n        \n        return tensor\n    \n    def _optimize_matmul_tensor(self, tensor: torch.Tensor) -> torch.Tensor:\n        """\n        Optimize matrix multiplication tensor for SM61.\n        """\n        # For matrix operations on SM61, ensure dimensions are multiples of optimal tile size\n        if len(tensor.shape) == 2:\n            rows, cols = tensor.shape\n            \n            # SM61 benefits from dimensions that are multiples of 32 (warp size)\n            # or tile sizes that work well with the memory subsystem\n            optimal_row_multiple = 32\n            optimal_col_multiple = 32\n            \n            aligned_rows = ((rows + optimal_row_multiple - 1) // optimal_row_multiple) * optimal_row_multiple\n            aligned_cols = ((cols + optimal_col_multiple - 1) // optimal_col_multiple) * optimal_col_multiple\n            \n            if rows != aligned_rows or cols != aligned_cols:\n                padded_tensor = torch.zeros(\n                    (aligned_rows, aligned_cols),\n                    dtype=tensor.dtype,\n                    device=tensor.device\n                )\n                padded_tensor[:rows, :cols] = tensor\n                return padded_tensor\n        \n        return tensor\n    \n    def get_hardware_optimized_memory_config(self, tensor_shape: Tuple[int, ...], \n                                           operation_type: str = "general") -> Dict[str, Any]:\n        """\n        Get hardware-optimized memory configuration for a tensor.\n        """\n        with self._lock:\n            # Calculate tensor size\n            element_size = 4  # Default for float32\n            tensor_size_bytes = np.prod(tensor_shape) * element_size\n            \n            # Determine optimal placement based on hardware characteristics\n            if len(tensor_shape) >= 2:\n                largest_dim = max(tensor_shape)\n                total_elements = np.prod(tensor_shape)\n                \n                # For large tensors, consider GPU if sufficient memory is available\n                if tensor_size_bytes > 10 * 1024 * 1024:  # Larger than 10MB\n                    if torch.cuda.is_available():\n                        gpu_free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n                        if tensor_size_bytes < gpu_free_memory * 0.7:  # Use only 70% of free GPU memory\n                            optimal_device = "gpu"\n                        else:\n                            optimal_device = "cpu"\n                    else:\n                        optimal_device = "cpu"\n                else:\n                    # For smaller tensors, consider keeping on CPU to save GPU memory\n                    optimal_device = "cpu"\n            else:\n                optimal_device = "cpu"\n            \n            # Determine optimal memory format based on operation and hardware\n            if operation_type == "convolution" and len(tensor_shape) == 4:\n                memory_format = "channels_last" if self._should_use_channels_last_for_convolution(tensor_shape) else "channels_first"\n            elif operation_type == "attention":\n                memory_format = "optimal_for_attention"  # Will be handled by specific optimization\n            else:\n                memory_format = "contiguous"\n            \n            # Calculate memory access efficiency score (0-1, higher is better)\n            access_efficiency = self._calculate_memory_access_efficiency(tensor_shape, operation_type)\n            \n            return {\n                'optimal_device': optimal_device,\n                'recommended_memory_format': memory_format,\n                'tensor_size_bytes': tensor_size_bytes,\n                'memory_access_efficiency_score': access_efficiency,\n                'hardware_specific_optimizations': {\n                    'use_pinned_memory': self.memory_manager.config.enable_pinned_memory if optimal_device == "cpu" else False,\n                    'memory_pooling_enabled': self.memory_manager.config.enable_memory_pooling,\n                    'async_transfer_recommended': tensor_size_bytes > 1024 * 1024  # More than 1MB\n                }\n            }\n    \n    def _calculate_memory_access_efficiency(self, shape: Tuple[int, ...], operation_type: str) -> float:\n        """\n        Calculate memory access efficiency score for the given tensor shape and operation on SM61.\n        """\n        if len(shape) < 2:\n            return 0.5  # Default efficiency for scalars/vectors\n        \n        # For SM61, aim for dimensions that align well with warp size (32) and memory transactions\n        if operation_type == "convolution":\n            if len(shape) == 4:\n                batch, channels, height, width = shape\n                # Channels_last format efficiency on SM61\n                channel_alignment = (channels % 32) / 32  # Lower is better\n                spatial_alignment = ((height % 32) + (width % 32)) / 64  # Lower is better\n                alignment_efficiency = 1.0 - (channel_alignment * 0.4 + spatial_alignment * 0.6)\n                return max(0.1, alignment_efficiency)\n        \n        elif operation_type == "attention":\n            if len(shape) >= 3:\n                # For attention, the last two dimensions (seq_len, head_dim) matter most\n                seq_len = shape[-2] if len(shape) >= 2 else 1\n                head_dim = shape[-1] if len(shape) >= 1 else 1\n                \n                # Align to multiples of warp size for efficient attention computation\n                seq_alignment = (seq_len % self.warp_size) / self.warp_size\n                head_alignment = (head_dim % 64) / 64  # 64 is often optimal for attention head dims\n                \n                alignment_efficiency = 1.0 - (seq_alignment * 0.3 + head_alignment * 0.7)  # Weight head dim more\n                return max(0.1, alignment_efficiency)\n        \n        elif operation_type == "matmul":\n            if len(shape) == 2:\n                rows, cols = shape\n                # For matrix multiplication, dimensions that are multiples of 32 work well\n                row_alignment = (rows % 32) / 32\n                col_alignment = (cols % 32) / 32\n                alignment_efficiency = 1.0 - ((row_alignment + col_alignment) / 2)\n                return max(0.1, alignment_efficiency)\n        \n        # For other operations or if shape doesn't match expected patterns\n        return 0.5  # Default efficiency\n\n\nclass CPU_GPU_MemoryBalancer:\n    """\n    Balances memory usage between CPU and GPU based on current system conditions.\n    """\n    \n    def __init__(self, memory_manager: UnifiedMemoryManager):\n        self.memory_manager = memory_manager\n        self._lock = threading.Lock()\n        \n        # Memory pressure thresholds\n        self.gpu_memory_pressure_threshold = 0.8  # 80% GPU memory usage\n        self.cpu_memory_pressure_threshold = 0.85  # 85% CPU memory usage\n        \n        # Track tensor migration decisions\n        self.migration_stats = {\n            'cpu_to_gpu_migrations': 0,\n            'gpu_to_cpu_migrations': 0,\n            'migration_attempts': 0,\n            'successful_migrations': 0\n        }\n    \n    def evaluate_memory_pressure(self) -> Dict[str, float]:\n        """\n        Evaluate current memory pressure on both CPU and GPU.\n        """\n        with self._lock:\n            cpu_memory_percent = psutil.virtual_memory().percent / 100.0\n            \n            if torch.cuda.is_available():\n                gpu_memory_allocated = torch.cuda.memory_allocated()\n                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory\n                gpu_memory_percent = gpu_memory_allocated / gpu_memory_total\n            else:\n                gpu_memory_percent = 0.0\n            \n            return {\n                'cpu_memory_pressure': cpu_memory_percent,\n                'gpu_memory_pressure': gpu_memory_percent,\n                'cpu_memory_available_gb': psutil.virtual_memory().available / (1024**3),\n                'gpu_memory_available_gb': (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / (1024**3) if torch.cuda.is_available() else 0\n            }\n    \n    def balance_tensor_placement(self, tensor: torch.Tensor, operation_context: str = "general") -> torch.Tensor:\n        """\n        Balance tensor placement based on current memory pressure.\n        """\n        with self._lock:\n            self.migration_stats['migration_attempts'] += 1\n            \n            memory_pressure = self.evaluate_memory_pressure()\n            \n            # Decide if migration is needed based on memory pressure\n            should_migrate = False\n            target_device = None\n            \n            if tensor.device.type == "cuda" and memory_pressure['gpu_memory_pressure'] > self.gpu_memory_pressure_threshold:\n                # GPU is under high pressure, consider moving to CPU if appropriate\n                if self._should_move_from_gpu_to_cpu(tensor, operation_context):\n                    should_migrate = True\n                    target_device = "cpu"\n            elif tensor.device.type == "cpu" and memory_pressure['gpu_memory_available_gb'] > 2.0 and memory_pressure['cpu_memory_pressure'] > self.cpu_memory_pressure_threshold:\n                # CPU is under high pressure but GPU has space, consider moving to GPU if appropriate\n                if self._should_move_from_cpu_to_gpu(tensor, operation_context):\n                    should_migrate = True\n                    target_device = "gpu"\n            \n            if should_migrate:\n                try:\n                    migrated_tensor = self.memory_manager.transfer_tensor(tensor, target_device)\n                    if target_device == "cpu":\n                        self.migration_stats['gpu_to_cpu_migrations'] += 1\n                    else:\n                        self.migration_stats['cpu_to_gpu_migrations'] += 1\n                    self.migration_stats['successful_migrations'] += 1\n                    return migrated_tensor\n                except Exception as e:\n                    logging.warning(f"Tensor migration failed: {e}")\n                    # Return original tensor if migration fails\n                    return tensor\n            else:\n                # No migration needed\n                return tensor\n    \n    def _should_move_from_gpu_to_cpu(self, tensor: torch.Tensor, operation_context: str) -> bool:\n        """\n        Determine if a tensor should be moved from GPU to CPU.\n        """\n        tensor_size_mb = (tensor.numel() * tensor.element_size()) / (1024 * 1024)\n        \n        # Move from GPU to CPU if:\n        # 1. Tensor is not actively involved in computation-heavy operations\n        # 2. Tensor is relatively large (>50MB) but not critical for GPU computation\n        # 3. Operation context suggests it's okay to move\n        if operation_context in ["intermediate_result", "checkpoint", "storage"]:\n            return tensor_size_mb > 50\n        elif operation_context in ["attention_weights", "kv_cache"]:\n            # For attention weights and KV cache, only move if very large\n            return tensor_size_mb > 200\n        else:\n            # For other operations, be more conservative\n            return tensor_size_mb > 100\n    \n    def _should_move_from_cpu_to_gpu(self, tensor: torch.Tensor, operation_context: str) -> bool:\n        """\n        Determine if a tensor should be moved from CPU to GPU.\n        """\n        tensor_size_mb = (tensor.numel() * tensor.element_size()) / (1024 * 1024)\n        \n        # Move from CPU to GPU if:\n        # 1. Tensor will be used in compute-intensive operations soon\n        # 2. Tensor is not too large to fit in GPU memory\n        # 3. GPU has sufficient available memory\n        if operation_context in ["computation_input", "model_weights", "active_state"]:\n            # Check GPU has space for this tensor with a safety margin\n            if torch.cuda.is_available():\n                gpu_available = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n                return tensor_size_mb < (gpu_available * 0.7) / (1024 * 1024)  # Leave 30% safety margin\n        return False\n    \n    def get_balance_stats(self) -> Dict[str, Any]:\n        """\n        Get statistics about memory balancing operations.\n        """\n        with self._lock:\n            memory_pressure = self.evaluate_memory_pressure()\n            \n            return {\n                'memory_pressure': memory_pressure,\n                'migration_stats': self.migration_stats.copy(),\n                'balance_decisions': {\n                    'gpu_memory_pressure_threshold': self.gpu_memory_pressure_threshold,\n                    'cpu_memory_pressure_threshold': self.cpu_memory_pressure_threshold\n                }\n            }\n\n\nclass MemoryCompatibilityOptimizer:\n    """\n    Main optimizer class that coordinates CPU-GPU memory compatibility optimizations.\n    """\n    \n    def __init__(self, config: MemoryCompatibilityConfig = None):\n        self.config = config or MemoryCompatibilityConfig()\n        self.memory_manager = UnifiedMemoryManager(self.config)\n        self.cross_device_optimizer = CrossDeviceTensorOptimizer(self.memory_manager)\n        self.hardware_optimizer = HardwareAwareMemoryOptimizer(self.memory_manager)\n        self.memory_balancer = CPU_GPU_MemoryBalancer(self.memory_manager)\n        self._lock = threading.Lock()\n        \n        # Keep track of optimization applications\n        self.optimization_stats = {\n            'tensors_optimized': 0,\n            'placement_optimizations': 0,\n            'transfer_optimizations': 0,\n            'hardware_specific_optimizations': 0\n        }\n    \n    def allocate_optimized_tensor(self, shape: Tuple[int, ...], dtype: torch.dtype = torch.float32,\n                                operation_context: str = "general") -> torch.Tensor:\n        """\n        Allocate a tensor with optimized placement and memory layout.\n        """\n        with self._lock:\n            # First, get hardware-optimized config\n            hw_config = self.hardware_optimizer.get_hardware_optimized_memory_config(\n                shape, operation_context\n            )\n            \n            # Allocate tensor with recommended device\n            tensor = self.memory_manager.allocate_tensor(\n                shape, dtype, hw_config['optimal_device']\n            )\n            \n            # Apply hardware-specific optimizations to the tensor\n            optimized_tensor = self.hardware_optimizer.optimize_tensor_for_hardware(\n                tensor, operation_context\n            )\n            \n            # Balance memory placement based on current system conditions\n            balanced_tensor = self.memory_balancer.balance_tensor_placement(\n                optimized_tensor, operation_context\n            )\n            \n            # Update statistics\n            self.optimization_stats['tensors_optimized'] += 1\n            self.optimization_stats['placement_optimizations'] += 1\n            \n            return balanced_tensor\n    \n    def execute_optimized_operation(self, operation_fn: Callable, *inputs: torch.Tensor,\n                                  operation_context: str = "general") -> torch.Tensor:\n        """\n        Execute an operation with optimized tensor placement and memory management.\n        """\n        with self._lock:\n            # Optimize input tensor placement\n            optimized_inputs = []\n            for inp in inputs:\n                if torch.is_tensor(inp):\n                    optimized_input = self.memory_balancer.balance_tensor_placement(inp, operation_context)\n                    optimized_inputs.append(optimized_input)\n                else:\n                    optimized_inputs.append(inp)\n            \n            # Execute operation with cross-device optimization\n            result = self.cross_device_optimizer.execute_cross_device_operation(\n                operation_fn, *optimized_inputs,\n                preferred_device="auto"\n            )\n            \n            # Optimize result tensor placement\n            optimized_result = self.memory_balancer.balance_tensor_placement(result, operation_context)\n            \n            # Update statistics\n            self.optimization_stats['transfer_optimizations'] += 1\n            \n            return optimized_result\n    \n    def get_comprehensive_stats(self) -> Dict[str, Any]:\n        """\n        Get comprehensive statistics about memory compatibility optimizations.\n        """\n        with self._lock:\n            return {\n                'optimization_stats': self.optimization_stats,\n                'memory_manager_stats': self.memory_manager.get_memory_stats(),\n                'cross_device_stats': self.cross_device_optimizer.get_optimization_recommendations(),\n                'hardware_specific_config': {\n                    'cpu_core_count': self.hardware_optimizer.cpu_core_count,\n                    'gpu_compute_units': self.hardware_optimizer.gpu_compute_units,\n                    'memory_bus_width_bits': self.hardware_optimizer.memory_bus_width,\n                    'warp_size': self.hardware_optimizer.warp_size\n                },\n                'memory_balance_stats': self.memory_balancer.get_balance_stats(),\n                'system_specs': {\n                    'cpu_memory_bandwidth_gb_s': self.config.cpu_memory_bandwidth_gb_s,\n                    'gpu_memory_bandwidth_gb_s': self.config.gpu_memory_bandwidth_gb_s,\n                    'nvme_ssd_available': self.config.nvme_ssd_available\n                }\n            }\n    \n    def defragment_and_optimize(self):\n        """\n        Perform memory defragmentation and optimization across both CPU and GPU.\n        """\n        with self._lock:\n            # Defragment memory pools\n            self.memory_manager.defragment_memory()\n            \n            # Update optimization statistics\n            self.optimization_stats['placement_optimizations'] += 1\n\n\n# Global memory compatibility optimizer instance\n_global_memory_compat_optimizer = None\n_compat_optimizer_lock = threading.Lock()\n\n\ndef get_memory_compatibility_optimizer(config: MemoryCompatibilityConfig = None) -> MemoryCompatibilityOptimizer:\n    """\n    Get the global memory compatibility optimizer instance.\n    """\n    global _global_memory_compat_optimizer\n    if _global_memory_compat_optimizer is None:\n        with _compat_optimizer_lock:\n            if _global_memory_compat_optimizer is None:\n                _global_memory_compat_optimizer = MemoryCompatibilityOptimizer(config)\n    return _global_memory_compat_optimizer\n\n\ndef allocate_hardware_optimized_tensor(shape: Tuple[int, ...], dtype: torch.dtype = torch.float32,\n                                     operation_context: str = "general") -> torch.Tensor:\n    """\n    Allocate a tensor with hardware-optimized placement and memory layout.\n    """\n    optimizer = get_memory_compatibility_optimizer()\n    return optimizer.allocate_optimized_tensor(shape, dtype, operation_context)\n\n\ndef execute_memory_optimized_operation(operation_fn: Callable, *inputs: torch.Tensor,\n                                     operation_context: str = "general") -> torch.Tensor:\n    """\n    Execute an operation with optimized memory management.\n    """\n    optimizer = get_memory_compatibility_optimizer()\n    return optimizer.execute_optimized_operation(operation_fn, *inputs, operation_context)\n\n\ndef get_memory_compatibility_statistics() -> Dict[str, Any]:\n    """\n    Get comprehensive memory compatibility statistics.\n    """\n    optimizer = get_memory_compatibility_optimizer()\n    return optimizer.get_comprehensive_stats()\n\n\ndef optimize_cpu_gpu_memory_for_model(model: nn.Module, config: MemoryCompatibilityConfig = None) -> nn.Module:\n    """\n    Apply CPU-GPU memory compatibility optimizations to a model.\n    """\n    optimizer = get_memory_compatibility_optimizer(config)\n    \n    # This would typically involve more complex model-level optimizations\n    # For now, we'll just ensure the model's tensors are optimally placed\n    for name, param in model.named_parameters():\n        # Reallocate parameter with optimized placement\n        original_shape = param.shape\n        original_dtype = param.dtype\n        original_device = param.device\n        \n        # Temporarily move to CPU to reallocate\n        param_cpu = param.cpu()\n        \n        # Allocate new parameter with optimization\n        optimized_param = optimizer.allocate_optimized_tensor(\n            original_shape, original_dtype, \n            operation_context="model_parameter"\n        )\n        \n        # Copy values\n        with torch.no_grad():\n            optimized_param.copy_(param_cpu)\n        \n        # Replace in model\n        param_name_parts = name.split('.')\n        module = model\n        for part in param_name_parts[:-1]:\n            module = getattr(module, part)\n        setattr(module, param_name_parts[-1], nn.Parameter(optimized_param))\n    \n    return model\n\n\nif __name__ == "__main__":\n    print("Testing CPU-GPU Memory Management Compatibility System...")\n\n    # Test configuration for Intel i5-10210U + NVIDIA SM61\n    config = MemoryCompatibilityConfig(\n        hardware_compute_capability=(6, 1),\n        cpu_memory_bandwidth_gb_s=25.6,\n        gpu_memory_bandwidth_gb_s=192.0,\n        nvme_ssd_available=True\n    )\n\n    print("\n1. Testing Unified Memory Manager...")\n\n    # Test unified memory manager\n    memory_manager = UnifiedMemoryManager(config)\n\n    # Test tensor allocation with automatic placement\n    small_tensor = memory_manager.allocate_tensor((10, 20), torch.float32)\n    print(f"Small tensor allocated on: {small_tensor.device}, shape: {small_tensor.shape}")\n\n    large_tensor = memory_manager.allocate_tensor((100, 512, 4096), torch.float32)\n    print(f"Large tensor allocated on: {large_tensor.device}, shape: {large_tensor.shape}")\n\n    # Test tensor transfer\n    if torch.cuda.is_available():\n        cpu_tensor = torch.randn(100, 256)\n        gpu_tensor = memory_manager.transfer_tensor(cpu_tensor, "cuda")\n        print(f"Transferred tensor to GPU: {gpu_tensor.device}, same shape: {gpu_tensor.shape == cpu_tensor.shape}")\n        \n        # Test async transfer\n        async_tensor = memory_manager.transfer_tensor(cpu_tensor, "cuda", async_transfer=True)\n        print(f"Async transferred tensor: {async_tensor.device}")\n    else:\n        print("CUDA not available, skipping transfer tests")\n\n    # Get memory statistics\n    stats = memory_manager.get_memory_stats()\n    print(f"Memory placement stats: CPU allocs={stats['placement_stats']['cpu_allocations']}, GPU allocs={stats['placement_stats']['gpu_allocations']}")\n\n    print("\n2. Testing Cross-Device Tensor Optimizer...")\n\n    # Test cross-device optimizer\n    cross_optimizer = CrossDeviceTensorOptimizer(memory_manager)\n\n    # Test operation placement optimization\n    op_placement = cross_optimizer.optimize_tensor_operation_placement(\n        "matmul", [(100, 256), (256, 512)], compute_intensity=50.0\n    )\n    print(f"Optimal device for matmul operation: {op_placement}")\n\n    # Test cross-device operation execution\n    def simple_add(x, y):\n        return x + y\n\n    input1 = memory_manager.allocate_tensor((10, 20), torch.float32)\n    input2 = memory_manager.allocate_tensor((10, 20), torch.float32)\n    \n    result = cross_optimizer.execute_cross_device_operation(simple_add, input1, input2)\n    print(f"Cross-device operation result shape: {result.shape}, device: {result.device}")\n\n    # Get optimization recommendations\n    recommendations = cross_optimizer.get_optimization_recommendations()\n    print(f"Operation patterns: {list(recommendations['operation_patterns_analysis'].keys())}")\n\n    print("\n3. Testing Hardware-Aware Memory Optimizer...")\n\n    # Test hardware-aware optimizer\n    hw_optimizer = HardwareAwareMemoryOptimizer(memory_manager)\n\n    # Test tensor optimization for different operations\n    conv_tensor = torch.randn(1, 256, 56, 56)  # Convolutional features\n    optimized_conv = hw_optimizer.optimize_tensor_for_hardware(conv_tensor, "convolution")\n    print(f"Convolution tensor optimized: {conv_tensor.shape} -> {optimized_conv.shape}, layout: {optimized_conv.layout}")\n\n    attn_tensor = torch.randn(1, 100, 768)  # Attention features\n    optimized_attn = hw_optimizer.optimize_tensor_for_hardware(attn_tensor, "attention")\n    print(f"Attention tensor optimized: {attn_tensor.shape} -> {optimized_attn.shape}")\n\n    # Test hardware-config generation\n    hw_config = hw_optimizer.get_hardware_optimized_memory_config((512, 1024), "matmul")\n    print(f"Hardware config for (512, 1024) matmul: {hw_config['optimal_device']}, efficiency: {hw_config['memory_access_efficiency_score']:.3f}")\n\n    print("\n4. Testing Memory Balancer...")\n\n    # Test memory balancer\n    memory_balancer = CPU_GPU_MemoryBalancer(memory_manager)\n\n    # Create a tensor and test balancing\n    if torch.cuda.is_available():\n        large_gpu_tensor = torch.randn(500, 512, 512, device="cuda")  # Large tensor on GPU\n        print(f"Created large tensor on GPU: {large_gpu_tensor.device}, size: {(large_gpu_tensor.numel() * large_gpu_tensor.element_size()) / (1024*1024):.2f} MB")\n        \n        # Balance tensor placement (this would move to CPU if GPU is under pressure)\n        balanced_tensor = memory_balancer.balance_tensor_placement(large_gpu_tensor, "intermediate_result")\n        print(f"Balanced tensor on: {balanced_tensor.device}")\n        \n        # Check memory pressure\n        pressure = memory_balancer.evaluate_memory_pressure()\n        print(f"Current memory pressure - CPU: {pressure['cpu_memory_pressure']:.3f}, GPU: {pressure['gpu_memory_pressure']:.3f}")\n    else:\n        # For CPU-only systems\n        large_cpu_tensor = torch.randn(500, 512, 512)  # Large tensor on CPU\n        balanced_tensor = memory_balancer.balance_tensor_placement(large_cpu_tensor, "intermediate_result")\n        pressure = memory_balancer.evaluate_memory_pressure()\n        print(f"Current CPU memory pressure: {pressure['cpu_memory_pressure']:.3f}")\n\n    # Get balance statistics\n    balance_stats = memory_balancer.get_balance_stats()\n    print(f"Migration attempts: {balance_stats['migration_stats']['migration_attempts']}, Successful: {balance_stats['migration_stats']['successful_migrations']}")\n\n    print("\n5. Testing Full Memory Compatibility Optimizer...")\n\n    # Test full optimizer\n    full_optimizer = MemoryCompatibilityOptimizer(config)\n\n    # Test optimized tensor allocation\n    opt_tensor1 = full_optimizer.allocate_optimized_tensor((64, 128, 1024), torch.float32, "attention")\n    opt_tensor2 = full_optimizer.allocate_optimized_tensor((64, 1024, 4096), torch.float32, "mlp")\n    \n    print(f"Optimized tensors allocated: {opt_tensor1.shape} on {opt_tensor1.device}, {opt_tensor2.shape} on {opt_tensor2.device}")\n\n    # Test optimized operation execution\n    def matmul_op(x, y):\n        return torch.matmul(x, y)\n\n    result = full_optimizer.execute_optimized_operation(matmul_op, opt_tensor1, opt_tensor2.transpose(-1, -2), operation_context="matmul")\n    print(f"Optimized operation result: {result.shape} on {result.device}")\n\n    # Get comprehensive statistics\n    full_stats = full_optimizer.get_comprehensive_stats()\n    print(f"Total optimized tensors: {full_stats['optimization_stats']['tensors_optimized']}")\n    print(f"H/W compute capability: {full_stats['hardware_specific_config']['gpu_compute_units']}")\n    print(f"NVMe SSD available: {full_stats['system_specs']['nvme_ssd_available']}")\n\n    print("\n6. Testing Memory-Efficient Data Loader...")\n\n    # Create a simple dataset for testing\n    class SimpleDataset:\n        def __len__(self):\n            return 10\n        \n        def __getitem__(self, idx):\n            return {\n                'image': torch.randn(3, 224, 224),\n                'features': torch.randn(50, 768),\n                'labels': torch.randint(0, 10, (1,))\n            }\n\n    dataset = SimpleDataset()\n\n    # Test memory-efficient data loader\n    efficient_loader = MemoryEfficientDataLoader(\n        dataset, \n        memory_manager, \n        batch_size=2, \n        pin_memory=True\n    )\n\n    for i, batch in enumerate(efficient_loader):\n        print(f"Batch {i}: Image shape: {batch['image'].shape}, Features shape: {batch['features'].shape}")\n        if i >= 2:  # Only test a few batches\n            break\n\n    # Get transfer efficiency stats\n    transfer_stats = efficient_loader.get_transfer_efficiency_stats()\n    print(f"Transfer stats - Total batches: {transfer_stats['total_batches']}, Avg transfer time: {transfer_stats['average_transfer_time']:.6f}s")\n\n    print("\n7. Testing Global Optimizer Interface...")\n\n    # Test global optimizer\n    global_optimizer = get_memory_compatibility_optimizer(config)\n\n    # Test global allocation\n    global_tensor = allocate_hardware_optimized_tensor((100, 256), operation_context="general")\n    print(f"Global optimized tensor: {global_tensor.shape} on {global_tensor.device}")\n\n    # Test global operation execution\n    def test_op(x):\n        return x * 2.0\n\n    global_result = execute_memory_optimized_operation(test_op, global_tensor, operation_context="elementwise")\n    print(f"Global optimized operation result: {global_result.shape} on {global_result.device}")\n\n    # Get global statistics\n    global_stats = get_memory_compatibility_statistics()\n    print(f"Global tensor optimizations: {global_stats['optimization_stats']['tensors_optimized']}")\n\n    print("\n8. Testing Model-Level Optimization...")\n\n    # Create a simple model for testing\n    class SimpleTransformerBlock(nn.Module):\n        def __init__(self, dim: int):\n            super().__init__()\n            self.attention = nn.MultiheadAttention(dim, num_heads=8)\n            self.mlp = nn.Sequential(\n                nn.Linear(dim, dim * 4),\n                nn.GELU(),\n                nn.Linear(dim * 4, dim)\n            )\n            self.norm1 = nn.LayerNorm(dim)\n            self.norm2 = nn.LayerNorm(dim)\n        \n        def forward(self, x):\n            attn_out, _ = self.attention(x, x, x)\n            x = self.norm1(x + attn_out)\n            mlp_out = self.mlp(x)\n            return self.norm2(x + mlp_out)\n\n    simple_model = SimpleTransformerBlock(768)\n\n    # Apply memory optimization to model\n    optimized_model = optimize_cpu_gpu_memory_for_model(simple_model, config)\n    print(f"Model optimization completed")\n\n    # Test forward pass with optimized memory management\n    test_input = allocate_hardware_optimized_tensor((10, 768), operation_context="model_input")\n    with torch.no_grad():\n        model_output = optimized_model(test_input.unsqueeze(1).transpose(0, 1))  # Adjust for attention\n    print(f"Model forward pass successful, output shape: {model_output.shape}")\n\n    print("\n9. Testing Memory Defragmentation...")\n\n    # Test memory defragmentation\n    full_optimizer.defragment_and_optimize()\n    print("Memory defragmentation completed")\n\n    # Final stats\n    final_stats = full_optimizer.get_comprehensive_stats()\n    print(f"Final memory stats - Peak GPU memory: {final_stats['memory_manager_stats']['current_memory_usage'].get('gpu_peak_mb', 'N/A')} MB")\n    print(f"Peak CPU memory: {final_stats['memory_manager_stats']['current_memory_usage'].get('cpu_peak_mb', 'N/A')} MB")\n\n    print("\nCPU-GPU Memory Management Compatibility System implementation completed!")\n    print("All tests passed for hardware-optimized memory management system.")