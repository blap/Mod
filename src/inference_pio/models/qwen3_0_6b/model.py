"""
Qwen3-0.6B Model Implementation - Self-Contained Version

This module implements the Qwen3-0.6B model following the self-contained plugin architecture
for the Inference-PIO system. This implementation is optimized specifically for Qwen3-0.6B
characteristics while maintaining compatibility with the generic model interface. Each model
plugin is completely independent with its own configuration, tests, and benchmarks.
"""

import logging
import os
import shutil
import time
from typing import Any, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn

# Removing hard dependency on transformers for model loading
try:
    from huggingface_hub import snapshot_download
except ImportError:
    snapshot_download = None

import subprocess
import sys

try:
    from inference_pio.common.hardware.hardware_analyzer import get_system_profile
except ImportError:
    try:
        from ...common.hardware.hardware_analyzer import get_system_profile
    except ImportError:
        # Fallback to absolute
        import sys
        import os
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))
        from src.inference_pio.common.hardware.hardware_analyzer import get_system_profile

from .architecture import Qwen3ForCausalLM as SelfContainedQwen3
from .config import Qwen3_0_6B_Config
from .intelligent_cache.intelligent_cache_manager import (
    apply_intelligent_caching_to_model,
    create_intelligent_cache_for_qwen3_0_6b
)

# Import the specialized Sparse Attention for Qwen3-0.6B
from ...common.attention.sparse_attention import SparseAttention, create_sparse_attention

# Import Optimization Floor Modules
try:
    from ...common.disk_offloading import TensorOffloadingManager, create_disk_offloader
    from ...common.flash_attention_2 import apply_flash_attention_2_to_model
    from ...common.tensor_pagination import (  # Reuse multimodal pager for generic tensor handling if needed
        create_multimodal_pagination_system,
    )
    # Import fused kernels if available
    try:
        from ...common.fused_kernels import apply_fused_kernels
    except ImportError:
        apply_fused_kernels = None

except ImportError:
    # Fallback para quando os imports relativos nÃ£o funcionam
    import sys
    import os
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))
    try:
        from src.inference_pio.common.optimization.disk_offloading import TensorOffloadingManager, create_disk_offloader
        from src.inference_pio.common.attention.flash_attention_2 import apply_flash_attention_2_to_model
        from src.inference_pio.common.optimization.tensor_pagination import (  # Reuse multimodal pager for generic tensor handling if needed
            create_multimodal_pagination_system,
        )
        try:
            from src.inference_pio.common.optimization.fused_kernels import apply_fused_kernels
        except ImportError:
            apply_fused_kernels = None
    except ImportError:
        apply_flash_attention_2_to_model = None
        create_disk_offloader = None
        apply_fused_kernels = None

logger = logging.getLogger(__name__)


class Qwen3_0_6B_Model(nn.Module):
    """
    Qwen3-0.6B model implementation with all optimizations integrated.
    """

    def __init__(self, config: Qwen3_0_6B_Config):
        super().__init__()
        self.config = config
        self._model = None
        self._tokenizer = None
        self._system_profile = get_system_profile()

        # Initialize the model logic
        self._initialize_model()

    def _initialize_model(self):
        """
        Initialize the Qwen3-0.6B model with appropriate optimizations and robust loading.
        """
        logger.info(f"Initializing Qwen3-0.6B model...")

        # 1. Determine Model Path
        model_path = self._resolve_model_path()
        if not model_path:
            raise RuntimeError(
                "CRITICAL: Qwen3-0.6B model not found and download failed. Stopping."
            )

        # 2. Prepare Loading Arguments
        # Note: 'device_map' and 'trust_remote_code' are transformers concepts.
        # We manually handle device placement for self-contained models.

        dtype = getattr(torch, self.config.torch_dtype) if isinstance(self.config.torch_dtype, str) else self.config.torch_dtype

        # 3. Load Model using Self-Contained Architecture
        logger.info("Loading self-contained Qwen3 architecture.")
        try:
            # Initialize empty model structure
            self._model = SelfContainedQwen3(self.config)

            # Load weights if available (implementing basic safetensors/bin loading)
            # Using CustomModelLoader logic
            from ...common.custom_components.model_loader import CustomModelLoader

            device = "cuda" if torch.cuda.is_available() and self.config.device_map != "cpu" else "cpu"
            CustomModelLoader.load_weights(self._model, model_path, device=device)
            self._model.to(dtype=dtype)

        except Exception as e:
            logger.error(f"CRITICAL: Failed to load self-contained model: {e}")
            raise RuntimeError(f"Could not load Qwen3-0.6B model: {e}")

        # 4. Load Custom Tokenizer
        try:
            from ...common.custom_components.tokenizer import load_custom_tokenizer
            self._tokenizer = load_custom_tokenizer(model_path)
        except Exception as e:
            logger.warning(f"Could not load custom tokenizer: {e}")
            self._tokenizer = None

        # 5. Apply Optimization Floor
        self._apply_optimization_floor()

        # 6. Apply Thinking Mode Optimizations
        if self.config.enable_thinking:
            self._apply_thinking_optimizations()


    def _resolve_model_path(self) -> str:
        """
        Prioritize H:/ -> Temp -> Download.
        """
        # Priority 1: H Drive (Local)
        h_drive_path = "H:/Qwen3-0.6B"
        if os.path.exists(h_drive_path):
            logger.info(f"Found model at H:/ drive: {h_drive_path}")
            return h_drive_path

        # Priority 2: Configured Path
        if os.path.exists(self.config.model_path):
            logger.info(f"Found model at configured path: {self.config.model_path}")
            return self.config.model_path

        # Priority 3: Temp Directory / Download
        # Assuming a temp directory structure
        temp_path = os.path.join(os.getcwd(), "temp_models", "Qwen3-0.6B")

        if (
            os.path.exists(temp_path)
            and os.path.isdir(temp_path)
            and os.listdir(temp_path)
        ):
            logger.info(f"Found model in temp path: {temp_path}")
            return temp_path

        # Download
        logger.info(f"Model not found locally. Attempting download to {temp_path}...")
        try:
            if snapshot_download:
                snapshot_download(repo_id="Qwen/Qwen3-0.6B", local_dir=temp_path)
                logger.info("Download successful.")
                return temp_path
            else:
                logger.error("huggingface_hub not installed, cannot download model.")
                return None
        except Exception as e:
            logger.error(f"Download failed: {e}")
            return None

    def _apply_optimization_floor(self):
        """
        Apply mandatory optimizations defined in OPTIMIZATIONS.md.
        """
        logger.info("Applying Optimization Floor...")

        # A. Flash Attention 2 / SDPA
        # In the custom architecture, Flash Attention is built-in or handled via config
        # We can explicitly set it here if needed
        pass

        # B. Fused Kernels (RMSNorm, MLP, RoPE)
        if apply_fused_kernels:
            try:
                # Apply to self-contained model
                # Note: apply_fused_kernels needs to support custom nn.Module structures
                self._model = apply_fused_kernels(self._model)
                logger.info("Fused Kernels applied.")
            except Exception as e:
                logger.warning(f"Fused Kernels application failed: {e}")
        else:
            logger.info("Fused Kernels not available, skipping.")

        # C. Paged KV Cache & Continuous Batching
        # We check if we can enable them via configuration on the underlying model
        if hasattr(self._model, "config"):
            if hasattr(self._model.config, "use_cache"):
                self._model.config.use_cache = True
                logger.info("KV Cache enabled.")

    def _apply_thinking_optimizations(self):
        """
        Apply optimizations specific to Thinking Mode.
        """
        logger.info("Applying Thinking Mode Optimizations...")

        # 1. Long-Sequence RoPE Support
        # Ensure rotary embeddings are calculated in float32
        for module in self._model.modules():
            if "RotaryEmbedding" in str(type(module)):
                if hasattr(module, "inv_freq"):
                    module.inv_freq = module.inv_freq.to(dtype=torch.float32)
                    logger.debug(
                        "Upcast RoPE inv_freq to float32 for long-context stability."
                    )

    def forward(self, *args, **kwargs):
        # Apply intelligent caching if enabled
        if hasattr(self.config, 'intelligent_cache_enabled') and self.config.intelligent_cache_enabled:
            # Apply intelligent caching to the model
            if not hasattr(self, 'intelligent_cache_manager'):
                self.intelligent_cache_manager = create_intelligent_cache_for_qwen3_0_6b(self.config)

        return self._model(*args, **kwargs)

    def generate(self, *args, **kwargs):
        """
        Optimized generation wrapper.
        """
        # Call the custom generate method of the self-contained model
        result = self._model.generate(*args, **kwargs)

        # Post-generation: Check for thought end token and compress if needed
        # This assumes result is token ids
        if self.config.enable_thought_compression and torch.is_tensor(result):
             # Log the event for analysis
             logger.debug("Thought segment generation complete. Cache compression will be handled by the session manager.")

        return result

    def compress_thought_segment(self, kv_cache):
        """
        Optimization: Compress the KV cache of the thought segment once </think> is reached.
        """
        if self.config.enable_thought_compression and kv_cache is not None:
            # Logic: Identify "thought" tokens in history and prune their KV entries
            # or quantize them heavily.
            # Since we don't have direct access to the cache structure here (it's tuple of tensors),
            # we implement a basic truncation/quantization strategy.

            logger.debug("Compressing thought segment in KV cache...")

            compressed_cache = []
            for layer_cache in kv_cache:
                # layer_cache is usually (key, value)
                k, v = layer_cache
                # Quantize to int8 for storage
                k_comp = k.to(torch.int8)
                v_comp = v.to(torch.int8)
                compressed_cache.append((k_comp, v_comp))

            return tuple(compressed_cache)
        return kv_cache

    def install(self):
        """
        Install or prepare any dependencies or configurations required for the model.

        This method ensures that all necessary components for the Qwen3-0.6B model
        are properly installed and configured before execution.
        """
        logger.info(
            "Installing/Preparing Qwen3-0.6B model dependencies and configurations..."
        )

        # Check and install required packages
        required_packages = ["torch", "huggingface_hub", "safetensors"]

        for package in required_packages:
            try:
                __import__(package)
                logger.info(f"{package} is already installed")
            except ImportError:
                logger.info(f"Installing {package}...")
                subprocess.check_call([sys.executable, "-m", "pip", "install", package])

        # Additional model-specific installations can go here
        # For example, downloading model files if not present

        # Verify model files are accessible
        model_path = self._resolve_model_path()
        if model_path:
            logger.info(f"Model is accessible at: {model_path}")
        else:
            logger.warning(
                "Model files not found locally, they will be downloaded when the model is initialized"
            )

        logger.info("Qwen3-0.6B model installation/preparation completed")


def create_qwen3_0_6b_model(config: Qwen3_0_6B_Config) -> Qwen3_0_6B_Model:
    return Qwen3_0_6B_Model(config)
