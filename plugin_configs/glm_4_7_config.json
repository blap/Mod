{
  "plugin_name": "glm_4_7_flash",
  "plugin_type": "model_component",
  "model_path": "H:\\GLM-4.7-Flash",
  "parameters": {
    "num_hidden_layers": 40,
    "num_attention_heads": 40,
    "hidden_size": 5120,
    "intermediate_size": 13952,
    "num_key_value_heads": 10,
    "max_position_embeddings": 8192,
    "rope_theta": 10000.0,
    "vocab_size": 151552,
    "gradient_checkpointing": true,
    "use_cache": true,
    "torch_dtype": "float16",
    "device_map": "auto",
    "enable_kernel_fusion": true,
    "kernel_fusion_patterns": ["linear_relu", "linear_gelu", "matmul_add", "add_layer_norm"],
    "use_custom_cuda_kernels": true,
    "custom_kernel_fallback_enabled": true,
    "enable_distributed_simulation": true,
    "num_distributed_partitions": 4,
    "partition_strategy": "layer_wise",
    "memory_per_partition_gb": 4.0,
    "overlap_communication": true,
    "pipeline_depth": 1,
    "sync_method": "barrier",
    "enable_gradient_checkpointing_in_distributed": true,
    "enable_tensor_parallelism": false,
    "tensor_parallel_size": 1,
    "enable_tensor_compression": true,
    "tensor_compression_method": "incremental_pca",
    "tensor_compression_ratio": 0.5,
    "tensor_compression_max_components": 256,
    "compression_memory_threshold_high": 0.8,
    "compression_memory_threshold_critical": 0.9,
    "enable_adaptive_compression": true,
    "enable_activation_compression": true,
    "compression_update_frequency": 100,
    "enable_sharding": true,
    "num_shards": 500,
    "sharding_storage_path": "./shards/glm_4_7_flash",
    "max_loaded_shards": 10,
    "sharding_priority": "high",
    "enable_model_surgery": true,
    "surgery_enabled": true,
    "auto_identify_components": true,
    "surgery_priority_threshold": 10,
    "analysis_only": false,
    "preserve_components": ["lm_head", "embed_tokens"],
    "components_to_remove": null,
    "enable_pipeline": true,
    "pipeline_checkpoint_dir": "./pipeline_checkpoints/glm_4_7_flash",
    "pipeline_max_concurrent_stages": 1,
    "pipeline_cleanup_after_completion": true,
    "pipeline_stages": [
      {
        "name": "tokenization",
        "function": "tokenize",
        "input_keys": ["text"],
        "output_keys": ["tokens"],
        "cache_intermediates": true
      },
      {
        "name": "model_inference",
        "function": "infer",
        "input_keys": ["tokens"],
        "output_keys": ["model_outputs"],
        "cache_intermediates": true
      },
      {
        "name": "decoding",
        "function": "detokenize",
        "input_keys": ["model_outputs"],
        "output_keys": ["decoded_text"],
        "cache_intermediates": false
      }
    ],
    "enable_activation_offloading": true,
    "activation_max_memory_ratio": 0.7,
    "activation_offload_directory": "./activation_offloads/glm_4_7_flash",
    "activation_page_size_mb": 8,
    "activation_eviction_policy": "predictive",
    "activation_offloading_priority": "medium",
    "enable_predictive_activation_offloading": true,
    "proactive_activation_offloading_interval": 5.0
  },
  "input_formats": ["text"],
  "output_formats": ["text"],
  "dependencies": ["torch>=2.0.0", "transformers>=4.30.0", "accelerate", "zhipuai"],
  "enabled": true,
  "priority": 90,
  "metadata": {
    "version": "1.0.0",
    "author": "Zhipu AI",
    "description": "GLM-4.7-Flash specialized language model with advanced reasoning capabilities",
    "model_architecture": "Transformer-based language model optimized for advanced reasoning",
    "model_size": "4.7B",
    "required_memory_gb": 16.0,
    "supported_modalities": ["text"],
    "license": "Proprietary",
    "tags": ["advanced-reasoning", "text-generation", "text-completion", "text-understanding", "glm", "4.7b", "flash", "instruction-following", "multilingual", "kernel-fusion", "cuda-kernels", "distributed-simulation", "extreme-sharding", "streaming-loader", "disk-pipeline"]
  }
}