{
  "plugin_name": "qwen3_coder_30b_a3b_instruct",
  "plugin_type": "model_component",
  "model_path": "H:/Qwen3-Coder-30B",
  "parameters": {
    "num_hidden_layers": 64,
    "num_attention_heads": 52,
    "hidden_size": 6656,
    "intermediate_size": 17920,
    "num_key_value_heads": 52,
    "max_position_embeddings": 32768,
    "rope_theta": 1000000.0,
    "vocab_size": 151936,
    "sliding_window": 131072,
    "gradient_checkpointing": true,
    "use_cache": true,
    "torch_dtype": "float16",
    "device_map": "auto",
    "enable_kernel_fusion": true,
    "kernel_fusion_patterns": ["linear_relu", "linear_gelu", "matmul_add", "add_layer_norm"],
    "use_custom_cuda_kernels": true,
    "custom_kernel_fallback_enabled": true,
    "enable_distributed_simulation": true,
    "num_distributed_partitions": 8,
    "partition_strategy": "layer_wise",
    "memory_per_partition_gb": 8.0,
    "overlap_communication": true,
    "pipeline_depth": 1,
    "sync_method": "barrier",
    "enable_gradient_checkpointing_in_distributed": true,
    "enable_tensor_parallelism": false,
    "tensor_parallel_size": 1,
    "enable_tensor_compression": true,
    "tensor_compression_method": "incremental_pca",
    "tensor_compression_ratio": 0.5,
    "tensor_compression_max_components": 256,
    "compression_memory_threshold_high": 0.8,
    "compression_memory_threshold_critical": 0.9,
    "enable_adaptive_compression": true,
    "enable_activation_compression": true,
    "compression_update_frequency": 100,
    "enable_sharding": true,
    "num_shards": 500,
    "sharding_storage_path": "./shards/qwen3_coder_30b",
    "max_loaded_shards": 10,
    "sharding_priority": "high",
    "enable_model_surgery": true,
    "surgery_enabled": true,
    "auto_identify_components": true,
    "surgery_priority_threshold": 10,
    "analysis_only": false,
    "preserve_components": ["lm_head", "embed_tokens"],
    "components_to_remove": null,
    "enable_pipeline": true,
    "pipeline_checkpoint_dir": "./pipeline_checkpoints/qwen3_coder_30b",
    "pipeline_max_concurrent_stages": 1,
    "pipeline_cleanup_after_completion": true,
    "pipeline_stages": [
      {
        "name": "tokenization",
        "function": "tokenize",
        "input_keys": ["text"],
        "output_keys": ["tokens"],
        "cache_intermediates": true
      },
      {
        "name": "model_inference",
        "function": "infer",
        "input_keys": ["tokens"],
        "output_keys": ["model_outputs"],
        "cache_intermediates": true
      },
      {
        "name": "decoding",
        "function": "detokenize",
        "input_keys": ["model_outputs"],
        "output_keys": ["decoded_text"],
        "cache_intermediates": false
      }
    ],
    "enable_activation_offloading": true,
    "activation_max_memory_ratio": 0.7,
    "activation_offload_directory": "./activation_offloads/qwen3_coder_30b",
    "activation_page_size_mb": 8,
    "activation_eviction_policy": "predictive",
    "activation_offloading_priority": "medium",
    "enable_predictive_activation_offloading": true,
    "proactive_activation_offloading_interval": 5.0
  },
  "input_formats": ["text"],
  "output_formats": ["text"],
  "dependencies": ["torch>=2.0.0", "transformers>=4.30.0", "accelerate"],
  "enabled": true,
  "priority": 90,
  "metadata": {
    "version": "1.0.0",
    "author": "Qwen Team",
    "description": "Qwen3-Coder-30B-A3B-Instruct model plugin with coding-specific optimizations",
    "model_architecture": "Transformer-based language model optimized for code processing",
    "model_size": "30B",
    "required_memory_gb": 64.0,
    "supported_modalities": ["text"],
    "license": "MIT",
    "tags": ["code-generation", "code-completion", "programming", "qwen3-coder", "30b", "coding-assistant", "instruction-following", "kernel-fusion", "cuda-kernels", "distributed-simulation", "extreme-sharding", "streaming-loader", "disk-pipeline"]
  }
}